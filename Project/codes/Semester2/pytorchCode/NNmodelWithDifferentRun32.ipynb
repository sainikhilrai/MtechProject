{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import preprocessingWithMissingvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make_Accura</th>\n",
       "      <th>Make_BMW</th>\n",
       "      <th>Make_Chevrolet</th>\n",
       "      <th>Make_Dodge</th>\n",
       "      <th>Make_Ferrari</th>\n",
       "      <th>Make_Ford</th>\n",
       "      <th>Make_Honda</th>\n",
       "      <th>Make_Jaguar</th>\n",
       "      <th>Make_Lexus</th>\n",
       "      <th>Make_Mazda</th>\n",
       "      <th>...</th>\n",
       "      <th>NumberOfCars_5 to 8</th>\n",
       "      <th>NumberOfCars_more than 8</th>\n",
       "      <th>BasePolicy_All Perils</th>\n",
       "      <th>BasePolicy_Collision</th>\n",
       "      <th>BasePolicy_Liability</th>\n",
       "      <th>Age</th>\n",
       "      <th>Deductible</th>\n",
       "      <th>DriverRating</th>\n",
       "      <th>DaysDiff</th>\n",
       "      <th>Lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Make_Accura  Make_BMW  Make_Chevrolet  Make_Dodge  Make_Ferrari  Make_Ford  \\\n",
       "0            0         0               0           0             0          0   \n",
       "1            0         0               0           0             0          0   \n",
       "2            0         0               0           0             0          0   \n",
       "3            0         0               0           0             0          0   \n",
       "4            0         0               0           0             0          0   \n",
       "\n",
       "   Make_Honda  Make_Jaguar  Make_Lexus  Make_Mazda  ...    \\\n",
       "0           1            0           0           0  ...     \n",
       "1           1            0           0           0  ...     \n",
       "2           1            0           0           0  ...     \n",
       "3           0            0           0           0  ...     \n",
       "4           1            0           0           0  ...     \n",
       "\n",
       "   NumberOfCars_5 to 8  NumberOfCars_more than 8  BasePolicy_All Perils  \\\n",
       "0                    0                         0                      0   \n",
       "1                    0                         0                      0   \n",
       "2                    0                         0                      0   \n",
       "3                    0                         0                      0   \n",
       "4                    0                         0                      0   \n",
       "\n",
       "   BasePolicy_Collision  BasePolicy_Liability     Age  Deductible  \\\n",
       "0                     0                     1  0.2625    0.000000   \n",
       "1                     1                     0  0.4250    1.000000   \n",
       "2                     1                     0  0.5875    0.666667   \n",
       "3                     0                     1  0.8125    0.333333   \n",
       "4                     1                     0  0.3375    0.000000   \n",
       "\n",
       "   DriverRating  DaysDiff  Lable  \n",
       "0      0.000000  0.071795      0  \n",
       "1      1.000000  0.069231      0  \n",
       "2      0.666667  0.089744      0  \n",
       "3      0.333333  0.107692      0  \n",
       "4      0.000000  0.094872      0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carDf= pd.read_csv('oneHotPreprocess.csv')\n",
    "carDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the lable from the dataset\n",
    "carDf.head()\n",
    "carLable= carDf['Lable']\n",
    "carDf.drop(['Lable'],inplace=True,axis=1) #drop the lable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "carDateNormalized= carDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudulent: <class 'pandas.core.frame.DataFrame'>\n",
      "carDateNormalized: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Divide the data into fraudulent and non-fraudulent data\n",
    "columnlable= list(carDateNormalized.columns.values)\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "nonFraudulent= pd.DataFrame(columns=columnlable)\n",
    "nonFraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "fraudulent= pd.DataFrame(columns=columnlable)\n",
    "fraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "print(\"nonFraudulent:\",type(nonFraudulent))\n",
    "print(\"carDateNormalized:\",type(carDateNormalized))\n",
    "\n",
    "j= 0\n",
    "k= 0\n",
    "for i in range(carDateNormalized.shape[0]):\n",
    "    if(carLable[i]==0):\n",
    "        nonFraudulent.loc[j]= carDateNormalized.loc[i]\n",
    "        nonFraudulentLable.loc[j]= 0.0\n",
    "        j += 1\n",
    "    else:\n",
    "        fraudulent.loc[k]= carDateNormalized.loc[i]\n",
    "        fraudulentLable.loc[i]= 1.0\n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nonFraudulent))\n",
    "print(type(fraudulent))\n",
    "\n",
    "#convert to dataframe\n",
    "nonFraudulentLable= pd.DataFrame(nonFraudulentLable)\n",
    "fraudulentLable= pd.DataFrame(fraudulentLable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train and test set in equal proportion\n",
    "#divide the nonFraudulent into test and train\n",
    "nonFraudX_train,nonFraudX_test,nonFraudY_train,nonFraudY_test = train_test_split(nonFraudulent,nonFraudulentLable,random_state=3,test_size=0.3)\n",
    "\n",
    "#divide the fraudulent into test and train\n",
    "fraudX_train,fraudX_test,fraudY_train,fraudY_test = train_test_split(fraudulent, fraudulentLable,random_state=3,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData set size: 8634\n",
      "Test Data set size: 4626\n",
      "ValidData set size: 2159\n"
     ]
    }
   ],
   "source": [
    "#combine the fraudulent and nonfraudulent training and test set to make the newTrain and newTest\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "newCombineTrain= pd.DataFrame(columns=columnlable)\n",
    "newCombineTrainLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "newCombineTest= pd.DataFrame(columns=columnlable)\n",
    "newCombineTestLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "\n",
    "\n",
    "#combine trainset\n",
    "framesFeatures= [nonFraudX_train,fraudX_train]\n",
    "framesLable= [nonFraudY_train,fraudY_train]\n",
    "newCombineTrain= pd.concat(framesFeatures)\n",
    "newCombineTrainLable= pd.concat(framesLable)\n",
    "\n",
    "\n",
    "framesFeatures2= [nonFraudX_test,fraudX_test]\n",
    "framesLable2= [nonFraudY_test,fraudY_test]\n",
    "newCombineTest= pd.concat(framesFeatures2)\n",
    "newCombineTestLable= pd.concat(framesLable2)\n",
    "\n",
    "#make the dataset as values\n",
    "#newCombineTrain= newCombineTrain.values\n",
    "#newCombineTrainLable= newCombineTrainLable.values\n",
    "\n",
    "\n",
    "#divide the trainset into trainset and validation set\n",
    "newX_train,newX_trainV,newY_train,newY_trainV= train_test_split(newCombineTrain,newCombineTrainLable,\n",
    "                                                                random_state=3,test_size=0.2)\n",
    "\n",
    "\n",
    "newCombineTest= newCombineTest.values\n",
    "newCombineTestLable= newCombineTestLable.values\n",
    "\n",
    "newCombineTrain= newX_train.values\n",
    "newCombineTrainLable= newY_train.values\n",
    "\n",
    "validX_train= newX_trainV.values\n",
    "validY_test= newY_trainV.values\n",
    "print(\"TrainData set size:\",newCombineTrain.shape[0])\n",
    "print(\"Test Data set size:\",newCombineTest.shape[0])\n",
    "print(\"ValidData set size:\",validX_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class Dataset which returns the data and labels\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,features,lables,transform= None):\n",
    "        dataTensor= []\n",
    "        lableTensor= []\n",
    "        dataSize= features.shape[0]\n",
    "        \n",
    "        for data in range(dataSize):\n",
    "            feature= features[data,:]\n",
    "            #feature= torch.from_numpy(feature).float()\n",
    "            feature= torch.Tensor(feature)\n",
    "            dataTensor.append(feature)\n",
    "            \n",
    "            lable= np.asanyarray(lables[data])\n",
    "            lable= torch.from_numpy(lable).float()\n",
    "           \n",
    "            #lable= torch.Tensor(lable)\n",
    "            #print(\"lable:\",lable)\n",
    "            #assert(False)\n",
    "           \n",
    "            lableTensor.append(lable)\n",
    "        \n",
    "        #put everything in features and lables\n",
    "        self.features= dataTensor\n",
    "        self.lables= lableTensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        feature= self.features[index]\n",
    "        lable= self.lables[index]\n",
    "        #print(\"get_item feature:\",feature)\n",
    "        #print(\"get_item lable:\",lable)\n",
    "        return feature,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the for train and test loader\n",
    "myTrainDataset= myDataset(newCombineTrain,newCombineTrainLable)\n",
    "myTestDataset= myDataset(newCombineTest,newCombineTestLable)\n",
    "myValidDataset= myDataset(validX_train,validY_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the trainloader and test loader for nonfraudulent dataset.\n",
    "trainLoader= torch.utils.data.DataLoader(myTrainDataset,batch_size=32,shuffle=True,num_workers=0)\n",
    "testLoader= torch.utils.data.DataLoader(myTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "validLoader= torch.utils.data.DataLoader(myValidDataset,batch_size=32,shuffle=True,num_workers=0)\n",
    "\n",
    "newTrainLoader= torch.utils.data.DataLoader(myTrainDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.fc1= nn.Linear(85,64)\n",
    "        self.act1= nn.LeakyReLU()\n",
    "        self.fc2= nn.Linear(64,43)\n",
    "        self.act2= nn.LeakyReLU()\n",
    "        self.fc3= nn.Linear(43,24)\n",
    "        self.act3= nn.LeakyReLU()\n",
    "        self.fc4= nn.Linear(24,10)\n",
    "        self.act4= nn.LeakyReLU()\n",
    "        self.fc5= nn.Linear(10,2)\n",
    "        self.act5= nn.Sigmoid();\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x= self.fc1(x)\n",
    "        x= self.act1(x)\n",
    "        x= self.fc2(x)\n",
    "        x= self.act2(x)\n",
    "        x= self.fc3(x)\n",
    "        x= self.act3(x)\n",
    "        x= self.fc4(x)\n",
    "        x= self.act4(x)\n",
    "        x= self.fc5(x)\n",
    "        x= self.act5(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Network()\n",
    "#criterion= nn.CrossEntropyLoss()\n",
    "criterion= nn.BCELoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.0001,momentum=0.9,weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def validation Test\n",
    "def validTest(trainedModel,loader):\n",
    "    running_loss= 0.0\n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= trainedModel(inputs)\n",
    "        lables= torch.tensor(lable).float()\n",
    "        \n",
    "        newMaxProb,maxIndex = torch.topk(output,1,dim=1)\n",
    "        loss= criterion(newMaxProb,lables.squeeze())\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss/validX_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/usr/local/anaconda35/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/usr/local/anaconda35/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.026270\n",
      "[2] loss: 0.024704\n",
      "[3] loss: 0.023634\n",
      "[4] loss: 0.022674\n",
      "[5] loss: 0.021777\n",
      "[6] loss: 0.020937\n",
      "[7] loss: 0.020150\n",
      "[8] loss: 0.019410\n",
      "[9] loss: 0.018714\n",
      "[10] loss: 0.018059\n",
      "[11] loss: 0.017439\n",
      "[12] loss: 0.016855\n",
      "[13] loss: 0.016301\n",
      "[14] loss: 0.015776\n",
      "[15] loss: 0.015278\n",
      "[16] loss: 0.014802\n",
      "[17] loss: 0.014350\n",
      "[18] loss: 0.013918\n",
      "[19] loss: 0.013504\n",
      "[20] loss: 0.013110\n",
      "[21] loss: 0.012731\n",
      "[22] loss: 0.012368\n",
      "[23] loss: 0.012019\n",
      "[24] loss: 0.011684\n",
      "[25] loss: 0.011363\n",
      "[26] loss: 0.011052\n",
      "[27] loss: 0.010753\n",
      "[28] loss: 0.010467\n",
      "[29] loss: 0.010192\n",
      "[30] loss: 0.009932\n",
      "[31] loss: 0.009679\n",
      "[32] loss: 0.009438\n",
      "[33] loss: 0.009211\n",
      "[34] loss: 0.008999\n",
      "[35] loss: 0.008794\n",
      "[36] loss: 0.008604\n",
      "[37] loss: 0.008423\n",
      "[38] loss: 0.008262\n",
      "[39] loss: 0.008109\n",
      "[40] loss: 0.007969\n",
      "[41] loss: 0.007849\n",
      "[42] loss: 0.007739\n",
      "[43] loss: 0.007633\n",
      "[44] loss: 0.007546\n",
      "[45] loss: 0.007471\n",
      "[46] loss: 0.007406\n",
      "[47] loss: 0.007345\n",
      "[48] loss: 0.007301\n",
      "[49] loss: 0.007252\n",
      "[50] loss: 0.007219\n",
      "[51] loss: 0.007194\n",
      "[52] loss: 0.007163\n",
      "[53] loss: 0.007149\n",
      "[54] loss: 0.007128\n",
      "[55] loss: 0.007115\n",
      "[56] loss: 0.007108\n",
      "[57] loss: 0.007102\n",
      "[58] loss: 0.007088\n",
      "[59] loss: 0.007082\n",
      "[60] loss: 0.007076\n",
      "[61] loss: 0.007076\n",
      "[62] loss: 0.007066\n",
      "[63] loss: 0.007060\n",
      "[64] loss: 0.007059\n",
      "[65] loss: 0.007059\n",
      "[66] loss: 0.007056\n",
      "[67] loss: 0.007052\n",
      "[68] loss: 0.007052\n",
      "[69] loss: 0.007050\n",
      "[70] loss: 0.007046\n",
      "[71] loss: 0.007044\n",
      "[72] loss: 0.007045\n",
      "[73] loss: 0.007043\n",
      "[74] loss: 0.007039\n",
      "[75] loss: 0.007037\n",
      "[76] loss: 0.007036\n",
      "[77] loss: 0.007041\n",
      "[78] loss: 0.007035\n",
      "[79] loss: 0.007038\n",
      "[80] loss: 0.007030\n",
      "[81] loss: 0.007026\n",
      "[82] loss: 0.007029\n",
      "[83] loss: 0.007025\n",
      "[84] loss: 0.007030\n",
      "[85] loss: 0.007036\n",
      "[86] loss: 0.007029\n",
      "[87] loss: 0.007023\n",
      "[88] loss: 0.007019\n",
      "[89] loss: 0.007020\n",
      "[90] loss: 0.007016\n",
      "[91] loss: 0.007017\n",
      "[92] loss: 0.007011\n",
      "[93] loss: 0.007007\n",
      "[94] loss: 0.007005\n",
      "[95] loss: 0.007006\n",
      "[96] loss: 0.007004\n",
      "[97] loss: 0.007003\n",
      "[98] loss: 0.007001\n",
      "[99] loss: 0.007004\n",
      "[100] loss: 0.006997\n",
      "[101] loss: 0.006996\n",
      "[102] loss: 0.006992\n",
      "[103] loss: 0.006997\n",
      "[104] loss: 0.006991\n",
      "[105] loss: 0.006989\n",
      "[106] loss: 0.006985\n",
      "[107] loss: 0.006988\n",
      "[108] loss: 0.006989\n",
      "[109] loss: 0.006986\n",
      "[110] loss: 0.006980\n",
      "[111] loss: 0.006983\n",
      "[112] loss: 0.006979\n",
      "[113] loss: 0.006977\n",
      "[114] loss: 0.006975\n",
      "[115] loss: 0.006973\n",
      "[116] loss: 0.006967\n",
      "[117] loss: 0.006970\n",
      "[118] loss: 0.006966\n",
      "[119] loss: 0.006964\n",
      "[120] loss: 0.006966\n",
      "[121] loss: 0.006960\n",
      "[122] loss: 0.006956\n",
      "[123] loss: 0.006954\n",
      "[124] loss: 0.006954\n",
      "[125] loss: 0.006949\n",
      "[126] loss: 0.006954\n",
      "[127] loss: 0.006945\n",
      "[128] loss: 0.006943\n",
      "[129] loss: 0.006948\n",
      "[130] loss: 0.006941\n",
      "[131] loss: 0.006940\n",
      "[132] loss: 0.006933\n",
      "[133] loss: 0.006931\n",
      "[134] loss: 0.006933\n",
      "[135] loss: 0.006929\n",
      "[136] loss: 0.006928\n",
      "[137] loss: 0.006928\n",
      "[138] loss: 0.006924\n",
      "[139] loss: 0.006919\n",
      "[140] loss: 0.006917\n",
      "[141] loss: 0.006919\n",
      "[142] loss: 0.006914\n",
      "[143] loss: 0.006907\n",
      "[144] loss: 0.006907\n",
      "[145] loss: 0.006907\n",
      "[146] loss: 0.006902\n",
      "[147] loss: 0.006904\n",
      "[148] loss: 0.006899\n",
      "[149] loss: 0.006896\n",
      "[150] loss: 0.006892\n",
      "[151] loss: 0.006889\n",
      "[152] loss: 0.006887\n",
      "[153] loss: 0.006881\n",
      "[154] loss: 0.006879\n",
      "[155] loss: 0.006878\n",
      "[156] loss: 0.006875\n",
      "[157] loss: 0.006875\n",
      "[158] loss: 0.006868\n",
      "[159] loss: 0.006867\n",
      "[160] loss: 0.006864\n",
      "[161] loss: 0.006863\n",
      "[162] loss: 0.006856\n",
      "[163] loss: 0.006855\n",
      "[164] loss: 0.006857\n",
      "[165] loss: 0.006854\n",
      "[166] loss: 0.006846\n",
      "[167] loss: 0.006845\n",
      "[168] loss: 0.006838\n",
      "[169] loss: 0.006839\n",
      "[170] loss: 0.006836\n",
      "[171] loss: 0.006833\n",
      "[172] loss: 0.006832\n",
      "[173] loss: 0.006823\n",
      "[174] loss: 0.006818\n",
      "[175] loss: 0.006819\n",
      "[176] loss: 0.006813\n",
      "[177] loss: 0.006817\n",
      "[178] loss: 0.006811\n",
      "[179] loss: 0.006805\n",
      "[180] loss: 0.006801\n",
      "[181] loss: 0.006800\n",
      "[182] loss: 0.006794\n",
      "[183] loss: 0.006790\n",
      "[184] loss: 0.006784\n",
      "[185] loss: 0.006783\n",
      "[186] loss: 0.006779\n",
      "[187] loss: 0.006773\n",
      "[188] loss: 0.006771\n",
      "[189] loss: 0.006765\n",
      "[190] loss: 0.006758\n",
      "[191] loss: 0.006763\n",
      "[192] loss: 0.006755\n",
      "[193] loss: 0.006753\n",
      "[194] loss: 0.006744\n",
      "[195] loss: 0.006742\n",
      "[196] loss: 0.006735\n",
      "[197] loss: 0.006733\n",
      "[198] loss: 0.006727\n",
      "[199] loss: 0.006720\n",
      "[200] loss: 0.006720\n",
      "[201] loss: 0.006713\n",
      "[202] loss: 0.006712\n",
      "[203] loss: 0.006710\n",
      "[204] loss: 0.006701\n",
      "[205] loss: 0.006696\n",
      "[206] loss: 0.006689\n",
      "[207] loss: 0.006695\n",
      "[208] loss: 0.006680\n",
      "[209] loss: 0.006673\n",
      "[210] loss: 0.006670\n",
      "[211] loss: 0.006665\n",
      "[212] loss: 0.006657\n",
      "[213] loss: 0.006655\n",
      "[214] loss: 0.006647\n",
      "[215] loss: 0.006644\n",
      "[216] loss: 0.006641\n",
      "[217] loss: 0.006631\n",
      "[218] loss: 0.006630\n",
      "[219] loss: 0.006625\n",
      "[220] loss: 0.006620\n",
      "[221] loss: 0.006609\n",
      "[222] loss: 0.006611\n",
      "[223] loss: 0.006601\n",
      "[224] loss: 0.006595\n",
      "[225] loss: 0.006594\n",
      "[226] loss: 0.006588\n",
      "[227] loss: 0.006576\n",
      "[228] loss: 0.006573\n",
      "[229] loss: 0.006567\n",
      "[230] loss: 0.006559\n",
      "[231] loss: 0.006553\n",
      "[232] loss: 0.006552\n",
      "[233] loss: 0.006542\n",
      "[234] loss: 0.006538\n",
      "[235] loss: 0.006530\n",
      "[236] loss: 0.006527\n",
      "[237] loss: 0.006521\n",
      "[238] loss: 0.006515\n",
      "[239] loss: 0.006514\n",
      "[240] loss: 0.006505\n",
      "[241] loss: 0.006500\n",
      "[242] loss: 0.006498\n",
      "[243] loss: 0.006489\n",
      "[244] loss: 0.006479\n",
      "[245] loss: 0.006482\n",
      "[246] loss: 0.006469\n",
      "[247] loss: 0.006463\n",
      "[248] loss: 0.006459\n",
      "[249] loss: 0.006449\n",
      "[250] loss: 0.006443\n",
      "[251] loss: 0.006445\n",
      "[252] loss: 0.006436\n",
      "[253] loss: 0.006428\n",
      "[254] loss: 0.006420\n",
      "[255] loss: 0.006418\n",
      "[256] loss: 0.006413\n",
      "[257] loss: 0.006404\n",
      "[258] loss: 0.006405\n",
      "[259] loss: 0.006393\n",
      "[260] loss: 0.006385\n",
      "[261] loss: 0.006380\n",
      "[262] loss: 0.006374\n",
      "[263] loss: 0.006371\n",
      "[264] loss: 0.006363\n",
      "[265] loss: 0.006358\n",
      "[266] loss: 0.006355\n",
      "[267] loss: 0.006347\n",
      "[268] loss: 0.006346\n",
      "[269] loss: 0.006340\n",
      "[270] loss: 0.006333\n",
      "[271] loss: 0.006329\n",
      "[272] loss: 0.006322\n",
      "[273] loss: 0.006318\n",
      "[274] loss: 0.006310\n",
      "[275] loss: 0.006309\n",
      "[276] loss: 0.006302\n",
      "[277] loss: 0.006300\n",
      "[278] loss: 0.006291\n",
      "[279] loss: 0.006288\n",
      "[280] loss: 0.006281\n",
      "[281] loss: 0.006280\n",
      "[282] loss: 0.006273\n",
      "[283] loss: 0.006271\n",
      "[284] loss: 0.006267\n",
      "[285] loss: 0.006257\n",
      "[286] loss: 0.006255\n",
      "[287] loss: 0.006251\n",
      "[288] loss: 0.006245\n",
      "[289] loss: 0.006244\n",
      "[290] loss: 0.006238\n",
      "[291] loss: 0.006235\n",
      "[292] loss: 0.006226\n",
      "[293] loss: 0.006224\n",
      "[294] loss: 0.006222\n",
      "[295] loss: 0.006219\n",
      "[296] loss: 0.006215\n",
      "[297] loss: 0.006209\n",
      "[298] loss: 0.006204\n",
      "[299] loss: 0.006202\n",
      "[300] loss: 0.006199\n",
      "[301] loss: 0.006190\n",
      "[302] loss: 0.006191\n",
      "[303] loss: 0.006183\n",
      "[304] loss: 0.006180\n",
      "[305] loss: 0.006181\n",
      "[306] loss: 0.006182\n",
      "[307] loss: 0.006169\n",
      "[308] loss: 0.006167\n",
      "[309] loss: 0.006163\n",
      "[310] loss: 0.006168\n",
      "[311] loss: 0.006159\n",
      "[312] loss: 0.006159\n",
      "[313] loss: 0.006150\n",
      "[314] loss: 0.006146\n",
      "[315] loss: 0.006146\n",
      "[316] loss: 0.006141\n",
      "[317] loss: 0.006140\n",
      "[318] loss: 0.006145\n",
      "[319] loss: 0.006132\n",
      "[320] loss: 0.006131\n",
      "[321] loss: 0.006134\n",
      "[322] loss: 0.006130\n",
      "[323] loss: 0.006121\n",
      "[324] loss: 0.006124\n",
      "[325] loss: 0.006120\n",
      "[326] loss: 0.006114\n",
      "[327] loss: 0.006113\n",
      "[328] loss: 0.006114\n",
      "[329] loss: 0.006108\n",
      "[330] loss: 0.006104\n",
      "[331] loss: 0.006104\n",
      "[332] loss: 0.006101\n",
      "[333] loss: 0.006099\n",
      "[334] loss: 0.006101\n",
      "[335] loss: 0.006093\n",
      "[336] loss: 0.006093\n",
      "[337] loss: 0.006091\n",
      "[338] loss: 0.006088\n",
      "[339] loss: 0.006091\n",
      "[340] loss: 0.006084\n",
      "[341] loss: 0.006086\n",
      "[342] loss: 0.006081\n",
      "[343] loss: 0.006080\n",
      "[344] loss: 0.006074\n",
      "[345] loss: 0.006075\n",
      "[346] loss: 0.006074\n",
      "[347] loss: 0.006068\n",
      "[348] loss: 0.006066\n",
      "[349] loss: 0.006069\n",
      "[350] loss: 0.006066\n",
      "[351] loss: 0.006063\n",
      "[352] loss: 0.006066\n",
      "[353] loss: 0.006062\n",
      "[354] loss: 0.006057\n",
      "[355] loss: 0.006055\n",
      "[356] loss: 0.006054\n",
      "[357] loss: 0.006052\n",
      "[358] loss: 0.006054\n",
      "[359] loss: 0.006049\n",
      "[360] loss: 0.006053\n",
      "[361] loss: 0.006047\n",
      "[362] loss: 0.006050\n",
      "[363] loss: 0.006044\n",
      "[364] loss: 0.006041\n",
      "[365] loss: 0.006045\n",
      "[366] loss: 0.006042\n",
      "[367] loss: 0.006035\n",
      "[368] loss: 0.006039\n",
      "[369] loss: 0.006032\n",
      "[370] loss: 0.006034\n",
      "[371] loss: 0.006032\n",
      "[372] loss: 0.006029\n",
      "[373] loss: 0.006030\n",
      "[374] loss: 0.006028\n",
      "[375] loss: 0.006025\n",
      "[376] loss: 0.006025\n",
      "[377] loss: 0.006024\n",
      "[378] loss: 0.006021\n",
      "[379] loss: 0.006026\n",
      "[380] loss: 0.006020\n",
      "[381] loss: 0.006017\n",
      "[382] loss: 0.006018\n",
      "[383] loss: 0.006015\n",
      "[384] loss: 0.006018\n",
      "[385] loss: 0.006012\n",
      "[386] loss: 0.006012\n",
      "[387] loss: 0.006011\n",
      "[388] loss: 0.006010\n",
      "[389] loss: 0.006009\n",
      "[390] loss: 0.006004\n",
      "[391] loss: 0.006004\n",
      "[392] loss: 0.006003\n",
      "[393] loss: 0.006003\n",
      "[394] loss: 0.006001\n",
      "[395] loss: 0.006001\n",
      "[396] loss: 0.006009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[397] loss: 0.005997\n",
      "[398] loss: 0.005994\n",
      "[399] loss: 0.005994\n",
      "[400] loss: 0.005995\n",
      "[401] loss: 0.005992\n",
      "[402] loss: 0.005997\n",
      "[403] loss: 0.005989\n",
      "[404] loss: 0.005989\n",
      "[405] loss: 0.005991\n",
      "[406] loss: 0.005991\n",
      "[407] loss: 0.005988\n",
      "[408] loss: 0.005988\n",
      "[409] loss: 0.005985\n",
      "[410] loss: 0.005982\n",
      "[411] loss: 0.005981\n",
      "[412] loss: 0.005978\n",
      "[413] loss: 0.005979\n",
      "[414] loss: 0.005979\n",
      "[415] loss: 0.005977\n",
      "[416] loss: 0.005981\n",
      "[417] loss: 0.005981\n",
      "[418] loss: 0.005981\n",
      "[419] loss: 0.005974\n",
      "[420] loss: 0.005971\n",
      "[421] loss: 0.005972\n",
      "[422] loss: 0.005969\n",
      "[423] loss: 0.005970\n",
      "[424] loss: 0.005970\n",
      "[425] loss: 0.005971\n",
      "[426] loss: 0.005965\n",
      "[427] loss: 0.005965\n",
      "[428] loss: 0.005963\n",
      "[429] loss: 0.005963\n",
      "[430] loss: 0.005960\n",
      "[431] loss: 0.005964\n",
      "[432] loss: 0.005960\n",
      "[433] loss: 0.005961\n",
      "[434] loss: 0.005957\n",
      "[435] loss: 0.005957\n",
      "[436] loss: 0.005956\n",
      "[437] loss: 0.005953\n",
      "[438] loss: 0.005953\n",
      "[439] loss: 0.005953\n",
      "[440] loss: 0.005953\n",
      "[441] loss: 0.005949\n",
      "[442] loss: 0.005961\n",
      "[443] loss: 0.005953\n",
      "[444] loss: 0.005948\n",
      "[445] loss: 0.005947\n",
      "[446] loss: 0.005944\n",
      "[447] loss: 0.005947\n",
      "[448] loss: 0.005944\n",
      "[449] loss: 0.005952\n",
      "[450] loss: 0.005946\n",
      "[451] loss: 0.005940\n",
      "[452] loss: 0.005942\n",
      "[453] loss: 0.005941\n",
      "[454] loss: 0.005941\n",
      "[455] loss: 0.005936\n",
      "[456] loss: 0.005934\n",
      "[457] loss: 0.005937\n",
      "[458] loss: 0.005933\n",
      "[459] loss: 0.005938\n",
      "[460] loss: 0.005937\n",
      "[461] loss: 0.005937\n",
      "[462] loss: 0.005936\n",
      "[463] loss: 0.005928\n",
      "[464] loss: 0.005936\n",
      "[465] loss: 0.005931\n",
      "[466] loss: 0.005930\n",
      "[467] loss: 0.005927\n",
      "[468] loss: 0.005926\n",
      "[469] loss: 0.005925\n",
      "[470] loss: 0.005927\n",
      "[471] loss: 0.005921\n",
      "[472] loss: 0.005922\n",
      "[473] loss: 0.005920\n",
      "[474] loss: 0.005921\n",
      "[475] loss: 0.005920\n",
      "[476] loss: 0.005917\n",
      "[477] loss: 0.005923\n",
      "[478] loss: 0.005918\n",
      "[479] loss: 0.005917\n",
      "[480] loss: 0.005915\n",
      "[481] loss: 0.005917\n",
      "[482] loss: 0.005913\n",
      "[483] loss: 0.005913\n",
      "[484] loss: 0.005911\n",
      "[485] loss: 0.005912\n",
      "[486] loss: 0.005914\n",
      "[487] loss: 0.005913\n",
      "[488] loss: 0.005906\n",
      "[489] loss: 0.005909\n",
      "[490] loss: 0.005912\n",
      "[491] loss: 0.005908\n",
      "[492] loss: 0.005906\n",
      "[493] loss: 0.005903\n",
      "[494] loss: 0.005905\n",
      "[495] loss: 0.005903\n",
      "[496] loss: 0.005899\n",
      "[497] loss: 0.005902\n",
      "[498] loss: 0.005899\n",
      "[499] loss: 0.005898\n",
      "[500] loss: 0.005900\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "training_loss= []\n",
    "validation_loss= []\n",
    "for epoch in range(500):\n",
    "    running_loss= 0.0\n",
    "    for i, (feature,lable) in enumerate(trainLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs= torch.tensor(feature)\n",
    "        lables= torch.tensor(lable).float()\n",
    "        #lables= lable.type(torch.LongTensor)\n",
    "        #print(\"lables:\",lables)\n",
    "        #print(\"lables.size:\",lables.size())\n",
    "       \n",
    "        # =====================forward====================\n",
    "        output = model(inputs)\n",
    "        \n",
    "        #print(\"output:\",output)\n",
    "        #print(\"output.size:\",output.size())\n",
    "        \n",
    "        #print(\"output.requires_grad:\",output.requires_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        maxIndex= torch.argmax(output,dim=1)\n",
    "        maxProb= []\n",
    "        tensorLength= output.size(0)\n",
    "        print(\"maxIndex:\",maxIndex)\n",
    "        \n",
    "        for i in range(tensorLength):\n",
    "            index= maxIndex[i]\n",
    "            maxProb.append(output[i][index].item())\n",
    "        newMaxProb= torch.tensor(maxProb)\n",
    "        newMaxProb= newMaxProb.view(-1,1)\n",
    "        \"\"\"\n",
    "        \n",
    "        newMaxProb,_ = torch.topk(output,1,dim=1)\n",
    "        #print(\"newMaxProb:\",newMaxProb)\n",
    "        #print(\"newMaxProb.size:\",newMaxProb.size())\n",
    "        #print(\"newMaxProb.requires_grad:\",newMaxProb.requires_grad)\n",
    "                        \n",
    "        #loss= criterion(output,lables.squeeze())\n",
    "        #output= newMaxProb\n",
    "        loss= criterion(newMaxProb,lables.squeeze())\n",
    "        \n",
    "        #print(\"loss:\",loss)\n",
    "        \n",
    "        # ===================backward====================\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # =======print the statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('[%d] loss: %.6f' %(epoch + 1,  running_loss /newCombineTrain.shape[0]))\n",
    "    validLoss= validTest(model,validLoader)\n",
    "    validation_loss.append(validLoss)\n",
    "    \n",
    "    training_loss.append(running_loss/newCombineTrain.shape[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025526988694048302"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loss)\n",
    "validation_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingLoss= pd.DataFrame({'Loss':training_loss})\n",
    "validLoss= pd.DataFrame({'Loss':validation_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingLoss.to_csv('trainLoss.csv',index=None,sep=' ')\n",
    "validLoss.to_csv('validLoss.csv',index=None,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the loss\n",
    "len(training_loss)\n",
    "lossBatch32= pd.DataFrame({'Loss':training_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the model:\n",
    "#function for testing the model\n",
    "def testModel(loader):\n",
    "    predictedLabel= []\n",
    "    maxList= []\n",
    "    trueLable= []\n",
    "    maxProb= []\n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model(inputs)\n",
    "        newMaxProb,maxIndex = torch.topk(output,1,dim=1)\n",
    "        \n",
    "        trueLable.append(lable)\n",
    "        \n",
    "        if(newMaxProb>0.06):\n",
    "            predictedLabel.append(1)\n",
    "        else:\n",
    "            predictedLabel.append(0)\n",
    "        \n",
    "    \n",
    "    return predictedLabel,trueLable,maxProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on test set\n",
    "predictedLable,trueLable,maxList= testModel(testLoader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "TP: 256\n",
      "\n",
      "FN: 21\n",
      "\n",
      "FP: 1692\n",
      "\n",
      "TN: 2657\n",
      "\n",
      "Result \n",
      "Accuracy: 62.970168612191955\n",
      "Sensitivity: 92.4187725631769\n",
      "Specificity: 61.09450448378938\n"
     ]
    }
   ],
   "source": [
    "totalLength= len(trueLable)\n",
    "TP=FP=FN=TN= 0\n",
    "for i in range(totalLength):\n",
    "    if(int(trueLable[i])==1 and predictedLable[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable[i])==1 and predictedLable[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable[i])==0 and predictedLable[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable[i])==0 and predictedLable[i]==1):\n",
    "        FP += 1\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\nTP:\",TP)\n",
    "print(\"\\nFN:\",FN)\n",
    "print(\"\\nFP:\",FP)\n",
    "print(\"\\nTN:\",TN)\n",
    "\n",
    "print(\"\\nResult \")\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for the ranking algorithm to test whether the model is correct or not.\n",
    "def ranking(loader):\n",
    "    listDic= []        \n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model(inputs)\n",
    "        newMaxProb,maxIndex = torch.topk(output,1,dim=1)\n",
    "        #probabilityDic[output.item()]= lable.item()\n",
    "        listDic.append((newMaxProb,lable.item()))\n",
    "    return listDic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dictionary: 8634\n"
     ]
    }
   ],
   "source": [
    "probabilityDic= ranking(newTrainLoader)\n",
    "\n",
    "print(\"Length of Dictionary:\",len(probabilityDic))\n",
    "\n",
    "#sort the list\n",
    "probabilityDic.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(probabilityDic[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newCombineTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the 10 buckets of equal size except 1\n",
    "bucket1=[]\n",
    "bucket2=[]\n",
    "bucket3=[]\n",
    "bucket4=[]\n",
    "bucket5=[]\n",
    "bucket6=[]\n",
    "bucket7=[]\n",
    "bucket8=[]\n",
    "bucket9=[]\n",
    "bucket10=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(probabilityDic)):\n",
    "    if(i<1082):\n",
    "        bucket1.append(int(probabilityDic[i][1]))\n",
    "    elif(i<2161):\n",
    "        bucket2.append(int(probabilityDic[i][1]))\n",
    "    elif(i<3240):\n",
    "        bucket3.append(int(probabilityDic[i][1]))\n",
    "    elif(i<4391):\n",
    "        bucket4.append(int(probabilityDic[i][1]))\n",
    "    elif(i<5398):\n",
    "        bucket5.append(int(probabilityDic[i][1]))\n",
    "    elif(i<6477):\n",
    "        bucket6.append(int(probabilityDic[i][1]))\n",
    "    elif(i<7556):\n",
    "        bucket7.append(int(probabilityDic[i][1]))\n",
    "    elif(i<8635):\n",
    "        bucket8.append(int(probabilityDic[i][1]))\n",
    "    elif(i<9714):\n",
    "        bucket9.append(int(probabilityDic[i][1]))\n",
    "    else:\n",
    "        bucket10.append(int(probabilityDic[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countOnesInBucket(bucket1,bucket2,bucket3,bucket4,bucket5,\n",
    "                     bucket6,bucket7,bucket8,bucket9,bucket10):\n",
    "    countOne= []\n",
    "    countOne.append(bucket1.count(1))\n",
    "    countOne.append(bucket2.count(1))\n",
    "    countOne.append(bucket3.count(1))\n",
    "    countOne.append(bucket4.count(1))\n",
    "    countOne.append(bucket5.count(1))\n",
    "    countOne.append(bucket6.count(1))\n",
    "    countOne.append(bucket7.count(1))\n",
    "    countOne.append(bucket8.count(1))\n",
    "    countOne.append(bucket9.count(1))\n",
    "    countOne.append(bucket10.count(1))\n",
    "    \n",
    "    countOne.sort()\n",
    "    return countOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual= countOnesInBucket(bucket1,bucket2,bucket3,bucket4,bucket5,\n",
    "                     bucket6,bucket7,bucket8,bucket9,bucket10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket actual: [0, 0, 3, 4, 6, 7, 59, 112, 151, 175]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bucket actual:\",actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalFraud: 277\n",
      "TotalNonFraud: 4349\n"
     ]
    }
   ],
   "source": [
    "#make the test set also balanced inorder to calculate the cost\n",
    "totalFraud= trueLable.count(1)\n",
    "totalNonFraud= trueLable.count(0)\n",
    "print(\"TotalFraud:\",totalFraud)\n",
    "print(\"TotalNonFraud:\",totalNonFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of FalseAlarm: 0.9401210549070471\n",
      "Weight of misses: 0.05987894509295288\n"
     ]
    }
   ],
   "source": [
    "#weight for fradulent and non-fraudulent\n",
    "weightFalseAlaram= totalNonFraud/(totalFraud + totalNonFraud)\n",
    "weightMisses= (totalFraud)/(totalFraud + totalNonFraud)\n",
    "\n",
    "print(\"Weight of FalseAlarm:\",weightFalseAlaram)\n",
    "print(\"Weight of misses:\",weightMisses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under sample the newCostTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of FraudMissClassify: 21\n",
      "Number of nonfraudMissClassify: 1692\n",
      "totalFraud Case: 277\n",
      "totalNonFraud Case: 4349\n",
      "Cost of false alarm: 56128.04734111543\n",
      "Cost of misses: 267472.0622568093\n",
      "totalCost of the model: 323600.10959792475\n"
     ]
    }
   ],
   "source": [
    "#cost model\n",
    "#Avg cost per Claim: USD 2640\n",
    "#Avg Cost per Investigation: $203\n",
    "\n",
    "\n",
    "totalFraudCase= TP+FN\n",
    "totalNonFraudCase= TN+FP\n",
    "\n",
    "#initialize the cost factor\n",
    "avgCostPerClaim= 2640\n",
    "avgCostperInvestigation= 203\n",
    "\n",
    "falseAlarm= weightFalseAlaram * FN * (avgCostPerClaim + avgCostperInvestigation) #costNonFraudMissClassify\n",
    "misses= weightMisses * FP * avgCostPerClaim    #costFraudMissClassify\n",
    "\n",
    "\n",
    "\n",
    "totalCost= falseAlarm + misses\n",
    "\n",
    "print(\"No of FraudMissClassify:\",FN)\n",
    "print(\"Number of nonfraudMissClassify:\",FP)\n",
    "print(\"totalFraud Case:\", totalFraudCase)\n",
    "print(\"totalNonFraud Case:\", totalNonFraudCase)\n",
    "print(\"Cost of false alarm:\",falseAlarm)\n",
    "print(\"Cost of misses:\",misses)\n",
    "\n",
    "print(\"totalCost of the model:\",totalCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this model\n",
    "\n",
    "torch.save(model.state_dict(),'./bestAnnPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load  the model\n",
    "loadedModel= Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel.load_state_dict(torch.load(\"./bestAnnPath\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
