{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the data into dataframe\n",
    "car_df = pd.read_csv('newCardata.csv')\n",
    "car_features = pd.read_csv('finalDataPreprocessBinary.csv')\n",
    "car_label = car_df['FraudFound']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15419,)\n",
      "xtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "Original dataset shape Counter({0: 14496, 1: 923})\n",
      "(11564,)\n",
      "(3855,)\n",
      "<class 'numpy.ndarray'>\n",
      "Majority Count1:  10887\n",
      "Minority Count1:  677\n",
      "Majority Count2:  3609\n",
      "Minority Count2:  246\n"
     ]
    }
   ],
   "source": [
    "#change the label of the data\n",
    "labelNo = LabelEncoder()\n",
    "car_df['FraudFound'] = labelNo.fit_transform(car_df['FraudFound'].astype('str'))\n",
    "car_label = car_df['FraudFound']\n",
    "print(car_label.shape)\n",
    "\n",
    "#split the data into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(car_features,car_label,random_state=3,test_size=0.25)\n",
    "print('xtrain:',type(X_train))\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(car_label)))\n",
    "\n",
    "#find the count of majority and minority\n",
    "maj_count1 = 0\n",
    "min_count1 = 0\n",
    "maj_count2 = 0\n",
    "min_count2 = 0\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train = pd.Series(y_train).values\n",
    "y_test = pd.Series(y_test).values\n",
    "print(type(y_train))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    if(y_train[i]==0):\n",
    "        maj_count1 = maj_count1 + 1\n",
    "    else:\n",
    "        min_count1 = min_count1 + 1\n",
    "\n",
    "for j in range(y_test.shape[0]):\n",
    "    if(y_test[j]==0):\n",
    "        maj_count2 += 1\n",
    "    else:\n",
    "        min_count2 += 1\n",
    "\n",
    "print(\"Majority Count1: \",maj_count1)\n",
    "print(\"Minority Count1: \",min_count1)\n",
    "print(\"Majority Count2: \",maj_count2)\n",
    "print(\"Minority Count2: \",min_count2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier:\n",
      "<class 'numpy.ndarray'> (3855,) [0 0 0 ..., 0 0 0]\n",
      "Accuracy is  93.7\n"
     ]
    }
   ],
   "source": [
    "#model object\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train,y_train)\n",
    "print('Random forest classifier:')\n",
    "predicted = model.predict(X_test)\n",
    "print(type(predicted),predicted.shape,predicted)\n",
    "print('Accuracy is ',round(accuracy_score(y_test,model.predict(X_test)) * 100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[3606    3]\n",
      " [ 240    6]]\n",
      "TN: 3606\n",
      "FP: 3\n",
      "FN: 240\n",
      "TP: 6\n",
      "Accuracy: 93.6964980545\n",
      "Sensitivity: 2.43902439024\n",
      "Specificity: 99.9168744805\n"
     ]
    }
   ],
   "source": [
    "# calculating specifity and sensitivity\n",
    "# 0  := Negative\n",
    "# 1 := Positive\n",
    "cm = confusion_matrix(y_test,predicted)\n",
    "print(\"Confusion Matrix:\\n\",cm)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(\"TN:\",TN)\n",
    "print(\"FP:\",FP)\n",
    "print(\"FN:\",FN)\n",
    "print(\"TP:\",TP)\n",
    "\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n",
      "3855\n",
      "No of fraud cases: 246\n",
      "No of fauld cases predicted by model: 9\n",
      "No of fraud correctly predicted  as fault: 6\n"
     ]
    }
   ],
   "source": [
    "#converting pandas.core.series.Series to numpy.ndarray\n",
    "print(type(y_test),type(predicted))\n",
    "ytest = pd.Series(y_test).values\n",
    "print((ytest.shape[0]))\n",
    "\n",
    "#find the index where both are 1.\n",
    "count = 0\n",
    "fault = 0\n",
    "predictedfault = 0\n",
    "for i in range(predicted.shape[0]):\n",
    "    if((predicted[i] == 1) and (ytest[i] == 1)):\n",
    "        count += 1\n",
    "    if(ytest[i] == 1):\n",
    "        fault += 1\n",
    "    if(predicted[i]==1):\n",
    "        predictedfault += 1\n",
    "print(\"No of fraud cases:\",fault)\n",
    "print(\"No of fauld cases predicted by model:\",predictedfault)\n",
    "print(\"No of fraud correctly predicted  as fault:\",count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 14496, 1: 923})\n",
      "Resampled dataset shape Counter({0: 923, 1: 923})\n",
      "Majority Count:  923\n",
      "Minority Count:  923\n"
     ]
    }
   ],
   "source": [
    "#with Random Under Sampling\n",
    "#rus = RandomUnderSampler(ratio={0:14496,0:923},random_state=42)\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(car_label)))\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42,replacement=False)\n",
    "X_resampled,Y_resampled = rus.fit_sample(car_features,car_label)\n",
    "print('Resampled dataset shape {}'.format(Counter(Y_resampled)))\n",
    "\n",
    "maj_count = 0\n",
    "min_count = 0\n",
    "for i in range(Y_resampled.shape[0]):\n",
    "    if(Y_resampled[i]==0):\n",
    "        maj_count += 1\n",
    "    else:\n",
    "        min_count += 1\n",
    "print(\"Majority Count: \",maj_count)\n",
    "print(\"Minority Count: \",min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1292,)\n",
      "(554,)\n",
      "Majority Count1:  638\n",
      "Minority Count1:  654\n",
      "Majority Count2:  285\n",
      "Minority Count2:  269\n"
     ]
    }
   ],
   "source": [
    "#divide the trainin set and test set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_resampled,Y_resampled,test_size=0.3)\n",
    "\n",
    "print(type(y_train))\n",
    "#find the count of majority and minority\n",
    "maj_count1 = 0\n",
    "min_count1 = 0\n",
    "maj_count2 = 0\n",
    "min_count2 = 0\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "for i in range(y_train.shape[0]):\n",
    "    if(y_train[i]==0):\n",
    "        maj_count1 += 1\n",
    "    else:\n",
    "        min_count1 += 1\n",
    "for j in range(y_test.shape[0]):\n",
    "    if(y_test[j]==0):\n",
    "        maj_count2 += 1\n",
    "    else:\n",
    "        min_count2 += 1\n",
    "\n",
    "print(\"Majority Count1: \",maj_count1)\n",
    "print(\"Minority Count1: \",min_count1)\n",
    "print(\"Majority Count2: \",maj_count2)\n",
    "print(\"Minority Count2: \",min_count2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier:\n",
      "Accuracy is  76.71\n"
     ]
    }
   ],
   "source": [
    "#model object\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train,y_train)\n",
    "print('Random forest classifier:')\n",
    "predicted = model.predict(X_test)\n",
    "print('Accuracy is ',round(accuracy_score(y_test,model.predict(X_test)) * 100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[189  96]\n",
      " [ 33 236]]\n",
      "Accuracy: 76.714801444\n",
      "Sensitivity: 87.7323420074\n",
      "Specificity: 66.3157894737\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,predicted)\n",
    "print(\"Confusion Matrix:\\n\",cm)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of fraud cases: 269\n",
      "No of fauld cases predicted by model: 332\n",
      "No of fraud correctly predicted  as fault: 236\n"
     ]
    }
   ],
   "source": [
    "#converting pandas.core.series.Series to numpy.ndarray\n",
    "ytest = pd.Series(y_test).values\n",
    "#find the index where both are 1.\n",
    "count = 0\n",
    "fault = 0\n",
    "predictedfault = 0\n",
    "for i in range(predicted.shape[0]):\n",
    "    \n",
    "    if((predicted[i] == 1) and (ytest[i] == 1)):\n",
    "        count += 1\n",
    "    if(ytest[i] == 1):\n",
    "        fault += 1\n",
    "    if(predicted[i]==1):\n",
    "        predictedfault += 1\n",
    "print(\"No of fraud cases:\",fault)\n",
    "print(\"No of fauld cases predicted by model:\",predictedfault)\n",
    "print(\"No of fraud correctly predicted  as fault:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7831263584928061, 0.76714801444043323, 0.76488526783716559, None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(y_test, predicted, average='micro')\n",
    "precision_recall_fscore_support(y_test, predicted, average='macro')\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier:\n",
      "Resampled dataset shape Counter({0: 923, 1: 923})\n"
     ]
    }
   ],
   "source": [
    "#cross-validation 10-FOLD cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "print('Random forest classifier:')\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42,replacement=False)\n",
    "X_resampled,Y_resampled = rus.fit_sample(car_features,car_label)\n",
    "print('Resampled dataset shape {}'.format(Counter(Y_resampled)))\n",
    "\n",
    "#convert np.array to dataframe \n",
    "df_features = pd.DataFrame(X_resampled)\n",
    "df_lables = pd.DataFrame(Y_resampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.690846063455\n",
      "0.503278688525\n",
      "0.488683901293\n",
      "0.443461683387\n"
     ]
    }
   ],
   "source": [
    "#add the both the labels and features\n",
    "df_features['FraudFound']= df_lables\n",
    "\n",
    "#shuffle the featrues\n",
    "new_combine_features = df_features.set_index(np.random.permutation(df_features.index))\n",
    "\n",
    "\n",
    "new_label = new_combine_features['FraudFound']\n",
    "\n",
    "#drop the FraudFound attribute\n",
    "new_combine_features.drop(['FraudFound'],inplace=True,axis=1)\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=10)\n",
    "model=RandomForestClassifier(n_estimators=100) \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,X=new_combine_features,y=new_label,cv=kfold,scoring=scoring)\n",
    "\n",
    "print(np.mean(results['test_accuracy']))\n",
    "print(np.mean(results['test_precision']))\n",
    "print(np.mean(results['test_recall']))\n",
    "print(np.mean(results['test_f1_score']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
