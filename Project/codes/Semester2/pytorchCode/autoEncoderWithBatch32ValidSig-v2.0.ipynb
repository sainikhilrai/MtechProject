{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make_Accura</th>\n",
       "      <th>Make_BMW</th>\n",
       "      <th>Make_Chevrolet</th>\n",
       "      <th>Make_Dodge</th>\n",
       "      <th>Make_Ferrari</th>\n",
       "      <th>Make_Ford</th>\n",
       "      <th>Make_Honda</th>\n",
       "      <th>Make_Jaguar</th>\n",
       "      <th>Make_Lexus</th>\n",
       "      <th>Make_Mazda</th>\n",
       "      <th>...</th>\n",
       "      <th>NumberOfCars_5 to 8</th>\n",
       "      <th>NumberOfCars_more than 8</th>\n",
       "      <th>BasePolicy_All Perils</th>\n",
       "      <th>BasePolicy_Collision</th>\n",
       "      <th>BasePolicy_Liability</th>\n",
       "      <th>Age</th>\n",
       "      <th>Deductible</th>\n",
       "      <th>DriverRating</th>\n",
       "      <th>DaysDiff</th>\n",
       "      <th>Lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Make_Accura  Make_BMW  Make_Chevrolet  Make_Dodge  Make_Ferrari  Make_Ford  \\\n",
       "0            0         0               0           0             0          0   \n",
       "1            0         0               0           0             0          0   \n",
       "2            0         0               0           0             0          0   \n",
       "3            0         0               0           0             0          0   \n",
       "4            0         0               0           0             0          0   \n",
       "\n",
       "   Make_Honda  Make_Jaguar  Make_Lexus  Make_Mazda  ...    \\\n",
       "0           1            0           0           0  ...     \n",
       "1           1            0           0           0  ...     \n",
       "2           1            0           0           0  ...     \n",
       "3           0            0           0           0  ...     \n",
       "4           1            0           0           0  ...     \n",
       "\n",
       "   NumberOfCars_5 to 8  NumberOfCars_more than 8  BasePolicy_All Perils  \\\n",
       "0                    0                         0                      0   \n",
       "1                    0                         0                      0   \n",
       "2                    0                         0                      0   \n",
       "3                    0                         0                      0   \n",
       "4                    0                         0                      0   \n",
       "\n",
       "   BasePolicy_Collision  BasePolicy_Liability     Age  Deductible  \\\n",
       "0                     0                     1  0.2625    0.000000   \n",
       "1                     1                     0  0.4250    1.000000   \n",
       "2                     1                     0  0.5875    0.666667   \n",
       "3                     0                     1  0.8125    0.333333   \n",
       "4                     1                     0  0.3375    0.000000   \n",
       "\n",
       "   DriverRating  DaysDiff  Lable  \n",
       "0      0.000000  0.071795      0  \n",
       "1      1.000000  0.069231      0  \n",
       "2      0.666667  0.089744      0  \n",
       "3      0.333333  0.107692      0  \n",
       "4      0.000000  0.094872      0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carDf= pd.read_csv('oneHotPreprocess.csv')\n",
    "carDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the lable from the dataset\n",
    "carDf.head()\n",
    "carLable= carDf['Lable']\n",
    "carDf.drop(['Lable'],inplace=True,axis=1) #drop the lable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "carDateNormalized= carDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudulent: <class 'pandas.core.frame.DataFrame'>\n",
      "carDateNormalized: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Divide the data into fraudulent and non-fraudulent data\n",
    "columnlable= list(carDateNormalized.columns.values)\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "nonFraudulent= pd.DataFrame(columns=columnlable)\n",
    "nonFraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "fraudulent= pd.DataFrame(columns=columnlable)\n",
    "fraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "print(\"nonFraudulent:\",type(nonFraudulent))\n",
    "print(\"carDateNormalized:\",type(carDateNormalized))\n",
    "\n",
    "j= 0\n",
    "k= 0\n",
    "for i in range(carDateNormalized.shape[0]):\n",
    "    if(carLable[i]==0):\n",
    "        nonFraudulent.loc[j]= carDateNormalized.loc[i]\n",
    "        nonFraudulentLable.loc[j]= 0.0\n",
    "        j += 1\n",
    "    else:\n",
    "        fraudulent.loc[k]= carDateNormalized.loc[i]\n",
    "        fraudulentLable.loc[i]= 1.0\n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nonFraudulent))\n",
    "print(type(fraudulent))\n",
    "\n",
    "#convert to dataframe\n",
    "nonFraudulentLable= pd.DataFrame(nonFraudulentLable)\n",
    "fraudulentLable= pd.DataFrame(fraudulentLable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(3624, 85)\n"
     ]
    }
   ],
   "source": [
    "#divide the nonFraudulent into test and train\n",
    "nonFraudX_train,nonFraudX_test,nonFraudY_train,nonFraudY_test = train_test_split(nonFraudulent,nonFraudulentLable,random_state=3,test_size=0.25)\n",
    "print('nonFraudxtrain:',type(nonFraudX_train))\n",
    "print(nonFraudX_test.shape)\n",
    "\n",
    "#divide the trainset into trainset and validation set\n",
    "nonFraudX_trainNew,nonFraudX_Valid,nonFraudY_trainNew,nonFraudY_Valid= train_test_split(nonFraudX_train,nonFraudY_train,random_state=3,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into numpy\n",
    "nonFraudX_trainNew= nonFraudX_trainNew.values\n",
    "nonFraudY_trainNew= nonFraudY_trainNew.values\n",
    "\n",
    "nonFraudX_test= nonFraudX_test.values\n",
    "nonFraudY_test= nonFraudY_test.values\n",
    "\n",
    "fraudFeatures= fraudulent.values\n",
    "fraudLabel= fraudulentLable.values\n",
    "\n",
    "nonFraudX_Valid= nonFraudX_Valid.values\n",
    "nonFraudY_Valid= nonFraudY_Valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class Dataset which returns the data and labels\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,features,lables,transform= None):\n",
    "        dataTensor= []\n",
    "        lableTensor= []\n",
    "        dataSize= features.shape[0]\n",
    "        \n",
    "        for data in range(dataSize):\n",
    "            feature= features[data,:]\n",
    "            #feature= torch.from_numpy(feature).float()\n",
    "            feature= torch.Tensor(feature)\n",
    "            dataTensor.append(feature)\n",
    "            \n",
    "            lable= np.asanyarray(lables[data])\n",
    "            lable= torch.from_numpy(lable).float()\n",
    "           \n",
    "            #lable= torch.Tensor(lable)\n",
    "            #print(\"lable:\",lable)\n",
    "            #assert(False)\n",
    "           \n",
    "            lableTensor.append(lable)\n",
    "        \n",
    "        #put everything in features and lables\n",
    "        self.features= dataTensor\n",
    "        self.lables= lableTensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        feature= self.features[index]\n",
    "        lable= self.lables[index]\n",
    "        #print(\"get_item feature:\",feature)\n",
    "        #print(\"get_item lable:\",lable)\n",
    "        return feature,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the  nonfraudulent dataset for train and test loader\n",
    "myNonFraudulentTrainDataset= myDataset(nonFraudX_trainNew,nonFraudY_trainNew)\n",
    "myNonFraudulentTestDataset= myDataset(nonFraudX_test,nonFraudY_test)\n",
    "\n",
    "#make validation loader\n",
    "myNonFraudulentValidation= myDataset(nonFraudX_Valid,nonFraudY_Valid)\n",
    "\n",
    "#make the fraudulent dataset for train and test loader\n",
    "myFraudulentDataset= myDataset(fraudFeatures,fraudLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the trainloader and test loader for nonfraudulent dataset.\n",
    "nonFraudulentTrainLoader= torch.utils.data.DataLoader(myNonFraudulentTrainDataset,batch_size=32,shuffle=True,num_workers=0)\n",
    "\n",
    "nonFraudulentTestLoader= torch.utils.data.DataLoader(myNonFraudulentTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "#make the trainloader and test loader for validation\n",
    "nonFraudulentValidationLoader= torch.utils.data.DataLoader(myNonFraudulentValidation,batch_size=32,shuffle=True,num_workers=0)\n",
    "\n",
    "#make the trainloader and test loader for nonfraudulent dataset.\n",
    "fraudulentLoader= torch.utils.data.DataLoader(myFraudulentDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder,self).__init__()\n",
    "        self.encoder1= nn.Sequential(\n",
    "            nn.Linear(85,40),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.decoder1= nn.Sequential(\n",
    "            nn.Linear(40,85),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.encoder1(x)\n",
    "        x= self.decoder1(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= autoencoder()\n",
    "criterion= nn.MSELoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.01,momentum=0.9,weight_decay=5e-4)\n",
    "trainingLoss= []\n",
    "validationLoss= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "def testModel(loader):\n",
    "    running_loss= 0.0\n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model(inputs)\n",
    "        loss= criterion(output,inputs)\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss/nonFraudX_Valid.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--[1] --------------------Training loss: 0.005973\n",
      "Epoch--[1] --------------------Validation loss: 0.004606\n",
      "Epoch--[2] --------------------Training loss: 0.003892\n",
      "Epoch--[2] --------------------Validation loss: 0.003375\n",
      "Epoch--[3] --------------------Training loss: 0.003130\n",
      "Epoch--[3] --------------------Validation loss: 0.002939\n",
      "Epoch--[4] --------------------Training loss: 0.002848\n",
      "Epoch--[4] --------------------Validation loss: 0.002763\n",
      "Epoch--[5] --------------------Training loss: 0.002725\n",
      "Epoch--[5] --------------------Validation loss: 0.002677\n",
      "Epoch--[6] --------------------Training loss: 0.002661\n",
      "Epoch--[6] --------------------Validation loss: 0.002629\n",
      "Epoch--[7] --------------------Training loss: 0.002623\n",
      "Epoch--[7] --------------------Validation loss: 0.002600\n",
      "Epoch--[8] --------------------Training loss: 0.002599\n",
      "Epoch--[8] --------------------Validation loss: 0.002580\n",
      "Epoch--[9] --------------------Training loss: 0.002582\n",
      "Epoch--[9] --------------------Validation loss: 0.002565\n",
      "Epoch--[10] --------------------Training loss: 0.002570\n",
      "Epoch--[10] --------------------Validation loss: 0.002555\n",
      "Epoch--[11] --------------------Training loss: 0.002560\n",
      "Epoch--[11] --------------------Validation loss: 0.002547\n",
      "Epoch--[12] --------------------Training loss: 0.002553\n",
      "Epoch--[12] --------------------Validation loss: 0.002540\n",
      "Epoch--[13] --------------------Training loss: 0.002547\n",
      "Epoch--[13] --------------------Validation loss: 0.002535\n",
      "Epoch--[14] --------------------Training loss: 0.002543\n",
      "Epoch--[14] --------------------Validation loss: 0.002531\n",
      "Epoch--[15] --------------------Training loss: 0.002539\n",
      "Epoch--[15] --------------------Validation loss: 0.002527\n",
      "Epoch--[16] --------------------Training loss: 0.002536\n",
      "Epoch--[16] --------------------Validation loss: 0.002524\n",
      "Epoch--[17] --------------------Training loss: 0.002533\n",
      "Epoch--[17] --------------------Validation loss: 0.002522\n",
      "Epoch--[18] --------------------Training loss: 0.002531\n",
      "Epoch--[18] --------------------Validation loss: 0.002520\n",
      "Epoch--[19] --------------------Training loss: 0.002529\n",
      "Epoch--[19] --------------------Validation loss: 0.002518\n",
      "Epoch--[20] --------------------Training loss: 0.002527\n",
      "Epoch--[20] --------------------Validation loss: 0.002516\n",
      "Epoch--[21] --------------------Training loss: 0.002526\n",
      "Epoch--[21] --------------------Validation loss: 0.002515\n",
      "Epoch--[22] --------------------Training loss: 0.002524\n",
      "Epoch--[22] --------------------Validation loss: 0.002514\n",
      "Epoch--[23] --------------------Training loss: 0.002523\n",
      "Epoch--[23] --------------------Validation loss: 0.002512\n",
      "Epoch--[24] --------------------Training loss: 0.002522\n",
      "Epoch--[24] --------------------Validation loss: 0.002511\n",
      "Epoch--[25] --------------------Training loss: 0.002521\n",
      "Epoch--[25] --------------------Validation loss: 0.002511\n",
      "Epoch--[26] --------------------Training loss: 0.002520\n",
      "Epoch--[26] --------------------Validation loss: 0.002510\n",
      "Epoch--[27] --------------------Training loss: 0.002519\n",
      "Epoch--[27] --------------------Validation loss: 0.002509\n",
      "Epoch--[28] --------------------Training loss: 0.002519\n",
      "Epoch--[28] --------------------Validation loss: 0.002508\n",
      "Epoch--[29] --------------------Training loss: 0.002518\n",
      "Epoch--[29] --------------------Validation loss: 0.002508\n",
      "Epoch--[30] --------------------Training loss: 0.002518\n",
      "Epoch--[30] --------------------Validation loss: 0.002507\n",
      "Epoch--[31] --------------------Training loss: 0.002517\n",
      "Epoch--[31] --------------------Validation loss: 0.002507\n",
      "Epoch--[32] --------------------Training loss: 0.002517\n",
      "Epoch--[32] --------------------Validation loss: 0.002506\n",
      "Epoch--[33] --------------------Training loss: 0.002516\n",
      "Epoch--[33] --------------------Validation loss: 0.002506\n",
      "Epoch--[34] --------------------Training loss: 0.002516\n",
      "Epoch--[34] --------------------Validation loss: 0.002506\n",
      "Epoch--[35] --------------------Training loss: 0.002516\n",
      "Epoch--[35] --------------------Validation loss: 0.002505\n",
      "Epoch--[36] --------------------Training loss: 0.002515\n",
      "Epoch--[36] --------------------Validation loss: 0.002505\n",
      "Epoch--[37] --------------------Training loss: 0.002515\n",
      "Epoch--[37] --------------------Validation loss: 0.002505\n",
      "Epoch--[38] --------------------Training loss: 0.002514\n",
      "Epoch--[38] --------------------Validation loss: 0.002504\n",
      "Epoch--[39] --------------------Training loss: 0.002514\n",
      "Epoch--[39] --------------------Validation loss: 0.002504\n",
      "Epoch--[40] --------------------Training loss: 0.002514\n",
      "Epoch--[40] --------------------Validation loss: 0.002504\n",
      "Epoch--[41] --------------------Training loss: 0.002514\n",
      "Epoch--[41] --------------------Validation loss: 0.002504\n",
      "Epoch--[42] --------------------Training loss: 0.002513\n",
      "Epoch--[42] --------------------Validation loss: 0.002503\n",
      "Epoch--[43] --------------------Training loss: 0.002513\n",
      "Epoch--[43] --------------------Validation loss: 0.002503\n",
      "Epoch--[44] --------------------Training loss: 0.002513\n",
      "Epoch--[44] --------------------Validation loss: 0.002503\n",
      "Epoch--[45] --------------------Training loss: 0.002513\n",
      "Epoch--[45] --------------------Validation loss: 0.002503\n",
      "Epoch--[46] --------------------Training loss: 0.002513\n",
      "Epoch--[46] --------------------Validation loss: 0.002503\n",
      "Epoch--[47] --------------------Training loss: 0.002513\n",
      "Epoch--[47] --------------------Validation loss: 0.002502\n",
      "Epoch--[48] --------------------Training loss: 0.002512\n",
      "Epoch--[48] --------------------Validation loss: 0.002502\n",
      "Epoch--[49] --------------------Training loss: 0.002512\n",
      "Epoch--[49] --------------------Validation loss: 0.002502\n",
      "Epoch--[50] --------------------Training loss: 0.002512\n",
      "Epoch--[50] --------------------Validation loss: 0.002502\n",
      "Epoch--[51] --------------------Training loss: 0.002512\n",
      "Epoch--[51] --------------------Validation loss: 0.002502\n",
      "Epoch--[52] --------------------Training loss: 0.002512\n",
      "Epoch--[52] --------------------Validation loss: 0.002501\n",
      "Epoch--[53] --------------------Training loss: 0.002512\n",
      "Epoch--[53] --------------------Validation loss: 0.002501\n",
      "Epoch--[54] --------------------Training loss: 0.002511\n",
      "Epoch--[54] --------------------Validation loss: 0.002501\n",
      "Epoch--[55] --------------------Training loss: 0.002511\n",
      "Epoch--[55] --------------------Validation loss: 0.002501\n",
      "Epoch--[56] --------------------Training loss: 0.002511\n",
      "Epoch--[56] --------------------Validation loss: 0.002501\n",
      "Epoch--[57] --------------------Training loss: 0.002511\n",
      "Epoch--[57] --------------------Validation loss: 0.002501\n",
      "Epoch--[58] --------------------Training loss: 0.002511\n",
      "Epoch--[58] --------------------Validation loss: 0.002501\n",
      "Epoch--[59] --------------------Training loss: 0.002511\n",
      "Epoch--[59] --------------------Validation loss: 0.002501\n",
      "Epoch--[60] --------------------Training loss: 0.002511\n",
      "Epoch--[60] --------------------Validation loss: 0.002500\n",
      "Epoch--[61] --------------------Training loss: 0.002510\n",
      "Epoch--[61] --------------------Validation loss: 0.002500\n",
      "Epoch--[62] --------------------Training loss: 0.002510\n",
      "Epoch--[62] --------------------Validation loss: 0.002500\n",
      "Epoch--[63] --------------------Training loss: 0.002510\n",
      "Epoch--[63] --------------------Validation loss: 0.002500\n",
      "Epoch--[64] --------------------Training loss: 0.002510\n",
      "Epoch--[64] --------------------Validation loss: 0.002500\n",
      "Epoch--[65] --------------------Training loss: 0.002510\n",
      "Epoch--[65] --------------------Validation loss: 0.002499\n",
      "Epoch--[66] --------------------Training loss: 0.002510\n",
      "Epoch--[66] --------------------Validation loss: 0.002499\n",
      "Epoch--[67] --------------------Training loss: 0.002509\n",
      "Epoch--[67] --------------------Validation loss: 0.002499\n",
      "Epoch--[68] --------------------Training loss: 0.002509\n",
      "Epoch--[68] --------------------Validation loss: 0.002499\n",
      "Epoch--[69] --------------------Training loss: 0.002509\n",
      "Epoch--[69] --------------------Validation loss: 0.002499\n",
      "Epoch--[70] --------------------Training loss: 0.002509\n",
      "Epoch--[70] --------------------Validation loss: 0.002499\n",
      "Epoch--[71] --------------------Training loss: 0.002509\n",
      "Epoch--[71] --------------------Validation loss: 0.002498\n",
      "Epoch--[72] --------------------Training loss: 0.002509\n",
      "Epoch--[72] --------------------Validation loss: 0.002498\n",
      "Epoch--[73] --------------------Training loss: 0.002508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--[73] --------------------Validation loss: 0.002498\n",
      "Epoch--[74] --------------------Training loss: 0.002508\n",
      "Epoch--[74] --------------------Validation loss: 0.002498\n",
      "Epoch--[75] --------------------Training loss: 0.002508\n",
      "Epoch--[75] --------------------Validation loss: 0.002498\n",
      "Epoch--[76] --------------------Training loss: 0.002508\n",
      "Epoch--[76] --------------------Validation loss: 0.002498\n",
      "Epoch--[77] --------------------Training loss: 0.002508\n",
      "Epoch--[77] --------------------Validation loss: 0.002497\n",
      "Epoch--[78] --------------------Training loss: 0.002508\n",
      "Epoch--[78] --------------------Validation loss: 0.002497\n",
      "Epoch--[79] --------------------Training loss: 0.002507\n",
      "Epoch--[79] --------------------Validation loss: 0.002497\n",
      "Epoch--[80] --------------------Training loss: 0.002507\n",
      "Epoch--[80] --------------------Validation loss: 0.002497\n",
      "Epoch--[81] --------------------Training loss: 0.002507\n",
      "Epoch--[81] --------------------Validation loss: 0.002497\n",
      "Epoch--[82] --------------------Training loss: 0.002507\n",
      "Epoch--[82] --------------------Validation loss: 0.002496\n",
      "Epoch--[83] --------------------Training loss: 0.002506\n",
      "Epoch--[83] --------------------Validation loss: 0.002496\n",
      "Epoch--[84] --------------------Training loss: 0.002506\n",
      "Epoch--[84] --------------------Validation loss: 0.002496\n",
      "Epoch--[85] --------------------Training loss: 0.002506\n",
      "Epoch--[85] --------------------Validation loss: 0.002496\n",
      "Epoch--[86] --------------------Training loss: 0.002506\n",
      "Epoch--[86] --------------------Validation loss: 0.002495\n",
      "Epoch--[87] --------------------Training loss: 0.002506\n",
      "Epoch--[87] --------------------Validation loss: 0.002495\n",
      "Epoch--[88] --------------------Training loss: 0.002505\n",
      "Epoch--[88] --------------------Validation loss: 0.002495\n",
      "Epoch--[89] --------------------Training loss: 0.002505\n",
      "Epoch--[89] --------------------Validation loss: 0.002495\n",
      "Epoch--[90] --------------------Training loss: 0.002505\n",
      "Epoch--[90] --------------------Validation loss: 0.002494\n",
      "Epoch--[91] --------------------Training loss: 0.002505\n",
      "Epoch--[91] --------------------Validation loss: 0.002494\n",
      "Epoch--[92] --------------------Training loss: 0.002504\n",
      "Epoch--[92] --------------------Validation loss: 0.002494\n",
      "Epoch--[93] --------------------Training loss: 0.002504\n",
      "Epoch--[93] --------------------Validation loss: 0.002494\n",
      "Epoch--[94] --------------------Training loss: 0.002504\n",
      "Epoch--[94] --------------------Validation loss: 0.002493\n",
      "Epoch--[95] --------------------Training loss: 0.002503\n",
      "Epoch--[95] --------------------Validation loss: 0.002493\n",
      "Epoch--[96] --------------------Training loss: 0.002503\n",
      "Epoch--[96] --------------------Validation loss: 0.002493\n",
      "Epoch--[97] --------------------Training loss: 0.002503\n",
      "Epoch--[97] --------------------Validation loss: 0.002492\n",
      "Epoch--[98] --------------------Training loss: 0.002503\n",
      "Epoch--[98] --------------------Validation loss: 0.002492\n",
      "Epoch--[99] --------------------Training loss: 0.002502\n",
      "Epoch--[99] --------------------Validation loss: 0.002492\n",
      "Epoch--[100] --------------------Training loss: 0.002502\n",
      "Epoch--[100] --------------------Validation loss: 0.002491\n",
      "Epoch--[101] --------------------Training loss: 0.002501\n",
      "Epoch--[101] --------------------Validation loss: 0.002491\n",
      "Epoch--[102] --------------------Training loss: 0.002501\n",
      "Epoch--[102] --------------------Validation loss: 0.002491\n",
      "Epoch--[103] --------------------Training loss: 0.002501\n",
      "Epoch--[103] --------------------Validation loss: 0.002490\n",
      "Epoch--[104] --------------------Training loss: 0.002500\n",
      "Epoch--[104] --------------------Validation loss: 0.002490\n",
      "Epoch--[105] --------------------Training loss: 0.002500\n",
      "Epoch--[105] --------------------Validation loss: 0.002489\n",
      "Epoch--[106] --------------------Training loss: 0.002500\n",
      "Epoch--[106] --------------------Validation loss: 0.002489\n",
      "Epoch--[107] --------------------Training loss: 0.002499\n",
      "Epoch--[107] --------------------Validation loss: 0.002489\n",
      "Epoch--[108] --------------------Training loss: 0.002499\n",
      "Epoch--[108] --------------------Validation loss: 0.002488\n",
      "Epoch--[109] --------------------Training loss: 0.002498\n",
      "Epoch--[109] --------------------Validation loss: 0.002488\n",
      "Epoch--[110] --------------------Training loss: 0.002498\n",
      "Epoch--[110] --------------------Validation loss: 0.002487\n",
      "Epoch--[111] --------------------Training loss: 0.002498\n",
      "Epoch--[111] --------------------Validation loss: 0.002487\n",
      "Epoch--[112] --------------------Training loss: 0.002497\n",
      "Epoch--[112] --------------------Validation loss: 0.002486\n",
      "Epoch--[113] --------------------Training loss: 0.002496\n",
      "Epoch--[113] --------------------Validation loss: 0.002486\n",
      "Epoch--[114] --------------------Training loss: 0.002496\n",
      "Epoch--[114] --------------------Validation loss: 0.002485\n",
      "Epoch--[115] --------------------Training loss: 0.002496\n",
      "Epoch--[115] --------------------Validation loss: 0.002485\n",
      "Epoch--[116] --------------------Training loss: 0.002495\n",
      "Epoch--[116] --------------------Validation loss: 0.002485\n",
      "Epoch--[117] --------------------Training loss: 0.002495\n",
      "Epoch--[117] --------------------Validation loss: 0.002484\n",
      "Epoch--[118] --------------------Training loss: 0.002494\n",
      "Epoch--[118] --------------------Validation loss: 0.002483\n",
      "Epoch--[119] --------------------Training loss: 0.002493\n",
      "Epoch--[119] --------------------Validation loss: 0.002483\n",
      "Epoch--[120] --------------------Training loss: 0.002493\n",
      "Epoch--[120] --------------------Validation loss: 0.002482\n",
      "Epoch--[121] --------------------Training loss: 0.002492\n",
      "Epoch--[121] --------------------Validation loss: 0.002482\n",
      "Epoch--[122] --------------------Training loss: 0.002492\n",
      "Epoch--[122] --------------------Validation loss: 0.002481\n",
      "Epoch--[123] --------------------Training loss: 0.002491\n",
      "Epoch--[123] --------------------Validation loss: 0.002481\n",
      "Epoch--[124] --------------------Training loss: 0.002490\n",
      "Epoch--[124] --------------------Validation loss: 0.002480\n",
      "Epoch--[125] --------------------Training loss: 0.002490\n",
      "Epoch--[125] --------------------Validation loss: 0.002479\n",
      "Epoch--[126] --------------------Training loss: 0.002489\n",
      "Epoch--[126] --------------------Validation loss: 0.002478\n",
      "Epoch--[127] --------------------Training loss: 0.002489\n",
      "Epoch--[127] --------------------Validation loss: 0.002478\n",
      "Epoch--[128] --------------------Training loss: 0.002488\n",
      "Epoch--[128] --------------------Validation loss: 0.002477\n",
      "Epoch--[129] --------------------Training loss: 0.002487\n",
      "Epoch--[129] --------------------Validation loss: 0.002476\n",
      "Epoch--[130] --------------------Training loss: 0.002486\n",
      "Epoch--[130] --------------------Validation loss: 0.002476\n",
      "Epoch--[131] --------------------Training loss: 0.002486\n",
      "Epoch--[131] --------------------Validation loss: 0.002475\n",
      "Epoch--[132] --------------------Training loss: 0.002485\n",
      "Epoch--[132] --------------------Validation loss: 0.002474\n",
      "Epoch--[133] --------------------Training loss: 0.002484\n",
      "Epoch--[133] --------------------Validation loss: 0.002473\n",
      "Epoch--[134] --------------------Training loss: 0.002483\n",
      "Epoch--[134] --------------------Validation loss: 0.002472\n",
      "Epoch--[135] --------------------Training loss: 0.002483\n",
      "Epoch--[135] --------------------Validation loss: 0.002472\n",
      "Epoch--[136] --------------------Training loss: 0.002482\n",
      "Epoch--[136] --------------------Validation loss: 0.002471\n",
      "Epoch--[137] --------------------Training loss: 0.002481\n",
      "Epoch--[137] --------------------Validation loss: 0.002470\n",
      "Epoch--[138] --------------------Training loss: 0.002480\n",
      "Epoch--[138] --------------------Validation loss: 0.002469\n",
      "Epoch--[139] --------------------Training loss: 0.002479\n",
      "Epoch--[139] --------------------Validation loss: 0.002468\n",
      "Epoch--[140] --------------------Training loss: 0.002478\n",
      "Epoch--[140] --------------------Validation loss: 0.002467\n",
      "Epoch--[141] --------------------Training loss: 0.002477\n",
      "Epoch--[141] --------------------Validation loss: 0.002466\n",
      "Epoch--[142] --------------------Training loss: 0.002476\n",
      "Epoch--[142] --------------------Validation loss: 0.002465\n",
      "Epoch--[143] --------------------Training loss: 0.002475\n",
      "Epoch--[143] --------------------Validation loss: 0.002464\n",
      "Epoch--[144] --------------------Training loss: 0.002474\n",
      "Epoch--[144] --------------------Validation loss: 0.002463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--[145] --------------------Training loss: 0.002473\n",
      "Epoch--[145] --------------------Validation loss: 0.002462\n",
      "Epoch--[146] --------------------Training loss: 0.002472\n",
      "Epoch--[146] --------------------Validation loss: 0.002461\n",
      "Epoch--[147] --------------------Training loss: 0.002471\n",
      "Epoch--[147] --------------------Validation loss: 0.002460\n",
      "Epoch--[148] --------------------Training loss: 0.002470\n",
      "Epoch--[148] --------------------Validation loss: 0.002459\n",
      "Epoch--[149] --------------------Training loss: 0.002469\n",
      "Epoch--[149] --------------------Validation loss: 0.002458\n",
      "Epoch--[150] --------------------Training loss: 0.002468\n",
      "Epoch--[150] --------------------Validation loss: 0.002457\n",
      "Epoch--[151] --------------------Training loss: 0.002467\n",
      "Epoch--[151] --------------------Validation loss: 0.002456\n",
      "Epoch--[152] --------------------Training loss: 0.002466\n",
      "Epoch--[152] --------------------Validation loss: 0.002454\n",
      "Epoch--[153] --------------------Training loss: 0.002464\n",
      "Epoch--[153] --------------------Validation loss: 0.002453\n",
      "Epoch--[154] --------------------Training loss: 0.002463\n",
      "Epoch--[154] --------------------Validation loss: 0.002452\n",
      "Epoch--[155] --------------------Training loss: 0.002462\n",
      "Epoch--[155] --------------------Validation loss: 0.002451\n",
      "Epoch--[156] --------------------Training loss: 0.002461\n",
      "Epoch--[156] --------------------Validation loss: 0.002450\n",
      "Epoch--[157] --------------------Training loss: 0.002459\n",
      "Epoch--[157] --------------------Validation loss: 0.002448\n",
      "Epoch--[158] --------------------Training loss: 0.002458\n",
      "Epoch--[158] --------------------Validation loss: 0.002447\n",
      "Epoch--[159] --------------------Training loss: 0.002457\n",
      "Epoch--[159] --------------------Validation loss: 0.002446\n",
      "Epoch--[160] --------------------Training loss: 0.002456\n",
      "Epoch--[160] --------------------Validation loss: 0.002444\n",
      "Epoch--[161] --------------------Training loss: 0.002454\n",
      "Epoch--[161] --------------------Validation loss: 0.002443\n",
      "Epoch--[162] --------------------Training loss: 0.002453\n",
      "Epoch--[162] --------------------Validation loss: 0.002442\n",
      "Epoch--[163] --------------------Training loss: 0.002451\n",
      "Epoch--[163] --------------------Validation loss: 0.002440\n",
      "Epoch--[164] --------------------Training loss: 0.002450\n",
      "Epoch--[164] --------------------Validation loss: 0.002439\n",
      "Epoch--[165] --------------------Training loss: 0.002449\n",
      "Epoch--[165] --------------------Validation loss: 0.002437\n",
      "Epoch--[166] --------------------Training loss: 0.002447\n",
      "Epoch--[166] --------------------Validation loss: 0.002436\n",
      "Epoch--[167] --------------------Training loss: 0.002446\n",
      "Epoch--[167] --------------------Validation loss: 0.002434\n",
      "Epoch--[168] --------------------Training loss: 0.002444\n",
      "Epoch--[168] --------------------Validation loss: 0.002433\n",
      "Epoch--[169] --------------------Training loss: 0.002443\n",
      "Epoch--[169] --------------------Validation loss: 0.002431\n",
      "Epoch--[170] --------------------Training loss: 0.002441\n",
      "Epoch--[170] --------------------Validation loss: 0.002430\n",
      "Epoch--[171] --------------------Training loss: 0.002440\n",
      "Epoch--[171] --------------------Validation loss: 0.002428\n",
      "Epoch--[172] --------------------Training loss: 0.002438\n",
      "Epoch--[172] --------------------Validation loss: 0.002427\n",
      "Epoch--[173] --------------------Training loss: 0.002437\n",
      "Epoch--[173] --------------------Validation loss: 0.002425\n",
      "Epoch--[174] --------------------Training loss: 0.002435\n",
      "Epoch--[174] --------------------Validation loss: 0.002424\n",
      "Epoch--[175] --------------------Training loss: 0.002434\n",
      "Epoch--[175] --------------------Validation loss: 0.002422\n",
      "Epoch--[176] --------------------Training loss: 0.002432\n",
      "Epoch--[176] --------------------Validation loss: 0.002421\n",
      "Epoch--[177] --------------------Training loss: 0.002431\n",
      "Epoch--[177] --------------------Validation loss: 0.002419\n",
      "Epoch--[178] --------------------Training loss: 0.002429\n",
      "Epoch--[178] --------------------Validation loss: 0.002417\n",
      "Epoch--[179] --------------------Training loss: 0.002427\n",
      "Epoch--[179] --------------------Validation loss: 0.002416\n",
      "Epoch--[180] --------------------Training loss: 0.002426\n",
      "Epoch--[180] --------------------Validation loss: 0.002414\n",
      "Epoch--[181] --------------------Training loss: 0.002424\n",
      "Epoch--[181] --------------------Validation loss: 0.002413\n",
      "Epoch--[182] --------------------Training loss: 0.002423\n",
      "Epoch--[182] --------------------Validation loss: 0.002411\n",
      "Epoch--[183] --------------------Training loss: 0.002421\n",
      "Epoch--[183] --------------------Validation loss: 0.002410\n",
      "Epoch--[184] --------------------Training loss: 0.002419\n",
      "Epoch--[184] --------------------Validation loss: 0.002408\n",
      "Epoch--[185] --------------------Training loss: 0.002418\n",
      "Epoch--[185] --------------------Validation loss: 0.002406\n",
      "Epoch--[186] --------------------Training loss: 0.002416\n",
      "Epoch--[186] --------------------Validation loss: 0.002405\n",
      "Epoch--[187] --------------------Training loss: 0.002415\n",
      "Epoch--[187] --------------------Validation loss: 0.002403\n",
      "Epoch--[188] --------------------Training loss: 0.002413\n",
      "Epoch--[188] --------------------Validation loss: 0.002402\n",
      "Epoch--[189] --------------------Training loss: 0.002412\n",
      "Epoch--[189] --------------------Validation loss: 0.002400\n",
      "Epoch--[190] --------------------Training loss: 0.002410\n",
      "Epoch--[190] --------------------Validation loss: 0.002398\n",
      "Epoch--[191] --------------------Training loss: 0.002408\n",
      "Epoch--[191] --------------------Validation loss: 0.002397\n",
      "Epoch--[192] --------------------Training loss: 0.002407\n",
      "Epoch--[192] --------------------Validation loss: 0.002395\n",
      "Epoch--[193] --------------------Training loss: 0.002405\n",
      "Epoch--[193] --------------------Validation loss: 0.002394\n",
      "Epoch--[194] --------------------Training loss: 0.002404\n",
      "Epoch--[194] --------------------Validation loss: 0.002392\n",
      "Epoch--[195] --------------------Training loss: 0.002402\n",
      "Epoch--[195] --------------------Validation loss: 0.002391\n",
      "Epoch--[196] --------------------Training loss: 0.002401\n",
      "Epoch--[196] --------------------Validation loss: 0.002389\n",
      "Epoch--[197] --------------------Training loss: 0.002399\n",
      "Epoch--[197] --------------------Validation loss: 0.002388\n",
      "Epoch--[198] --------------------Training loss: 0.002398\n",
      "Epoch--[198] --------------------Validation loss: 0.002386\n",
      "Epoch--[199] --------------------Training loss: 0.002396\n",
      "Epoch--[199] --------------------Validation loss: 0.002385\n",
      "Epoch--[200] --------------------Training loss: 0.002395\n",
      "Epoch--[200] --------------------Validation loss: 0.002383\n",
      "Epoch--[201] --------------------Training loss: 0.002393\n",
      "Epoch--[201] --------------------Validation loss: 0.002382\n",
      "Epoch--[202] --------------------Training loss: 0.002392\n",
      "Epoch--[202] --------------------Validation loss: 0.002380\n",
      "Epoch--[203] --------------------Training loss: 0.002391\n",
      "Epoch--[203] --------------------Validation loss: 0.002379\n",
      "Epoch--[204] --------------------Training loss: 0.002389\n",
      "Epoch--[204] --------------------Validation loss: 0.002378\n",
      "Epoch--[205] --------------------Training loss: 0.002388\n",
      "Epoch--[205] --------------------Validation loss: 0.002376\n",
      "Epoch--[206] --------------------Training loss: 0.002386\n",
      "Epoch--[206] --------------------Validation loss: 0.002375\n",
      "Epoch--[207] --------------------Training loss: 0.002385\n",
      "Epoch--[207] --------------------Validation loss: 0.002374\n",
      "Epoch--[208] --------------------Training loss: 0.002384\n",
      "Epoch--[208] --------------------Validation loss: 0.002372\n",
      "Epoch--[209] --------------------Training loss: 0.002382\n",
      "Epoch--[209] --------------------Validation loss: 0.002371\n",
      "Epoch--[210] --------------------Training loss: 0.002381\n",
      "Epoch--[210] --------------------Validation loss: 0.002370\n",
      "Epoch--[211] --------------------Training loss: 0.002380\n",
      "Epoch--[211] --------------------Validation loss: 0.002368\n",
      "Epoch--[212] --------------------Training loss: 0.002379\n",
      "Epoch--[212] --------------------Validation loss: 0.002367\n",
      "Epoch--[213] --------------------Training loss: 0.002377\n",
      "Epoch--[213] --------------------Validation loss: 0.002366\n",
      "Epoch--[214] --------------------Training loss: 0.002376\n",
      "Epoch--[214] --------------------Validation loss: 0.002365\n",
      "Epoch--[215] --------------------Training loss: 0.002375\n",
      "Epoch--[215] --------------------Validation loss: 0.002363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--[216] --------------------Training loss: 0.002374\n",
      "Epoch--[216] --------------------Validation loss: 0.002362\n",
      "Epoch--[217] --------------------Training loss: 0.002373\n",
      "Epoch--[217] --------------------Validation loss: 0.002361\n",
      "Epoch--[218] --------------------Training loss: 0.002371\n",
      "Epoch--[218] --------------------Validation loss: 0.002360\n",
      "Epoch--[219] --------------------Training loss: 0.002370\n",
      "Epoch--[219] --------------------Validation loss: 0.002359\n",
      "Epoch--[220] --------------------Training loss: 0.002369\n",
      "Epoch--[220] --------------------Validation loss: 0.002358\n",
      "Epoch--[221] --------------------Training loss: 0.002368\n",
      "Epoch--[221] --------------------Validation loss: 0.002357\n",
      "Epoch--[222] --------------------Training loss: 0.002367\n",
      "Epoch--[222] --------------------Validation loss: 0.002356\n",
      "Epoch--[223] --------------------Training loss: 0.002366\n",
      "Epoch--[223] --------------------Validation loss: 0.002355\n",
      "Epoch--[224] --------------------Training loss: 0.002365\n",
      "Epoch--[224] --------------------Validation loss: 0.002353\n",
      "Epoch--[225] --------------------Training loss: 0.002364\n",
      "Epoch--[225] --------------------Validation loss: 0.002353\n",
      "Epoch--[226] --------------------Training loss: 0.002363\n",
      "Epoch--[226] --------------------Validation loss: 0.002351\n",
      "Epoch--[227] --------------------Training loss: 0.002362\n",
      "Epoch--[227] --------------------Validation loss: 0.002351\n",
      "Epoch--[228] --------------------Training loss: 0.002361\n",
      "Epoch--[228] --------------------Validation loss: 0.002350\n",
      "Epoch--[229] --------------------Training loss: 0.002360\n",
      "Epoch--[229] --------------------Validation loss: 0.002349\n",
      "Epoch--[230] --------------------Training loss: 0.002359\n",
      "Epoch--[230] --------------------Validation loss: 0.002348\n",
      "Epoch--[231] --------------------Training loss: 0.002358\n",
      "Epoch--[231] --------------------Validation loss: 0.002347\n",
      "Epoch--[232] --------------------Training loss: 0.002357\n",
      "Epoch--[232] --------------------Validation loss: 0.002346\n",
      "Epoch--[233] --------------------Training loss: 0.002356\n",
      "Epoch--[233] --------------------Validation loss: 0.002345\n",
      "Epoch--[234] --------------------Training loss: 0.002356\n",
      "Epoch--[234] --------------------Validation loss: 0.002344\n",
      "Epoch--[235] --------------------Training loss: 0.002355\n",
      "Epoch--[235] --------------------Validation loss: 0.002343\n",
      "Epoch--[236] --------------------Training loss: 0.002354\n",
      "Epoch--[236] --------------------Validation loss: 0.002343\n",
      "Epoch--[237] --------------------Training loss: 0.002353\n",
      "Epoch--[237] --------------------Validation loss: 0.002342\n",
      "Epoch--[238] --------------------Training loss: 0.002353\n",
      "Epoch--[238] --------------------Validation loss: 0.002341\n",
      "Epoch--[239] --------------------Training loss: 0.002352\n",
      "Epoch--[239] --------------------Validation loss: 0.002340\n",
      "Epoch--[240] --------------------Training loss: 0.002351\n",
      "Epoch--[240] --------------------Validation loss: 0.002340\n",
      "Epoch--[241] --------------------Training loss: 0.002350\n",
      "Epoch--[241] --------------------Validation loss: 0.002339\n",
      "Epoch--[242] --------------------Training loss: 0.002349\n",
      "Epoch--[242] --------------------Validation loss: 0.002338\n",
      "Epoch--[243] --------------------Training loss: 0.002349\n",
      "Epoch--[243] --------------------Validation loss: 0.002337\n",
      "Epoch--[244] --------------------Training loss: 0.002348\n",
      "Epoch--[244] --------------------Validation loss: 0.002337\n",
      "Epoch--[245] --------------------Training loss: 0.002347\n",
      "Epoch--[245] --------------------Validation loss: 0.002336\n",
      "Epoch--[246] --------------------Training loss: 0.002347\n",
      "Epoch--[246] --------------------Validation loss: 0.002336\n",
      "Epoch--[247] --------------------Training loss: 0.002346\n",
      "Epoch--[247] --------------------Validation loss: 0.002335\n",
      "Epoch--[248] --------------------Training loss: 0.002345\n",
      "Epoch--[248] --------------------Validation loss: 0.002334\n",
      "Epoch--[249] --------------------Training loss: 0.002345\n",
      "Epoch--[249] --------------------Validation loss: 0.002334\n",
      "Epoch--[250] --------------------Training loss: 0.002344\n",
      "Epoch--[250] --------------------Validation loss: 0.002333\n",
      "Epoch--[251] --------------------Training loss: 0.002344\n",
      "Epoch--[251] --------------------Validation loss: 0.002332\n",
      "Epoch--[252] --------------------Training loss: 0.002343\n",
      "Epoch--[252] --------------------Validation loss: 0.002332\n",
      "Epoch--[253] --------------------Training loss: 0.002343\n",
      "Epoch--[253] --------------------Validation loss: 0.002331\n",
      "Epoch--[254] --------------------Training loss: 0.002342\n",
      "Epoch--[254] --------------------Validation loss: 0.002331\n",
      "Epoch--[255] --------------------Training loss: 0.002341\n",
      "Epoch--[255] --------------------Validation loss: 0.002330\n",
      "Epoch--[256] --------------------Training loss: 0.002341\n",
      "Epoch--[256] --------------------Validation loss: 0.002330\n",
      "Epoch--[257] --------------------Training loss: 0.002340\n",
      "Epoch--[257] --------------------Validation loss: 0.002329\n",
      "Epoch--[258] --------------------Training loss: 0.002340\n",
      "Epoch--[258] --------------------Validation loss: 0.002329\n",
      "Epoch--[259] --------------------Training loss: 0.002339\n",
      "Epoch--[259] --------------------Validation loss: 0.002328\n",
      "Epoch--[260] --------------------Training loss: 0.002339\n",
      "Epoch--[260] --------------------Validation loss: 0.002328\n",
      "Epoch--[261] --------------------Training loss: 0.002339\n",
      "Epoch--[261] --------------------Validation loss: 0.002327\n",
      "Epoch--[262] --------------------Training loss: 0.002338\n",
      "Epoch--[262] --------------------Validation loss: 0.002327\n",
      "Epoch--[263] --------------------Training loss: 0.002338\n",
      "Epoch--[263] --------------------Validation loss: 0.002327\n",
      "Epoch--[264] --------------------Training loss: 0.002337\n",
      "Epoch--[264] --------------------Validation loss: 0.002326\n",
      "Epoch--[265] --------------------Training loss: 0.002337\n",
      "Epoch--[265] --------------------Validation loss: 0.002326\n",
      "Epoch--[266] --------------------Training loss: 0.002336\n",
      "Epoch--[266] --------------------Validation loss: 0.002325\n",
      "Epoch--[267] --------------------Training loss: 0.002336\n",
      "Epoch--[267] --------------------Validation loss: 0.002325\n",
      "Epoch--[268] --------------------Training loss: 0.002335\n",
      "Epoch--[268] --------------------Validation loss: 0.002325\n",
      "Epoch--[269] --------------------Training loss: 0.002335\n",
      "Epoch--[269] --------------------Validation loss: 0.002324\n",
      "Epoch--[270] --------------------Training loss: 0.002335\n",
      "Epoch--[270] --------------------Validation loss: 0.002324\n",
      "Epoch--[271] --------------------Training loss: 0.002335\n",
      "Epoch--[271] --------------------Validation loss: 0.002323\n",
      "Epoch--[272] --------------------Training loss: 0.002334\n",
      "Epoch--[272] --------------------Validation loss: 0.002323\n",
      "Epoch--[273] --------------------Training loss: 0.002334\n",
      "Epoch--[273] --------------------Validation loss: 0.002323\n",
      "Epoch--[274] --------------------Training loss: 0.002333\n",
      "Epoch--[274] --------------------Validation loss: 0.002322\n",
      "Epoch--[275] --------------------Training loss: 0.002333\n",
      "Epoch--[275] --------------------Validation loss: 0.002322\n",
      "Epoch--[276] --------------------Training loss: 0.002333\n",
      "Epoch--[276] --------------------Validation loss: 0.002322\n",
      "Epoch--[277] --------------------Training loss: 0.002333\n",
      "Epoch--[277] --------------------Validation loss: 0.002321\n",
      "Epoch--[278] --------------------Training loss: 0.002332\n",
      "Epoch--[278] --------------------Validation loss: 0.002321\n",
      "Epoch--[279] --------------------Training loss: 0.002332\n",
      "Epoch--[279] --------------------Validation loss: 0.002321\n",
      "Epoch--[280] --------------------Training loss: 0.002332\n",
      "Epoch--[280] --------------------Validation loss: 0.002321\n",
      "Epoch--[281] --------------------Training loss: 0.002331\n",
      "Epoch--[281] --------------------Validation loss: 0.002320\n",
      "Epoch--[282] --------------------Training loss: 0.002331\n",
      "Epoch--[282] --------------------Validation loss: 0.002320\n",
      "Epoch--[283] --------------------Training loss: 0.002331\n",
      "Epoch--[283] --------------------Validation loss: 0.002320\n",
      "Epoch--[284] --------------------Training loss: 0.002331\n",
      "Epoch--[284] --------------------Validation loss: 0.002319\n",
      "Epoch--[285] --------------------Training loss: 0.002330\n",
      "Epoch--[285] --------------------Validation loss: 0.002319\n",
      "Epoch--[286] --------------------Training loss: 0.002330\n",
      "Epoch--[286] --------------------Validation loss: 0.002319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--[287] --------------------Training loss: 0.002330\n",
      "Epoch--[287] --------------------Validation loss: 0.002319\n",
      "Epoch--[288] --------------------Training loss: 0.002329\n",
      "Epoch--[288] --------------------Validation loss: 0.002318\n",
      "Epoch--[289] --------------------Training loss: 0.002329\n",
      "Epoch--[289] --------------------Validation loss: 0.002318\n",
      "Epoch--[290] --------------------Training loss: 0.002329\n",
      "Epoch--[290] --------------------Validation loss: 0.002318\n",
      "Epoch--[291] --------------------Training loss: 0.002329\n",
      "Epoch--[291] --------------------Validation loss: 0.002318\n",
      "Epoch--[292] --------------------Training loss: 0.002328\n",
      "Epoch--[292] --------------------Validation loss: 0.002318\n",
      "Epoch--[293] --------------------Training loss: 0.002328\n",
      "Epoch--[293] --------------------Validation loss: 0.002317\n",
      "Epoch--[294] --------------------Training loss: 0.002328\n",
      "Epoch--[294] --------------------Validation loss: 0.002317\n",
      "Epoch--[295] --------------------Training loss: 0.002328\n",
      "Epoch--[295] --------------------Validation loss: 0.002317\n",
      "Epoch--[296] --------------------Training loss: 0.002328\n",
      "Epoch--[296] --------------------Validation loss: 0.002317\n",
      "Epoch--[297] --------------------Training loss: 0.002328\n",
      "Epoch--[297] --------------------Validation loss: 0.002316\n",
      "Epoch--[298] --------------------Training loss: 0.002327\n",
      "Epoch--[298] --------------------Validation loss: 0.002316\n",
      "Epoch--[299] --------------------Training loss: 0.002327\n",
      "Epoch--[299] --------------------Validation loss: 0.002316\n",
      "Epoch--[300] --------------------Training loss: 0.002327\n",
      "Epoch--[300] --------------------Validation loss: 0.002316\n",
      "Epoch--[301] --------------------Training loss: 0.002327\n",
      "Epoch--[301] --------------------Validation loss: 0.002316\n",
      "Epoch--[302] --------------------Training loss: 0.002327\n",
      "Epoch--[302] --------------------Validation loss: 0.002316\n",
      "Epoch--[303] --------------------Training loss: 0.002326\n",
      "Epoch--[303] --------------------Validation loss: 0.002315\n",
      "Epoch--[304] --------------------Training loss: 0.002326\n",
      "Epoch--[304] --------------------Validation loss: 0.002315\n",
      "Epoch--[305] --------------------Training loss: 0.002326\n",
      "Epoch--[305] --------------------Validation loss: 0.002315\n",
      "Epoch--[306] --------------------Training loss: 0.002326\n",
      "Epoch--[306] --------------------Validation loss: 0.002315\n",
      "Epoch--[307] --------------------Training loss: 0.002326\n",
      "Epoch--[307] --------------------Validation loss: 0.002315\n",
      "Epoch--[308] --------------------Training loss: 0.002326\n",
      "Epoch--[308] --------------------Validation loss: 0.002315\n",
      "Epoch--[309] --------------------Training loss: 0.002326\n",
      "Epoch--[309] --------------------Validation loss: 0.002315\n",
      "Epoch--[310] --------------------Training loss: 0.002325\n",
      "Epoch--[310] --------------------Validation loss: 0.002314\n",
      "Epoch--[311] --------------------Training loss: 0.002325\n",
      "Epoch--[311] --------------------Validation loss: 0.002314\n",
      "Epoch--[312] --------------------Training loss: 0.002325\n",
      "Epoch--[312] --------------------Validation loss: 0.002314\n",
      "Epoch--[313] --------------------Training loss: 0.002325\n",
      "Epoch--[313] --------------------Validation loss: 0.002314\n",
      "Epoch--[314] --------------------Training loss: 0.002325\n",
      "Epoch--[314] --------------------Validation loss: 0.002314\n",
      "Epoch--[315] --------------------Training loss: 0.002325\n",
      "Epoch--[315] --------------------Validation loss: 0.002314\n",
      "Epoch--[316] --------------------Training loss: 0.002324\n",
      "Epoch--[316] --------------------Validation loss: 0.002314\n",
      "Epoch--[317] --------------------Training loss: 0.002324\n",
      "Epoch--[317] --------------------Validation loss: 0.002314\n",
      "Epoch--[318] --------------------Training loss: 0.002324\n",
      "Epoch--[318] --------------------Validation loss: 0.002313\n",
      "Epoch--[319] --------------------Training loss: 0.002324\n",
      "Epoch--[319] --------------------Validation loss: 0.002313\n",
      "Epoch--[320] --------------------Training loss: 0.002324\n",
      "Epoch--[320] --------------------Validation loss: 0.002313\n",
      "Epoch--[321] --------------------Training loss: 0.002324\n",
      "Epoch--[321] --------------------Validation loss: 0.002313\n",
      "Epoch--[322] --------------------Training loss: 0.002324\n",
      "Epoch--[322] --------------------Validation loss: 0.002313\n",
      "Epoch--[323] --------------------Training loss: 0.002324\n",
      "Epoch--[323] --------------------Validation loss: 0.002313\n",
      "Epoch--[324] --------------------Training loss: 0.002324\n",
      "Epoch--[324] --------------------Validation loss: 0.002313\n",
      "Epoch--[325] --------------------Training loss: 0.002324\n",
      "Epoch--[325] --------------------Validation loss: 0.002313\n",
      "Epoch--[326] --------------------Training loss: 0.002324\n",
      "Epoch--[326] --------------------Validation loss: 0.002313\n",
      "Epoch--[327] --------------------Training loss: 0.002323\n",
      "Epoch--[327] --------------------Validation loss: 0.002313\n",
      "Epoch--[328] --------------------Training loss: 0.002323\n",
      "Epoch--[328] --------------------Validation loss: 0.002312\n",
      "Epoch--[329] --------------------Training loss: 0.002323\n",
      "Epoch--[329] --------------------Validation loss: 0.002312\n",
      "Epoch--[330] --------------------Training loss: 0.002323\n",
      "Epoch--[330] --------------------Validation loss: 0.002312\n",
      "Epoch--[331] --------------------Training loss: 0.002323\n",
      "Epoch--[331] --------------------Validation loss: 0.002312\n",
      "Epoch--[332] --------------------Training loss: 0.002323\n",
      "Epoch--[332] --------------------Validation loss: 0.002312\n",
      "Epoch--[333] --------------------Training loss: 0.002323\n",
      "Epoch--[333] --------------------Validation loss: 0.002312\n",
      "Epoch--[334] --------------------Training loss: 0.002323\n",
      "Epoch--[334] --------------------Validation loss: 0.002312\n",
      "Epoch--[335] --------------------Training loss: 0.002323\n",
      "Epoch--[335] --------------------Validation loss: 0.002312\n",
      "Epoch--[336] --------------------Training loss: 0.002323\n",
      "Epoch--[336] --------------------Validation loss: 0.002312\n",
      "Epoch--[337] --------------------Training loss: 0.002323\n",
      "Epoch--[337] --------------------Validation loss: 0.002312\n",
      "Epoch--[338] --------------------Training loss: 0.002322\n",
      "Epoch--[338] --------------------Validation loss: 0.002312\n",
      "Epoch--[339] --------------------Training loss: 0.002322\n",
      "Epoch--[339] --------------------Validation loss: 0.002312\n",
      "Epoch--[340] --------------------Training loss: 0.002322\n",
      "Epoch--[340] --------------------Validation loss: 0.002312\n",
      "Epoch--[341] --------------------Training loss: 0.002322\n",
      "Epoch--[341] --------------------Validation loss: 0.002311\n",
      "Epoch--[342] --------------------Training loss: 0.002322\n",
      "Epoch--[342] --------------------Validation loss: 0.002311\n",
      "Epoch--[343] --------------------Training loss: 0.002322\n",
      "Epoch--[343] --------------------Validation loss: 0.002311\n",
      "Epoch--[344] --------------------Training loss: 0.002322\n",
      "Epoch--[344] --------------------Validation loss: 0.002311\n",
      "Epoch--[345] --------------------Training loss: 0.002322\n",
      "Epoch--[345] --------------------Validation loss: 0.002311\n",
      "Epoch--[346] --------------------Training loss: 0.002322\n",
      "Epoch--[346] --------------------Validation loss: 0.002311\n",
      "Epoch--[347] --------------------Training loss: 0.002322\n",
      "Epoch--[347] --------------------Validation loss: 0.002311\n",
      "Epoch--[348] --------------------Training loss: 0.002322\n",
      "Epoch--[348] --------------------Validation loss: 0.002311\n",
      "Epoch--[349] --------------------Training loss: 0.002322\n",
      "Epoch--[349] --------------------Validation loss: 0.002311\n",
      "Epoch--[350] --------------------Training loss: 0.002322\n",
      "Epoch--[350] --------------------Validation loss: 0.002311\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "trainingLoss= []\n",
    "validationLoss= []\n",
    "\n",
    "for epoch in range(350):\n",
    "    running_loss= 0.0\n",
    "    for i, (feature,lable) in enumerate(nonFraudulentTrainLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs= torch.tensor(feature)\n",
    "        lables= torch.tensor(lable)\n",
    "        lables= lables.type(torch.LongTensor)\n",
    "        \n",
    "        # =====================forward====================\n",
    "        output = model(inputs)\n",
    "        loss= criterion(output,inputs)\n",
    "     \n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # =======print the statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch--[%d] --------------------Training loss: %.6f' %(epoch + 1,  running_loss /nonFraudX_trainNew.shape[0]))\n",
    "    \n",
    "    #test the model on the validation set.\n",
    "    valid_loss= testModel(nonFraudulentValidationLoader)\n",
    "    \n",
    "    print('Epoch--[%d] --------------------Validation loss: %.6f' %(epoch + 1,  valid_loss))\n",
    "    #append the loss after each epoch\n",
    "    trainingLoss.append(running_loss/nonFraudX_trainNew.shape[0])\n",
    "    validationLoss.append(valid_loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingLoss= pd.DataFrame(trainingLoss)\n",
    "validationLoss= pd.DataFrame(validationLoss)\n",
    "\n",
    "#save the dataframe\n",
    "trainingLoss.to_csv('Batch8TrainingLoss.csv',index=None,sep='\\t')\n",
    "validationLoss.to_csv('Batch8ValidationLoss.csv',index=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for testing the model\n",
    "def testModel2(loader):\n",
    "    predictedLabel= []\n",
    "    lossList= []\n",
    "    trueLable= []\n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model(inputs)\n",
    "        \n",
    "        trueLable.append(lable)\n",
    "        \n",
    "        loss= criterion(output,inputs)\n",
    "        lossList.append(loss.item())\n",
    "        \n",
    "        if(loss.item()<0.01):\n",
    "            predictedLabel.append(0)\n",
    "        else:\n",
    "            predictedLabel.append(1)\n",
    "    return predictedLabel,lossList,trueLable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on test set\n",
    "predictedLable1,lossList1,trueLable1= testModel2(myNonFraudulentTestDataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the predicted lables\n",
    "class1= predictedLable1.count(1)\n",
    "class0= predictedLable1.count(0)\n",
    "print(class1)\n",
    "print(class0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on test set\n",
    "predictedLable2,lossList2,trueLable2= testModel2(myFraudulentTestDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the predicted lables\n",
    "class1= predictedLable2.count(1)\n",
    "class0= predictedLable2.count(0)\n",
    "print(class1)\n",
    "print(class0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to csv\n",
    "nonFraudLoss= pd.DataFrame(lossList1)\n",
    "fraudLoss= pd.DataFrame(lossList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonFraudLoss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataLoss:\n",
    "nonFraudLoss.to_csv('nonFraudLoss.csv',index=None,sep='\\t')\n",
    "fraudLoss.to_csv('fraudLoss.csv',index=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalLength1= len(trueLable1)\n",
    "TP=FP=FN=TN= 0\n",
    "for i in range(totalLength1):\n",
    "    if(int(trueLable1[i])==1 and predictedLable1[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable1[i])==1 and predictedLable1[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable1[i])==0 and predictedLable1[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable1[i])==0 and predictedLable1[i]==1):\n",
    "        FP += 1\n",
    "\n",
    "totalLength2= len(trueLable2)\n",
    "for i in range(totalLength2):\n",
    "    if(int(trueLable2[i])==1 and predictedLable2[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable2[i])==1 and predictedLable2[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable2[i])==0 and predictedLable2[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable2[i])==0 and predictedLable2[i]==1):\n",
    "        FP += 1\n",
    "        \n",
    "print(\"\\nResult \")\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)\n",
    "print(\"\\n\")\n",
    "print(\"\\nTP:\",TP)\n",
    "print(\"\\nFN:\",FN)\n",
    "print(\"\\nFP:\",FP)\n",
    "print(\"\\nTN:\",TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
