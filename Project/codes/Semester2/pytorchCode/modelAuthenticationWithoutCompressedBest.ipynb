{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import preprocessingWithMissingvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15420 entries, 0 to 15419\n",
      "Data columns (total 33 columns):\n",
      "Month                   15420 non-null object\n",
      "WeekOfMonth             15420 non-null int64\n",
      "DayOfWeek               15420 non-null object\n",
      "Make                    15420 non-null object\n",
      "AccidentArea            15420 non-null object\n",
      "DayOfWeekClaimed        15420 non-null object\n",
      "MonthClaimed            15420 non-null object\n",
      "WeekOfMonthClaimed      15420 non-null int64\n",
      "Sex                     15420 non-null object\n",
      "MaritalStatus           15420 non-null object\n",
      "Age                     15420 non-null int64\n",
      "Fault                   15420 non-null object\n",
      "PolicyType              15420 non-null object\n",
      "VehicleCategory         15420 non-null object\n",
      "VehiclePrice            15420 non-null object\n",
      "FraudFound              15420 non-null object\n",
      "PolicyNumber            15420 non-null int64\n",
      "RepNumber               15420 non-null int64\n",
      "Deductible              15420 non-null int64\n",
      "DriverRating            15420 non-null int64\n",
      "Days:Policy-Accident    15420 non-null object\n",
      "Days:Policy-Claim       15420 non-null object\n",
      "PastNumberOfClaims      15420 non-null object\n",
      "AgeOfVehicle            15420 non-null object\n",
      "AgeOfPolicyHolder       15420 non-null object\n",
      "PoliceReportFiled       15420 non-null object\n",
      "WitnessPresent          15420 non-null object\n",
      "AgentType               15420 non-null object\n",
      "NumberOfSuppliments     15420 non-null object\n",
      "AddressChange-Claim     15420 non-null object\n",
      "NumberOfCars            15420 non-null object\n",
      "Year                    15420 non-null int64\n",
      "BasePolicy              15420 non-null object\n",
      "dtypes: int64(8), object(25)\n",
      "memory usage: 3.9+ MB\n",
      "index:  1516\n",
      "\n",
      "index: 1516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "carDf= preprocessingWithMissingvalues.preprocess('./cardata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the lable from the dataset\n",
    "carDf.head()\n",
    "carLable= carDf['Lable']\n",
    "carDf.drop(['Lable'],inplace=True,axis=1) #drop the lable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "carDateNormalized= carDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudulent: <class 'pandas.core.frame.DataFrame'>\n",
      "carDateNormalized: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Divide the data into fraudulent and non-fraudulent data\n",
    "columnlable= list(carDateNormalized.columns.values)\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "nonFraudulent= pd.DataFrame(columns=columnlable)\n",
    "nonFraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "fraudulent= pd.DataFrame(columns=columnlable)\n",
    "fraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "print(\"nonFraudulent:\",type(nonFraudulent))\n",
    "print(\"carDateNormalized:\",type(carDateNormalized))\n",
    "\n",
    "j= 0\n",
    "k= 0\n",
    "for i in range(carDateNormalized.shape[0]):\n",
    "    if(carLable[i]==0):\n",
    "        nonFraudulent.loc[j]= carDateNormalized.loc[i]\n",
    "        nonFraudulentLable.loc[j]= 0.0\n",
    "        j += 1\n",
    "    else:\n",
    "        fraudulent.loc[k]= carDateNormalized.loc[i]\n",
    "        fraudulentLable.loc[i]= 1.0\n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nonFraudulent))\n",
    "print(type(fraudulent))\n",
    "\n",
    "#convert to dataframe\n",
    "nonFraudulentLable= pd.DataFrame(nonFraudulentLable)\n",
    "fraudulentLable= pd.DataFrame(fraudulentLable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(3624, 97)\n",
      "fraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(231, 97)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into train and test set in equal proportion\n",
    "#divide the nonFraudulent into test and train\n",
    "nonFraudX_train,nonFraudX_test,nonFraudY_train,nonFraudY_test = train_test_split(nonFraudulent,nonFraudulentLable,random_state=3,test_size=0.25)\n",
    "\n",
    "print('nonFraudxtrain:',type(nonFraudX_train))\n",
    "print(nonFraudX_test.shape)\n",
    "\n",
    "#divide the fraudulent into test and train\n",
    "fraudX_train,fraudX_test,fraudY_train,fraudY_test = train_test_split(fraudulent, fraudulentLable,random_state=3,test_size=0.25)\n",
    "\n",
    "print('fraudxtrain:',type(fraudX_train))\n",
    "print(fraudX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData set size: 11564\n",
      "Test Data set size: 3855\n"
     ]
    }
   ],
   "source": [
    "#combine the fraudulent and nonfraudulent training and test set to make the newTrain and newTest\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "newCombineTrain= pd.DataFrame(columns=columnlable)\n",
    "newCombineTrainLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "newCombineTest= pd.DataFrame(columns=columnlable)\n",
    "newCombineTestLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "\n",
    "\n",
    "#combine trainset\n",
    "framesFeatures= [nonFraudX_train,fraudX_train]\n",
    "framesLable= [nonFraudY_train,fraudY_train]\n",
    "newCombineTrain= pd.concat(framesFeatures)\n",
    "newCombineTrainLable= pd.concat(framesLable)\n",
    "\n",
    "\n",
    "framesFeatures2= [nonFraudX_test,fraudX_test]\n",
    "framesLable2= [nonFraudY_test,fraudY_test]\n",
    "newCombineTest= pd.concat(framesFeatures2)\n",
    "newCombineTestLable= pd.concat(framesLable2)\n",
    "\n",
    "#make the dataset as values\n",
    "newCombineTrain= newCombineTrain.values\n",
    "newCombineTrainLable= newCombineTrainLable.values\n",
    "\n",
    "newCombineTest= newCombineTest.values\n",
    "newCombineTestLable= newCombineTestLable.values\n",
    "\n",
    "print(\"TrainData set size:\",newCombineTrain.shape[0])\n",
    "print(\"Test Data set size:\",newCombineTest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class Dataset which returns the data and labels\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,features,lables,transform= None):\n",
    "        dataTensor= []\n",
    "        lableTensor= []\n",
    "        dataSize= features.shape[0]\n",
    "        \n",
    "        for data in range(dataSize):\n",
    "            feature= features[data,:]\n",
    "            #feature= torch.from_numpy(feature).float()\n",
    "            feature= torch.Tensor(feature)\n",
    "            dataTensor.append(feature)\n",
    "            \n",
    "            lable= np.asanyarray(lables[data])\n",
    "            lable= torch.from_numpy(lable).float()\n",
    "           \n",
    "            #lable= torch.Tensor(lable)\n",
    "            #print(\"lable:\",lable)\n",
    "            #assert(False)\n",
    "           \n",
    "            lableTensor.append(lable)\n",
    "        \n",
    "        #put everything in features and lables\n",
    "        self.features= dataTensor\n",
    "        self.lables= lableTensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        feature= self.features[index]\n",
    "        lable= self.lables[index]\n",
    "        #print(\"get_item feature:\",feature)\n",
    "        #print(\"get_item lable:\",lable)\n",
    "        return feature,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the for train and test loader\n",
    "myTrainDataset= myDataset(newCombineTrain,newCombineTrainLable)\n",
    "myTestDataset= myDataset(newCombineTest,newCombineTestLable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the trainloader and test loader for nonfraudulent dataset.\n",
    "trainLoader= torch.utils.data.DataLoader(myTrainDataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "testLoader= torch.utils.data.DataLoader(myTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "newTrainLoader= torch.utils.data.DataLoader(myTrainDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.fc1= nn.Linear(97,72)\n",
    "        self.act1= nn.LeakyReLU()\n",
    "        self.fc2= nn.Linear(72,48)\n",
    "        self.act2= nn.LeakyReLU()\n",
    "        self.fc3= nn.Linear(48,24)\n",
    "        self.act3= nn.LeakyReLU()\n",
    "        self.fc4= nn.Linear(24,16)\n",
    "        self.act4= nn.LeakyReLU()\n",
    "        self.fc5= nn.Linear(16,2)\n",
    "        self.act5= nn.Sigmoid();\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x= self.fc1(x)\n",
    "        x= self.act1(x)\n",
    "        x= self.fc2(x)\n",
    "        x= self.act2(x)\n",
    "        x= self.fc3(x)\n",
    "        x= self.act3(x)\n",
    "        x= self.fc4(x)\n",
    "        x= self.act4(x)\n",
    "        x= self.fc5(x)\n",
    "        x= self.act5(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Network()\n",
    "#criterion= nn.CrossEntropyLoss()\n",
    "criterion= nn.BCELoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/usr/local/anaconda35/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.041837\n",
      "[2] loss: 0.036105\n",
      "[3] loss: 0.031319\n",
      "[4] loss: 0.027293\n",
      "[5] loss: 0.023955\n",
      "[6] loss: 0.021257\n",
      "[7] loss: 0.019153\n",
      "[8] loss: 0.017570\n",
      "[9] loss: 0.016426\n",
      "[10] loss: 0.015624\n",
      "[11] loss: 0.015078\n",
      "[12] loss: 0.014721\n",
      "[13] loss: 0.014492\n",
      "[14] loss: 0.014349\n",
      "[15] loss: 0.014261\n",
      "[16] loss: 0.014208\n",
      "[17] loss: 0.014171\n",
      "[18] loss: 0.014152\n",
      "[19] loss: 0.014139\n",
      "[20] loss: 0.014135\n",
      "[21] loss: 0.014128\n",
      "[22] loss: 0.014117\n",
      "[23] loss: 0.014122\n",
      "[24] loss: 0.014118\n",
      "[25] loss: 0.014109\n",
      "[26] loss: 0.014105\n",
      "[27] loss: 0.014096\n",
      "[28] loss: 0.014092\n",
      "[29] loss: 0.014088\n",
      "[30] loss: 0.014088\n",
      "[31] loss: 0.014079\n",
      "[32] loss: 0.014090\n",
      "[33] loss: 0.014071\n",
      "[34] loss: 0.014066\n",
      "[35] loss: 0.014062\n",
      "[36] loss: 0.014062\n",
      "[37] loss: 0.014057\n",
      "[38] loss: 0.014053\n",
      "[39] loss: 0.014048\n",
      "[40] loss: 0.014043\n",
      "[41] loss: 0.014038\n",
      "[42] loss: 0.014028\n",
      "[43] loss: 0.014024\n",
      "[44] loss: 0.014033\n",
      "[45] loss: 0.014013\n",
      "[46] loss: 0.014012\n",
      "[47] loss: 0.014011\n",
      "[48] loss: 0.013996\n",
      "[49] loss: 0.014005\n",
      "[50] loss: 0.013989\n",
      "[51] loss: 0.013977\n",
      "[52] loss: 0.013972\n",
      "[53] loss: 0.013964\n",
      "[54] loss: 0.013958\n",
      "[55] loss: 0.013950\n",
      "[56] loss: 0.013948\n",
      "[57] loss: 0.013935\n",
      "[58] loss: 0.013928\n",
      "[59] loss: 0.013919\n",
      "[60] loss: 0.013911\n",
      "[61] loss: 0.013912\n",
      "[62] loss: 0.013899\n",
      "[63] loss: 0.013885\n",
      "[64] loss: 0.013875\n",
      "[65] loss: 0.013870\n",
      "[66] loss: 0.013855\n",
      "[67] loss: 0.013845\n",
      "[68] loss: 0.013834\n",
      "[69] loss: 0.013823\n",
      "[70] loss: 0.013821\n",
      "[71] loss: 0.013804\n",
      "[72] loss: 0.013791\n",
      "[73] loss: 0.013774\n",
      "[74] loss: 0.013765\n",
      "[75] loss: 0.013756\n",
      "[76] loss: 0.013741\n",
      "[77] loss: 0.013727\n",
      "[78] loss: 0.013706\n",
      "[79] loss: 0.013695\n",
      "[80] loss: 0.013678\n",
      "[81] loss: 0.013651\n",
      "[82] loss: 0.013632\n",
      "[83] loss: 0.013623\n",
      "[84] loss: 0.013593\n",
      "[85] loss: 0.013572\n",
      "[86] loss: 0.013550\n",
      "[87] loss: 0.013532\n",
      "[88] loss: 0.013510\n",
      "[89] loss: 0.013486\n",
      "[90] loss: 0.013461\n",
      "[91] loss: 0.013430\n",
      "[92] loss: 0.013403\n",
      "[93] loss: 0.013381\n",
      "[94] loss: 0.013352\n",
      "[95] loss: 0.013328\n",
      "[96] loss: 0.013292\n",
      "[97] loss: 0.013270\n",
      "[98] loss: 0.013232\n",
      "[99] loss: 0.013195\n",
      "[100] loss: 0.013156\n",
      "[101] loss: 0.013121\n",
      "[102] loss: 0.013089\n",
      "[103] loss: 0.013048\n",
      "[104] loss: 0.013011\n",
      "[105] loss: 0.012973\n",
      "[106] loss: 0.012939\n",
      "[107] loss: 0.012896\n",
      "[108] loss: 0.012858\n",
      "[109] loss: 0.012822\n",
      "[110] loss: 0.012783\n",
      "[111] loss: 0.012746\n",
      "[112] loss: 0.012706\n",
      "[113] loss: 0.012663\n",
      "[114] loss: 0.012625\n",
      "[115] loss: 0.012591\n",
      "[116] loss: 0.012549\n",
      "[117] loss: 0.012512\n",
      "[118] loss: 0.012480\n",
      "[119] loss: 0.012440\n",
      "[120] loss: 0.012414\n",
      "[121] loss: 0.012371\n",
      "[122] loss: 0.012347\n",
      "[123] loss: 0.012309\n",
      "[124] loss: 0.012283\n",
      "[125] loss: 0.012243\n",
      "[126] loss: 0.012220\n",
      "[127] loss: 0.012191\n",
      "[128] loss: 0.012170\n",
      "[129] loss: 0.012142\n",
      "[130] loss: 0.012109\n",
      "[131] loss: 0.012089\n",
      "[132] loss: 0.012065\n",
      "[133] loss: 0.012043\n",
      "[134] loss: 0.012026\n",
      "[135] loss: 0.012009\n",
      "[136] loss: 0.012002\n",
      "[137] loss: 0.011985\n",
      "[138] loss: 0.011961\n",
      "[139] loss: 0.011941\n",
      "[140] loss: 0.011931\n",
      "[141] loss: 0.011915\n",
      "[142] loss: 0.011903\n",
      "[143] loss: 0.011890\n",
      "[144] loss: 0.011878\n",
      "[145] loss: 0.011866\n",
      "[146] loss: 0.011856\n",
      "[147] loss: 0.011846\n",
      "[148] loss: 0.011841\n",
      "[149] loss: 0.011830\n",
      "[150] loss: 0.011816\n",
      "[151] loss: 0.011811\n",
      "[152] loss: 0.011800\n",
      "[153] loss: 0.011796\n",
      "[154] loss: 0.011781\n",
      "[155] loss: 0.011774\n",
      "[156] loss: 0.011764\n",
      "[157] loss: 0.011761\n",
      "[158] loss: 0.011749\n",
      "[159] loss: 0.011742\n",
      "[160] loss: 0.011732\n",
      "[161] loss: 0.011731\n",
      "[162] loss: 0.011720\n",
      "[163] loss: 0.011713\n",
      "[164] loss: 0.011708\n",
      "[165] loss: 0.011707\n",
      "[166] loss: 0.011690\n",
      "[167] loss: 0.011684\n",
      "[168] loss: 0.011676\n",
      "[169] loss: 0.011676\n",
      "[170] loss: 0.011663\n",
      "[171] loss: 0.011665\n",
      "[172] loss: 0.011653\n",
      "[173] loss: 0.011659\n",
      "[174] loss: 0.011643\n",
      "[175] loss: 0.011639\n",
      "[176] loss: 0.011631\n",
      "[177] loss: 0.011617\n",
      "[178] loss: 0.011613\n",
      "[179] loss: 0.011611\n",
      "[180] loss: 0.011602\n",
      "[181] loss: 0.011595\n",
      "[182] loss: 0.011588\n",
      "[183] loss: 0.011586\n",
      "[184] loss: 0.011583\n",
      "[185] loss: 0.011568\n",
      "[186] loss: 0.011570\n",
      "[187] loss: 0.011565\n",
      "[188] loss: 0.011562\n",
      "[189] loss: 0.011551\n",
      "[190] loss: 0.011539\n",
      "[191] loss: 0.011536\n",
      "[192] loss: 0.011533\n",
      "[193] loss: 0.011529\n",
      "[194] loss: 0.011522\n",
      "[195] loss: 0.011522\n",
      "[196] loss: 0.011513\n",
      "[197] loss: 0.011513\n",
      "[198] loss: 0.011502\n",
      "[199] loss: 0.011499\n",
      "[200] loss: 0.011488\n",
      "[201] loss: 0.011492\n",
      "[202] loss: 0.011476\n",
      "[203] loss: 0.011483\n",
      "[204] loss: 0.011470\n",
      "[205] loss: 0.011467\n",
      "[206] loss: 0.011457\n",
      "[207] loss: 0.011453\n",
      "[208] loss: 0.011460\n",
      "[209] loss: 0.011443\n",
      "[210] loss: 0.011442\n",
      "[211] loss: 0.011447\n",
      "[212] loss: 0.011435\n",
      "[213] loss: 0.011428\n",
      "[214] loss: 0.011424\n",
      "[215] loss: 0.011421\n",
      "[216] loss: 0.011413\n",
      "[217] loss: 0.011413\n",
      "[218] loss: 0.011404\n",
      "[219] loss: 0.011401\n",
      "[220] loss: 0.011399\n",
      "[221] loss: 0.011397\n",
      "[222] loss: 0.011387\n",
      "[223] loss: 0.011385\n",
      "[224] loss: 0.011379\n",
      "[225] loss: 0.011374\n",
      "[226] loss: 0.011370\n",
      "[227] loss: 0.011374\n",
      "[228] loss: 0.011366\n",
      "[229] loss: 0.011357\n",
      "[230] loss: 0.011350\n",
      "[231] loss: 0.011354\n",
      "[232] loss: 0.011349\n",
      "[233] loss: 0.011348\n",
      "[234] loss: 0.011338\n",
      "[235] loss: 0.011346\n",
      "[236] loss: 0.011339\n",
      "[237] loss: 0.011335\n",
      "[238] loss: 0.011335\n",
      "[239] loss: 0.011324\n",
      "[240] loss: 0.011319\n",
      "[241] loss: 0.011322\n",
      "[242] loss: 0.011320\n",
      "[243] loss: 0.011316\n",
      "[244] loss: 0.011307\n",
      "[245] loss: 0.011304\n",
      "[246] loss: 0.011302\n",
      "[247] loss: 0.011300\n",
      "[248] loss: 0.011306\n",
      "[249] loss: 0.011305\n",
      "[250] loss: 0.011290\n",
      "[251] loss: 0.011284\n",
      "[252] loss: 0.011280\n",
      "[253] loss: 0.011281\n",
      "[254] loss: 0.011277\n",
      "[255] loss: 0.011274\n",
      "[256] loss: 0.011274\n",
      "[257] loss: 0.011268\n",
      "[258] loss: 0.011269\n",
      "[259] loss: 0.011262\n",
      "[260] loss: 0.011258\n",
      "[261] loss: 0.011254\n",
      "[262] loss: 0.011258\n",
      "[263] loss: 0.011250\n",
      "[264] loss: 0.011254\n",
      "[265] loss: 0.011241\n",
      "[266] loss: 0.011237\n",
      "[267] loss: 0.011236\n",
      "[268] loss: 0.011237\n",
      "[269] loss: 0.011229\n",
      "[270] loss: 0.011227\n",
      "[271] loss: 0.011233\n",
      "[272] loss: 0.011216\n",
      "[273] loss: 0.011226\n",
      "[274] loss: 0.011213\n",
      "[275] loss: 0.011208\n",
      "[276] loss: 0.011199\n",
      "[277] loss: 0.011209\n",
      "[278] loss: 0.011204\n",
      "[279] loss: 0.011196\n",
      "[280] loss: 0.011193\n",
      "[281] loss: 0.011194\n",
      "[282] loss: 0.011191\n",
      "[283] loss: 0.011181\n",
      "[284] loss: 0.011182\n",
      "[285] loss: 0.011178\n",
      "[286] loss: 0.011183\n",
      "[287] loss: 0.011187\n",
      "[288] loss: 0.011167\n",
      "[289] loss: 0.011161\n",
      "[290] loss: 0.011162\n",
      "[291] loss: 0.011159\n",
      "[292] loss: 0.011158\n",
      "[293] loss: 0.011156\n",
      "[294] loss: 0.011156\n",
      "[295] loss: 0.011145\n",
      "[296] loss: 0.011142\n",
      "[297] loss: 0.011138\n",
      "[298] loss: 0.011140\n",
      "[299] loss: 0.011135\n",
      "[300] loss: 0.011131\n",
      "[301] loss: 0.011131\n",
      "[302] loss: 0.011121\n",
      "[303] loss: 0.011116\n",
      "[304] loss: 0.011127\n",
      "[305] loss: 0.011118\n",
      "[306] loss: 0.011111\n",
      "[307] loss: 0.011109\n",
      "[308] loss: 0.011110\n",
      "[309] loss: 0.011101\n",
      "[310] loss: 0.011101\n",
      "[311] loss: 0.011099\n",
      "[312] loss: 0.011096\n",
      "[313] loss: 0.011095\n",
      "[314] loss: 0.011082\n",
      "[315] loss: 0.011089\n",
      "[316] loss: 0.011076\n",
      "[317] loss: 0.011076\n",
      "[318] loss: 0.011066\n",
      "[319] loss: 0.011065\n",
      "[320] loss: 0.011064\n",
      "[321] loss: 0.011064\n",
      "[322] loss: 0.011069\n",
      "[323] loss: 0.011057\n",
      "[324] loss: 0.011059\n",
      "[325] loss: 0.011056\n",
      "[326] loss: 0.011050\n",
      "[327] loss: 0.011050\n",
      "[328] loss: 0.011042\n",
      "[329] loss: 0.011035\n",
      "[330] loss: 0.011021\n",
      "[331] loss: 0.011026\n",
      "[332] loss: 0.011025\n",
      "[333] loss: 0.011022\n",
      "[334] loss: 0.011027\n",
      "[335] loss: 0.011014\n",
      "[336] loss: 0.011009\n",
      "[337] loss: 0.011006\n",
      "[338] loss: 0.011000\n",
      "[339] loss: 0.010996\n",
      "[340] loss: 0.010995\n",
      "[341] loss: 0.010994\n",
      "[342] loss: 0.010984\n",
      "[343] loss: 0.010990\n",
      "[344] loss: 0.010977\n",
      "[345] loss: 0.010980\n",
      "[346] loss: 0.010969\n",
      "[347] loss: 0.010976\n",
      "[348] loss: 0.010965\n",
      "[349] loss: 0.010955\n",
      "[350] loss: 0.010950\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for epoch in range(350):\n",
    "    validation_loss= []\n",
    "    training_loss= []\n",
    "    running_loss= 0.0\n",
    "    for i, (feature,lable) in enumerate(trainLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs= torch.tensor(feature)\n",
    "        lables= torch.tensor(lable).float()\n",
    "        #lables= lable.type(torch.LongTensor)\n",
    "        #print(\"lables:\",lables)\n",
    "        #print(\"lables.size:\",lables.size())\n",
    "       \n",
    "        # =====================forward====================\n",
    "        output = model(inputs)\n",
    "        \n",
    "        #print(\"output:\",output)\n",
    "        #print(\"output.size:\",output.size())\n",
    "        \n",
    "        #print(\"output.requires_grad:\",output.requires_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        maxIndex= torch.argmax(output,dim=1)\n",
    "        maxProb= []\n",
    "        tensorLength= output.size(0)\n",
    "        print(\"maxIndex:\",maxIndex)\n",
    "        \n",
    "        for i in range(tensorLength):\n",
    "            index= maxIndex[i]\n",
    "            maxProb.append(output[i][index].item())\n",
    "        newMaxProb= torch.tensor(maxProb)\n",
    "        newMaxProb= newMaxProb.view(-1,1)\n",
    "        \"\"\"\n",
    "        \n",
    "        newMaxProb,_ = torch.topk(output,1,dim=1)\n",
    "        #print(\"newMaxProb:\",newMaxProb)\n",
    "        #print(\"newMaxProb.size:\",newMaxProb.size())\n",
    "        #print(\"newMaxProb.requires_grad:\",newMaxProb.requires_grad)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #loss= criterion(output,lables.squeeze())\n",
    "        #output= newMaxProb\n",
    "        loss= criterion(newMaxProb,lables.squeeze())\n",
    "        \n",
    "        #print(\"loss:\",loss)\n",
    "        \n",
    "        # ===================backward====================\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # =======print the statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('[%d] loss: %.6f' %(epoch + 1,  running_loss /newCombineTrain.shape[0]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the model:\n",
    "#function for testing the model\n",
    "def testModel(loader):\n",
    "    predictedLabel= []\n",
    "    maxList= []\n",
    "    trueLable= []\n",
    "    maxProb= []\n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model(inputs)\n",
    "        newMaxProb,maxIndex = torch.topk(output,1,dim=1)\n",
    "        \n",
    "        #combine= torch.sum(output)\n",
    "        #print(\"combine:\",combine)\n",
    "        #assert(False)\n",
    "        '''        print(\"maxIndex:\",maxIndex)\n",
    "        if(i<100):\n",
    "            print(\"output:\",output)\n",
    "            print(\"newMaxProb:\",newMaxProb)\n",
    "            print(\"newMaxProb.size:\",newMaxProb.size())\n",
    "        else:\n",
    "            assert(False)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        trueLable.append(lable)\n",
    "        #predictedLabel.append(maxIndex.item())\n",
    "        #maxList.append(maxProb)\n",
    "        \n",
    "\n",
    "        \n",
    "        if(newMaxProb>0.095):\n",
    "            predictedLabel.append(1)\n",
    "        else:\n",
    "            predictedLabel.append(0)\n",
    "        \n",
    "    \n",
    "    return predictedLabel,trueLable,maxProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on test set\n",
    "predictedLable,trueLable,maxList= testModel(testLoader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "TP: 166\n",
      "\n",
      "FN: 65\n",
      "\n",
      "FP: 978\n",
      "\n",
      "TN: 2646\n",
      "\n",
      "Result \n",
      "Accuracy: 72.94422827496757\n",
      "Sensitivity: 71.86147186147186\n",
      "Specificity: 73.01324503311258\n"
     ]
    }
   ],
   "source": [
    "totalLength= len(trueLable)\n",
    "TP=FP=FN=TN= 0\n",
    "for i in range(totalLength):\n",
    "    if(int(trueLable[i])==1 and predictedLable[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable[i])==1 and predictedLable[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable[i])==0 and predictedLable[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable[i])==0 and predictedLable[i]==1):\n",
    "        FP += 1\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\nTP:\",TP)\n",
    "print(\"\\nFN:\",FN)\n",
    "print(\"\\nFP:\",FP)\n",
    "print(\"\\nTN:\",TN)\n",
    "\n",
    "print(\"\\nResult \")\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for the ranking algorithm to test whether the model is correct or not.\n",
    "def ranking(loader):\n",
    "    \n",
    "    probabilityDic= {}\n",
    "    \n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model(inputs)\n",
    "        #print(lable.item())\n",
    "        #print(\"output:\",output.item())\n",
    "        probabilityDic[output.item()]= lable.item()\n",
    "    return probabilityDic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8f63459466c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilityDic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewTrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of Dictionary:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilityDic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-92a637bf8798>\u001b[0m in \u001b[0;36mranking\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print(lable.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"output:\",output.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprobabilityDic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobabilityDic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "probabilityDic= ranking(newTrainLoader)\n",
    "\n",
    "print(\"Length of Dictionary:\",len(probabilityDic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the \n",
    "#sort the dictionary\n",
    "sorted(probabilityDic.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the bucket\n",
    "dictLength= len(probabilityDic)\n",
    "print(dictLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyList= list(probabilityDic.keys())\n",
    "valueList= list(probabilityDic.values())\n",
    "print(\"length:\",len(keyList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the 10 buckets of equal size except 1\n",
    "bucket1=[]\n",
    "bucket2=[]\n",
    "bucket3=[]\n",
    "bucket4=[]\n",
    "bucket5=[]\n",
    "bucket6=[]\n",
    "bucket7=[]\n",
    "bucket8=[]\n",
    "bucket9=[]\n",
    "bucket10=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keyList)):\n",
    "    if(i<1159):\n",
    "        bucket1.append(valueList[i])\n",
    "    elif(i<2309):\n",
    "        bucket2.append(valueList[i])\n",
    "    elif(i<3459):\n",
    "        bucket3.append(valueList[i])\n",
    "    elif(i<4609):\n",
    "        bucket4.append(valueList[i])\n",
    "    elif(i<5759):\n",
    "        bucket5.append(valueList[i])\n",
    "    elif(i<6909):\n",
    "        bucket6.append(valueList[i])\n",
    "    elif(i<8059):\n",
    "        bucket7.append(valueList[i])\n",
    "    elif(i<9209):\n",
    "        bucket8.append(valueList[i])\n",
    "    elif(i<10359):\n",
    "        bucket9.append(valueList[i])\n",
    "    else:\n",
    "        bucket10.append(valueList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket1_OneCount= bucket1.count(1)\n",
    "print(\"bucket1_OneCount:\",bucket1_OneCount)\n",
    "\n",
    "bucket2_OneCount= bucket2.count(1)\n",
    "print(\"bucket2_OneCount:\",bucket2_OneCount)\n",
    "\n",
    "bucket3_OneCount= bucket3.count(1)\n",
    "print(\"bucket3_OneCount:\",bucket3_OneCount)\n",
    "\n",
    "bucket4_OneCount= bucket4.count(1)\n",
    "print(\"bucket4_OneCount:\",bucket4_OneCount)\n",
    "\n",
    "bucket5_OneCount= bucket5.count(1)\n",
    "print(\"bucket5_OneCount:\",bucket5_OneCount)\n",
    "\n",
    "bucket6_OneCount= bucket6.count(1)\n",
    "print(\"bucket6_OneCount:\",bucket6_OneCount)\n",
    "\n",
    "bucket7_OneCount= bucket7.count(1)\n",
    "print(\"bucket7_OneCount:\",bucket7_OneCount)\n",
    "\n",
    "bucket8_OneCount= bucket8.count(1)\n",
    "print(\"bucket8_OneCount:\",bucket8_OneCount)\n",
    "\n",
    "bucket9_OneCount= bucket9.count(1)\n",
    "print(\"bucket9_OneCount:\",bucket9_OneCount)\n",
    "\n",
    "bucket10_OneCount= bucket10.count(1)\n",
    "print(\"bucket10_OneCount:\",bucket10_OneCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
