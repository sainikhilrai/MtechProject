{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15420 entries, 0 to 15419\n",
      "Data columns (total 33 columns):\n",
      "Month                   15420 non-null object\n",
      "WeekOfMonth             15420 non-null int64\n",
      "DayOfWeek               15420 non-null object\n",
      "Make                    15420 non-null object\n",
      "AccidentArea            15420 non-null object\n",
      "DayOfWeekClaimed        15420 non-null object\n",
      "MonthClaimed            15420 non-null object\n",
      "WeekOfMonthClaimed      15420 non-null int64\n",
      "Sex                     15420 non-null object\n",
      "MaritalStatus           15420 non-null object\n",
      "Age                     15420 non-null int64\n",
      "Fault                   15420 non-null object\n",
      "PolicyType              15420 non-null object\n",
      "VehicleCategory         15420 non-null object\n",
      "VehiclePrice            15420 non-null object\n",
      "FraudFound              15420 non-null object\n",
      "PolicyNumber            15420 non-null int64\n",
      "RepNumber               15420 non-null int64\n",
      "Deductible              15420 non-null int64\n",
      "DriverRating            15420 non-null int64\n",
      "Days:Policy-Accident    15420 non-null object\n",
      "Days:Policy-Claim       15420 non-null object\n",
      "PastNumberOfClaims      15420 non-null object\n",
      "AgeOfVehicle            15420 non-null object\n",
      "AgeOfPolicyHolder       15420 non-null object\n",
      "PoliceReportFiled       15420 non-null object\n",
      "WitnessPresent          15420 non-null object\n",
      "AgentType               15420 non-null object\n",
      "NumberOfSuppliments     15420 non-null object\n",
      "AddressChange-Claim     15420 non-null object\n",
      "NumberOfCars            15420 non-null object\n",
      "Year                    15420 non-null int64\n",
      "BasePolicy              15420 non-null object\n",
      "dtypes: int64(8), object(25)\n",
      "memory usage: 3.9+ MB\n",
      "index:  1516\n",
      "\n",
      "index: 1516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "import preprocessingWithMissingvalues\n",
    "\n",
    "carDf= preprocessingWithMissingvalues.preprocess('../ann/cardata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make_Accura</th>\n",
       "      <th>Make_BMW</th>\n",
       "      <th>Make_Chevrolet</th>\n",
       "      <th>Make_Dodge</th>\n",
       "      <th>Make_Ferrari</th>\n",
       "      <th>Make_Ford</th>\n",
       "      <th>Make_Honda</th>\n",
       "      <th>Make_Jaguar</th>\n",
       "      <th>Make_Lexus</th>\n",
       "      <th>Make_Mazda</th>\n",
       "      <th>...</th>\n",
       "      <th>AddressChange-Claim_under 6 months</th>\n",
       "      <th>NumberOfCars_1 vehicle</th>\n",
       "      <th>NumberOfCars_2 vehicles</th>\n",
       "      <th>NumberOfCars_3 to 4</th>\n",
       "      <th>NumberOfCars_5 to 8</th>\n",
       "      <th>NumberOfCars_more than 8</th>\n",
       "      <th>RepNo</th>\n",
       "      <th>Deductible</th>\n",
       "      <th>DriverRating</th>\n",
       "      <th>DaysDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.069231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Make_Accura  Make_BMW  Make_Chevrolet  Make_Dodge  Make_Ferrari  Make_Ford  \\\n",
       "0            0         0               0           0             0          0   \n",
       "1            0         0               0           0             0          0   \n",
       "\n",
       "   Make_Honda  Make_Jaguar  Make_Lexus  Make_Mazda    ...     \\\n",
       "0           1            0           0           0    ...      \n",
       "1           1            0           0           0    ...      \n",
       "\n",
       "   AddressChange-Claim_under 6 months  NumberOfCars_1 vehicle  \\\n",
       "0                                   0                       0   \n",
       "1                                   0                       1   \n",
       "\n",
       "   NumberOfCars_2 vehicles  NumberOfCars_3 to 4  NumberOfCars_5 to 8  \\\n",
       "0                        0                    1                    0   \n",
       "1                        0                    0                    0   \n",
       "\n",
       "   NumberOfCars_more than 8     RepNo  Deductible  DriverRating  DaysDiff  \n",
       "0                         0  0.733333        0.00           0.0  0.071795  \n",
       "1                         0  0.933333        0.25           1.0  0.069231  \n",
       "\n",
       "[2 rows x 97 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the lable from the dataset\n",
    "carDf.head()\n",
    "carLable= carDf['Lable']\n",
    "carDf.drop(['Lable'],inplace=True,axis=1) #drop the lable;\n",
    "carDf[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/anaconda35/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "#apply the PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "carDf = StandardScaler().fit_transform(carDf)\n",
    "\n",
    "\n",
    "pca = PCA(.95)\n",
    "pca_result = pca.fit_transform(carDf)\n",
    "#pca_df = pd.DataFrame(columns = ['pca1','pca2','pca3','pca4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pca_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.385616</td>\n",
       "      <td>3.854620</td>\n",
       "      <td>-2.064169</td>\n",
       "      <td>2.316098</td>\n",
       "      <td>0.587232</td>\n",
       "      <td>-1.553042</td>\n",
       "      <td>-1.340943</td>\n",
       "      <td>3.049831</td>\n",
       "      <td>-1.770466</td>\n",
       "      <td>10.359703</td>\n",
       "      <td>...</td>\n",
       "      <td>22.472592</td>\n",
       "      <td>-23.907883</td>\n",
       "      <td>26.256987</td>\n",
       "      <td>60.084584</td>\n",
       "      <td>3.724137</td>\n",
       "      <td>15.336528</td>\n",
       "      <td>9.814129</td>\n",
       "      <td>1.489515</td>\n",
       "      <td>-3.001330</td>\n",
       "      <td>-5.697575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.847836</td>\n",
       "      <td>-2.088748</td>\n",
       "      <td>-0.386049</td>\n",
       "      <td>2.777993</td>\n",
       "      <td>3.492078</td>\n",
       "      <td>4.313151</td>\n",
       "      <td>-1.597910</td>\n",
       "      <td>0.637190</td>\n",
       "      <td>-1.133189</td>\n",
       "      <td>-1.285239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.260817</td>\n",
       "      <td>-0.659223</td>\n",
       "      <td>-0.221485</td>\n",
       "      <td>1.909207</td>\n",
       "      <td>1.759892</td>\n",
       "      <td>1.099109</td>\n",
       "      <td>0.143473</td>\n",
       "      <td>-1.505178</td>\n",
       "      <td>-0.724217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.451507</td>\n",
       "      <td>-0.428698</td>\n",
       "      <td>0.565207</td>\n",
       "      <td>3.293118</td>\n",
       "      <td>0.234022</td>\n",
       "      <td>-0.427259</td>\n",
       "      <td>-1.256547</td>\n",
       "      <td>0.691969</td>\n",
       "      <td>1.267305</td>\n",
       "      <td>0.333467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.585429</td>\n",
       "      <td>-0.146577</td>\n",
       "      <td>-0.465165</td>\n",
       "      <td>-0.251005</td>\n",
       "      <td>1.444187</td>\n",
       "      <td>1.557063</td>\n",
       "      <td>0.951375</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>-1.622217</td>\n",
       "      <td>-1.128883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.889307</td>\n",
       "      <td>0.077026</td>\n",
       "      <td>0.863538</td>\n",
       "      <td>-0.406910</td>\n",
       "      <td>4.688306</td>\n",
       "      <td>3.972506</td>\n",
       "      <td>0.874295</td>\n",
       "      <td>-2.097643</td>\n",
       "      <td>-2.236658</td>\n",
       "      <td>0.098089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323089</td>\n",
       "      <td>-0.379188</td>\n",
       "      <td>-0.440375</td>\n",
       "      <td>-0.059298</td>\n",
       "      <td>0.559230</td>\n",
       "      <td>0.849083</td>\n",
       "      <td>-0.295101</td>\n",
       "      <td>0.335771</td>\n",
       "      <td>-0.242074</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.852191</td>\n",
       "      <td>-2.335400</td>\n",
       "      <td>-0.636814</td>\n",
       "      <td>1.696282</td>\n",
       "      <td>2.128785</td>\n",
       "      <td>-1.504042</td>\n",
       "      <td>-0.879829</td>\n",
       "      <td>1.887913</td>\n",
       "      <td>1.541650</td>\n",
       "      <td>2.341173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.906605</td>\n",
       "      <td>-0.069863</td>\n",
       "      <td>-0.727519</td>\n",
       "      <td>-0.373084</td>\n",
       "      <td>1.469164</td>\n",
       "      <td>2.177938</td>\n",
       "      <td>0.343096</td>\n",
       "      <td>-0.191737</td>\n",
       "      <td>-1.295324</td>\n",
       "      <td>-0.945496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  8.385616  3.854620 -2.064169  2.316098  0.587232 -1.553042 -1.340943   \n",
       "1  2.847836 -2.088748 -0.386049  2.777993  3.492078  4.313151 -1.597910   \n",
       "2 -0.451507 -0.428698  0.565207  3.293118  0.234022 -0.427259 -1.256547   \n",
       "3 -1.889307  0.077026  0.863538 -0.406910  4.688306  3.972506  0.874295   \n",
       "4  3.852191 -2.335400 -0.636814  1.696282  2.128785 -1.504042 -0.879829   \n",
       "\n",
       "         7         8          9     ...            59         60         61  \\\n",
       "0  3.049831 -1.770466  10.359703    ...     22.472592 -23.907883  26.256987   \n",
       "1  0.637190 -1.133189  -1.285239    ...      0.061326   0.260817  -0.659223   \n",
       "2  0.691969  1.267305   0.333467    ...     -0.585429  -0.146577  -0.465165   \n",
       "3 -2.097643 -2.236658   0.098089    ...      0.323089  -0.379188  -0.440375   \n",
       "4  1.887913  1.541650   2.341173    ...     -0.906605  -0.069863  -0.727519   \n",
       "\n",
       "          62        63         64        65        66        67        68  \n",
       "0  60.084584  3.724137  15.336528  9.814129  1.489515 -3.001330 -5.697575  \n",
       "1  -0.221485  1.909207   1.759892  1.099109  0.143473 -1.505178 -0.724217  \n",
       "2  -0.251005  1.444187   1.557063  0.951375  0.010909 -1.622217 -1.128883  \n",
       "3  -0.059298  0.559230   0.849083 -0.295101  0.335771 -0.242074  1.055900  \n",
       "4  -0.373084  1.469164   2.177938  0.343096 -0.191737 -1.295324 -0.945496  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result= pd.DataFrame(pca_result)\n",
    "pca_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "carDateNormalized= pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into fraudulent and non-fraudulent data\n",
    "columnlable= list(carDateNormalized.columns.values)\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "nonFraudulent= pd.DataFrame(columns=columnlable)\n",
    "nonFraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "fraudulent= pd.DataFrame(columns=columnlable)\n",
    "fraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "\n",
    "j= 0\n",
    "k= 0\n",
    "for i in range(carDateNormalized.shape[0]):\n",
    "    if(carLable[i]==0):\n",
    "        nonFraudulent.loc[j]= carDateNormalized.loc[i]\n",
    "        nonFraudulentLable.loc[j]= 0.0\n",
    "        j += 1\n",
    "    else:\n",
    "        fraudulent.loc[k]= carDateNormalized.loc[i]\n",
    "        fraudulentLable.loc[i]= 1.0\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14496, 69)\n",
      "(923, 69)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.385616</td>\n",
       "      <td>3.854620</td>\n",
       "      <td>-2.064169</td>\n",
       "      <td>2.316098</td>\n",
       "      <td>0.587232</td>\n",
       "      <td>-1.553042</td>\n",
       "      <td>-1.340943</td>\n",
       "      <td>3.049831</td>\n",
       "      <td>-1.770466</td>\n",
       "      <td>10.359703</td>\n",
       "      <td>...</td>\n",
       "      <td>22.472592</td>\n",
       "      <td>-23.907883</td>\n",
       "      <td>26.256987</td>\n",
       "      <td>60.084584</td>\n",
       "      <td>3.724137</td>\n",
       "      <td>15.336528</td>\n",
       "      <td>9.814129</td>\n",
       "      <td>1.489515</td>\n",
       "      <td>-3.001330</td>\n",
       "      <td>-5.697575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.847836</td>\n",
       "      <td>-2.088748</td>\n",
       "      <td>-0.386049</td>\n",
       "      <td>2.777993</td>\n",
       "      <td>3.492078</td>\n",
       "      <td>4.313151</td>\n",
       "      <td>-1.597910</td>\n",
       "      <td>0.637190</td>\n",
       "      <td>-1.133189</td>\n",
       "      <td>-1.285239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.260817</td>\n",
       "      <td>-0.659223</td>\n",
       "      <td>-0.221485</td>\n",
       "      <td>1.909207</td>\n",
       "      <td>1.759892</td>\n",
       "      <td>1.099109</td>\n",
       "      <td>0.143473</td>\n",
       "      <td>-1.505178</td>\n",
       "      <td>-0.724217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.451507</td>\n",
       "      <td>-0.428698</td>\n",
       "      <td>0.565207</td>\n",
       "      <td>3.293118</td>\n",
       "      <td>0.234022</td>\n",
       "      <td>-0.427259</td>\n",
       "      <td>-1.256547</td>\n",
       "      <td>0.691969</td>\n",
       "      <td>1.267305</td>\n",
       "      <td>0.333467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.585429</td>\n",
       "      <td>-0.146577</td>\n",
       "      <td>-0.465165</td>\n",
       "      <td>-0.251005</td>\n",
       "      <td>1.444187</td>\n",
       "      <td>1.557063</td>\n",
       "      <td>0.951375</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>-1.622217</td>\n",
       "      <td>-1.128883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.889307</td>\n",
       "      <td>0.077026</td>\n",
       "      <td>0.863538</td>\n",
       "      <td>-0.406910</td>\n",
       "      <td>4.688306</td>\n",
       "      <td>3.972506</td>\n",
       "      <td>0.874295</td>\n",
       "      <td>-2.097643</td>\n",
       "      <td>-2.236658</td>\n",
       "      <td>0.098089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323089</td>\n",
       "      <td>-0.379188</td>\n",
       "      <td>-0.440375</td>\n",
       "      <td>-0.059298</td>\n",
       "      <td>0.559230</td>\n",
       "      <td>0.849083</td>\n",
       "      <td>-0.295101</td>\n",
       "      <td>0.335771</td>\n",
       "      <td>-0.242074</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.852191</td>\n",
       "      <td>-2.335400</td>\n",
       "      <td>-0.636814</td>\n",
       "      <td>1.696282</td>\n",
       "      <td>2.128785</td>\n",
       "      <td>-1.504042</td>\n",
       "      <td>-0.879829</td>\n",
       "      <td>1.887913</td>\n",
       "      <td>1.541650</td>\n",
       "      <td>2.341173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.906605</td>\n",
       "      <td>-0.069863</td>\n",
       "      <td>-0.727519</td>\n",
       "      <td>-0.373084</td>\n",
       "      <td>1.469164</td>\n",
       "      <td>2.177938</td>\n",
       "      <td>0.343096</td>\n",
       "      <td>-0.191737</td>\n",
       "      <td>-1.295324</td>\n",
       "      <td>-0.945496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  8.385616  3.854620 -2.064169  2.316098  0.587232 -1.553042 -1.340943   \n",
       "1  2.847836 -2.088748 -0.386049  2.777993  3.492078  4.313151 -1.597910   \n",
       "2 -0.451507 -0.428698  0.565207  3.293118  0.234022 -0.427259 -1.256547   \n",
       "3 -1.889307  0.077026  0.863538 -0.406910  4.688306  3.972506  0.874295   \n",
       "4  3.852191 -2.335400 -0.636814  1.696282  2.128785 -1.504042 -0.879829   \n",
       "\n",
       "         7         8          9     ...            59         60         61  \\\n",
       "0  3.049831 -1.770466  10.359703    ...     22.472592 -23.907883  26.256987   \n",
       "1  0.637190 -1.133189  -1.285239    ...      0.061326   0.260817  -0.659223   \n",
       "2  0.691969  1.267305   0.333467    ...     -0.585429  -0.146577  -0.465165   \n",
       "3 -2.097643 -2.236658   0.098089    ...      0.323089  -0.379188  -0.440375   \n",
       "4  1.887913  1.541650   2.341173    ...     -0.906605  -0.069863  -0.727519   \n",
       "\n",
       "          62        63         64        65        66        67        68  \n",
       "0  60.084584  3.724137  15.336528  9.814129  1.489515 -3.001330 -5.697575  \n",
       "1  -0.221485  1.909207   1.759892  1.099109  0.143473 -1.505178 -0.724217  \n",
       "2  -0.251005  1.444187   1.557063  0.951375  0.010909 -1.622217 -1.128883  \n",
       "3  -0.059298  0.559230   0.849083 -0.295101  0.335771 -0.242074  1.055900  \n",
       "4  -0.373084  1.469164   2.177938  0.343096 -0.191737 -1.295324 -0.945496  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nonFraudulent.shape)\n",
    "print(fraudulent.shape)\n",
    "nonFraudulent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "nonFraudulentLable= pd.DataFrame(nonFraudulentLable)\n",
    "fraudulentLable= pd.DataFrame(fraudulentLable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lable\n",
       "28    1.0\n",
       "52    1.0\n",
       "53    1.0\n",
       "94    1.0\n",
       "96    1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonFraudulent.head()\n",
    "fraudulentLable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(3624, 69)\n",
      "fraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(231, 69)\n"
     ]
    }
   ],
   "source": [
    "#divide the nonFraudulent into test and train\n",
    "nonFraudX_train,nonFraudX_test,nonFraudY_train,nonFraudY_test = train_test_split(nonFraudulent,nonFraudulentLable,random_state=3,test_size=0.25)\n",
    "print('nonFraudxtrain:',type(nonFraudX_train))\n",
    "print(nonFraudX_test.shape)\n",
    "\n",
    "#divide the fraudulent into test and train\n",
    "fraudX_train,fraudX_test,fraudY_train,fraudY_test = train_test_split(fraudulent, fraudulentLable,random_state=3,test_size=0.25)\n",
    "print('fraudxtrain:',type(fraudX_train))\n",
    "print(fraudX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into numpy\n",
    "nonFraudX_train= nonFraudX_train.values\n",
    "nonFraudX_test= nonFraudX_test.values\n",
    "nonFraudY_train= nonFraudY_train.values\n",
    "nonFraudY_test= nonFraudY_test.values\n",
    "\n",
    "fraudX_train= fraudX_train.values\n",
    "fraudX_test= fraudX_test.values\n",
    "fraudY_train= fraudY_train.values\n",
    "fraudY_test= fraudY_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class Dataset which returns the data and labels\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,features,lables,transform= None):\n",
    "        dataTensor= []\n",
    "        lableTensor= []\n",
    "        dataSize= features.shape[0]\n",
    "        \n",
    "        for data in range(dataSize):\n",
    "            feature= features[data,:]\n",
    "            #feature= torch.from_numpy(feature).float()\n",
    "            feature= torch.Tensor(feature)\n",
    "            dataTensor.append(feature)\n",
    "            \n",
    "            lable= np.asanyarray(lables[data])\n",
    "            lable= torch.from_numpy(lable).float()\n",
    "           \n",
    "            #lable= torch.Tensor(lable)\n",
    "            #print(\"lable:\",lable)\n",
    "            #assert(False)\n",
    "           \n",
    "            lableTensor.append(lable)\n",
    "        \n",
    "        #put everything in features and lables\n",
    "        self.features= dataTensor\n",
    "        self.lables= lableTensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        feature= self.features[index]\n",
    "        lable= self.lables[index]\n",
    "        #print(\"get_item feature:\",feature)\n",
    "        #print(\"get_item lable:\",lable)\n",
    "        return feature,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the  nonfraudulent dataset for train and test loader\n",
    "\n",
    "myNonFraudulentTrainDataset= myDataset(nonFraudX_train,nonFraudY_train)\n",
    "myNonFraudulentTestDataset= myDataset(nonFraudX_test,nonFraudY_test)\n",
    "\n",
    "#make the fraudulent dataset for train and test loader\n",
    "myFraudulentTrainDataset= myDataset(fraudX_train,fraudY_train)\n",
    "myFraudulentTestDataset= myDataset(fraudX_test,fraudY_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the trainloader and test loader for nonfraudulent dataset.\n",
    "nonFraudulentTrainLoader= torch.utils.data.DataLoader(myNonFraudulentTrainDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "nonFraudulentTestLoader= torch.utils.data.DataLoader(myNonFraudulentTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "fraudulentTrainLoader= torch.utils.data.DataLoader(myFraudulentTrainDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "fraudulentTestLoader= torch.utils.data.DataLoader(myFraudulentTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class autoencoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder1,self).__init__()\n",
    "        self.encoder1= nn.Sequential(\n",
    "            nn.Linear(69,59),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(59,49),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(49,39),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(39,29),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(29,19),nn.ReLU(True),nn.Linear(19,10))\n",
    "        \n",
    "        self.decoder1= nn.Sequential(\n",
    "            nn.Linear(10,19),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(19,29),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(29,39),nn.ReLU(True),nn.Linear(39,49),\n",
    "            nn.ReLU(True),nn.Linear(49,59),\n",
    "            nn.ReLU(True),nn.Linear(59,69),nn.Tanh())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.encoder1(x)\n",
    "        x= self.decoder1(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1= autoencoder1()\n",
    "criterion1= nn.MSELoss()\n",
    "optimizer1= torch.optim.SGD(model1.parameters(), lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.316\n",
      "[2] loss: 1.316\n",
      "[3] loss: 1.316\n",
      "[4] loss: 1.316\n",
      "[5] loss: 1.316\n",
      "[6] loss: 1.316\n",
      "[7] loss: 1.316\n",
      "[8] loss: 1.316\n",
      "[9] loss: 1.316\n",
      "[10] loss: 1.316\n",
      "[11] loss: 1.316\n",
      "[12] loss: 1.316\n",
      "[13] loss: 1.316\n",
      "[14] loss: 1.316\n",
      "[15] loss: 1.316\n",
      "[16] loss: 1.316\n",
      "[17] loss: 1.316\n",
      "[18] loss: 1.316\n",
      "[19] loss: 1.316\n",
      "[20] loss: 1.316\n",
      "[21] loss: 1.316\n",
      "[22] loss: 1.316\n",
      "[23] loss: 1.316\n",
      "[24] loss: 1.316\n",
      "[25] loss: 1.316\n",
      "[26] loss: 1.316\n",
      "[27] loss: 1.316\n",
      "[28] loss: 1.316\n",
      "[29] loss: 1.316\n",
      "[30] loss: 1.316\n",
      "[31] loss: 1.316\n",
      "[32] loss: 1.316\n",
      "[33] loss: 1.316\n",
      "[34] loss: 1.316\n",
      "[35] loss: 1.316\n",
      "[36] loss: 1.316\n",
      "[37] loss: 1.316\n",
      "[38] loss: 1.316\n",
      "[39] loss: 1.316\n",
      "[40] loss: 1.316\n",
      "[41] loss: 1.316\n",
      "[42] loss: 1.316\n",
      "[43] loss: 1.316\n",
      "[44] loss: 1.316\n",
      "[45] loss: 1.316\n",
      "[46] loss: 1.316\n",
      "[47] loss: 1.316\n",
      "[48] loss: 1.316\n",
      "[49] loss: 1.316\n",
      "[50] loss: 1.316\n",
      "[51] loss: 1.316\n",
      "[52] loss: 1.316\n",
      "[53] loss: 1.316\n",
      "[54] loss: 1.316\n",
      "[55] loss: 1.316\n",
      "[56] loss: 1.316\n",
      "[57] loss: 1.316\n",
      "[58] loss: 1.316\n",
      "[59] loss: 1.316\n",
      "[60] loss: 1.316\n",
      "[61] loss: 1.316\n",
      "[62] loss: 1.316\n",
      "[63] loss: 1.316\n",
      "[64] loss: 1.316\n",
      "[65] loss: 1.316\n",
      "[66] loss: 1.316\n",
      "[67] loss: 1.316\n",
      "[68] loss: 1.316\n",
      "[69] loss: 1.316\n",
      "[70] loss: 1.316\n",
      "[71] loss: 1.316\n",
      "[72] loss: 1.316\n",
      "[73] loss: 1.316\n",
      "[74] loss: 1.316\n",
      "[75] loss: 1.316\n",
      "[76] loss: 1.316\n",
      "[77] loss: 1.316\n",
      "[78] loss: 1.316\n",
      "[79] loss: 1.316\n",
      "[80] loss: 1.316\n",
      "[81] loss: 1.316\n",
      "[82] loss: 1.316\n",
      "[83] loss: 1.316\n",
      "[84] loss: 1.316\n",
      "[85] loss: 1.316\n",
      "[86] loss: 1.316\n",
      "[87] loss: 1.316\n",
      "[88] loss: 1.316\n",
      "[89] loss: 1.316\n",
      "[90] loss: 1.316\n",
      "[91] loss: 1.316\n",
      "[92] loss: 1.316\n",
      "[93] loss: 1.316\n",
      "[94] loss: 1.316\n",
      "[95] loss: 1.316\n",
      "[96] loss: 1.316\n",
      "[97] loss: 1.316\n",
      "[98] loss: 1.316\n",
      "[99] loss: 1.316\n",
      "[100] loss: 1.316\n",
      "[101] loss: 1.316\n",
      "[102] loss: 1.316\n",
      "[103] loss: 1.316\n",
      "[104] loss: 1.316\n",
      "[105] loss: 1.316\n",
      "[106] loss: 1.316\n",
      "[107] loss: 1.316\n",
      "[108] loss: 1.316\n",
      "[109] loss: 1.316\n",
      "[110] loss: 1.316\n",
      "[111] loss: 1.316\n",
      "[112] loss: 1.316\n",
      "[113] loss: 1.316\n",
      "[114] loss: 1.316\n",
      "[115] loss: 1.316\n",
      "[116] loss: 1.316\n",
      "[117] loss: 1.316\n",
      "[118] loss: 1.316\n",
      "[119] loss: 1.316\n",
      "[120] loss: 1.316\n",
      "[121] loss: 1.316\n",
      "[122] loss: 1.316\n",
      "[123] loss: 1.316\n",
      "[124] loss: 1.316\n",
      "[125] loss: 1.316\n",
      "[126] loss: 1.316\n",
      "[127] loss: 1.316\n",
      "[128] loss: 1.316\n",
      "[129] loss: 1.316\n",
      "[130] loss: 1.316\n",
      "[131] loss: 1.316\n",
      "[132] loss: 1.316\n",
      "[133] loss: 1.316\n",
      "[134] loss: 1.316\n",
      "[135] loss: 1.316\n",
      "[136] loss: 1.316\n",
      "[137] loss: 1.316\n",
      "[138] loss: 1.316\n",
      "[139] loss: 1.316\n",
      "[140] loss: 1.316\n",
      "[141] loss: 1.316\n",
      "[142] loss: 1.316\n",
      "[143] loss: 1.316\n",
      "[144] loss: 1.316\n",
      "[145] loss: 1.316\n",
      "[146] loss: 1.316\n",
      "[147] loss: 1.316\n",
      "[148] loss: 1.316\n",
      "[149] loss: 1.316\n",
      "[150] loss: 1.316\n",
      "[151] loss: 1.316\n",
      "[152] loss: 1.316\n",
      "[153] loss: 1.316\n",
      "[154] loss: 1.316\n",
      "[155] loss: 1.316\n",
      "[156] loss: 1.316\n",
      "[157] loss: 1.316\n",
      "[158] loss: 1.316\n",
      "[159] loss: 1.316\n",
      "[160] loss: 1.316\n",
      "[161] loss: 1.316\n",
      "[162] loss: 1.316\n",
      "[163] loss: 1.316\n",
      "[164] loss: 1.316\n",
      "[165] loss: 1.316\n",
      "[166] loss: 1.316\n",
      "[167] loss: 1.316\n",
      "[168] loss: 1.316\n",
      "[169] loss: 1.316\n",
      "[170] loss: 1.315\n",
      "[171] loss: 1.315\n",
      "[172] loss: 1.315\n",
      "[173] loss: 1.315\n",
      "[174] loss: 1.308\n",
      "[175] loss: 1.284\n",
      "[176] loss: 1.272\n",
      "[177] loss: 1.262\n",
      "[178] loss: 1.252\n",
      "[179] loss: 1.244\n",
      "[180] loss: 1.236\n",
      "[181] loss: 1.229\n",
      "[182] loss: 1.224\n",
      "[183] loss: 1.221\n",
      "[184] loss: 1.216\n",
      "[185] loss: 1.202\n",
      "[186] loss: 1.195\n",
      "[187] loss: 1.180\n",
      "[188] loss: 1.173\n",
      "[189] loss: 1.162\n",
      "[190] loss: 1.159\n",
      "[191] loss: 1.147\n",
      "[192] loss: 1.146\n",
      "[193] loss: 1.138\n",
      "[194] loss: 1.130\n",
      "[195] loss: 1.126\n",
      "[196] loss: 1.120\n",
      "[197] loss: 1.156\n",
      "[198] loss: 1.137\n",
      "[199] loss: 1.126\n",
      "[200] loss: 1.115\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for epoch in range(200):\n",
    "    running_loss= 0.0\n",
    "    for i, (feature1,lable1) in enumerate(nonFraudulentTrainLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs1= torch.tensor(feature1)\n",
    "        lables1= torch.tensor(lable1)\n",
    "        lables1= lables1.type(torch.LongTensor)\n",
    "        \n",
    "        # =====================forward====================\n",
    "        output1 = model1(inputs1)\n",
    "        loss1 = criterion1(output1,inputs1)\n",
    "        \n",
    "         # ===================backward====================\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        # =======print the statistics\n",
    "        running_loss += loss1.item()\n",
    "        \n",
    "        #print(\"i: \",i)\n",
    "    #if i%100 == 0:              #print every 2000 mini-batches\n",
    "    \n",
    "    print('[%d] loss: %.3f' %\n",
    "              (epoch + 1,  running_loss /nonFraudX_train.shape[0]))\n",
    "      #running_loss = 0.0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " #test the model\n",
    "def modelTest1(Loader):\n",
    "    lossList1= []\n",
    "    trueLable1= []\n",
    "    for i,(feature1,lable1) in enumerate(Loader):\n",
    "        inputs1= torch.tensor(feature1)\n",
    "        output1= model1(inputs1)\n",
    "        loss1= criterion1(output1,inputs1)\n",
    "        trueLable1.append(lable1)\n",
    "        lossList1.append(loss1.item())\n",
    "        lossValue1= loss1.item()\n",
    "        f= open(\"./modle1Loss.txt\",'a')\n",
    "        f.write(str(lossValue1) + '\\n')\n",
    "        '''\n",
    "        if(i!=10):\n",
    "            print(\"input:\",inputs1)\n",
    "            print(\"output:\",output1)\n",
    "            print(\"loss1:\",loss1)\n",
    "            print(\"lable:\",lable1)\n",
    "        else:\n",
    "            assert(False)\n",
    "        '''\n",
    "    f.close()\n",
    "    return lossList1,trueLable1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class autoencoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder2,self).__init__()\n",
    "        self.encoder2= nn.Sequential(\n",
    "            nn.Linear(69,45),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(45,25),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(25,10))\n",
    "       \n",
    "        self.decoder2= nn.Sequential(\n",
    "            nn.Linear(10,25),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(25,45),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(45,69),nn.Tanh())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.encoder2(x)\n",
    "        x= self.decoder2(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= autoencoder2()\n",
    "criterion2= nn.MSELoss()\n",
    "optimizer2= torch.optim.SGD(model2.parameters(), lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.472\n",
      "[2] loss: 0.464\n",
      "[3] loss: 0.457\n",
      "[4] loss: 0.451\n",
      "[5] loss: 0.446\n",
      "[6] loss: 0.442\n",
      "[7] loss: 0.438\n",
      "[8] loss: 0.433\n",
      "[9] loss: 0.428\n",
      "[10] loss: 0.422\n",
      "[11] loss: 0.416\n",
      "[12] loss: 0.410\n",
      "[13] loss: 0.405\n",
      "[14] loss: 0.400\n",
      "[15] loss: 0.396\n",
      "[16] loss: 0.391\n",
      "[17] loss: 0.388\n",
      "[18] loss: 0.385\n",
      "[19] loss: 0.382\n",
      "[20] loss: 0.380\n",
      "[21] loss: 0.377\n",
      "[22] loss: 0.375\n",
      "[23] loss: 0.373\n",
      "[24] loss: 0.371\n",
      "[25] loss: 0.370\n",
      "[26] loss: 0.368\n",
      "[27] loss: 0.366\n",
      "[28] loss: 0.365\n",
      "[29] loss: 0.363\n",
      "[30] loss: 0.362\n",
      "[31] loss: 0.361\n",
      "[32] loss: 0.360\n",
      "[33] loss: 0.358\n",
      "[34] loss: 0.357\n",
      "[35] loss: 0.356\n",
      "[36] loss: 0.355\n",
      "[37] loss: 0.354\n",
      "[38] loss: 0.352\n",
      "[39] loss: 0.351\n",
      "[40] loss: 0.350\n",
      "[41] loss: 0.349\n",
      "[42] loss: 0.348\n",
      "[43] loss: 0.347\n",
      "[44] loss: 0.346\n",
      "[45] loss: 0.345\n",
      "[46] loss: 0.344\n",
      "[47] loss: 0.344\n",
      "[48] loss: 0.343\n",
      "[49] loss: 0.342\n",
      "[50] loss: 0.341\n",
      "[51] loss: 0.340\n",
      "[52] loss: 0.339\n",
      "[53] loss: 0.339\n",
      "[54] loss: 0.338\n",
      "[55] loss: 0.337\n",
      "[56] loss: 0.336\n",
      "[57] loss: 0.335\n",
      "[58] loss: 0.335\n",
      "[59] loss: 0.334\n",
      "[60] loss: 0.334\n",
      "[61] loss: 0.332\n",
      "[62] loss: 0.332\n",
      "[63] loss: 0.331\n",
      "[64] loss: 0.331\n",
      "[65] loss: 0.330\n",
      "[66] loss: 0.330\n",
      "[67] loss: 0.329\n",
      "[68] loss: 0.329\n",
      "[69] loss: 0.329\n",
      "[70] loss: 0.329\n",
      "[71] loss: 0.327\n",
      "[72] loss: 0.327\n",
      "[73] loss: 0.327\n",
      "[74] loss: 0.327\n",
      "[75] loss: 0.328\n"
     ]
    }
   ],
   "source": [
    "#train the model2\n",
    "for epoch in range(75):\n",
    "    running_loss2= 0.0\n",
    "    for i, (feature2,lable2) in enumerate(fraudulentTestLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs2= torch.tensor(feature2)\n",
    "        lables2= torch.tensor(lable2)\n",
    "        lables2= lables2.type(torch.LongTensor)\n",
    "       \n",
    "        # =====================forward====================\n",
    "        output2= model2(inputs2)\n",
    "        loss2= criterion2(output2,inputs2)\n",
    "        \n",
    "        # ===================backward====================\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        # =======print the statistics\n",
    "        running_loss2 += loss2.item()\n",
    "        \n",
    "        #print(\"i: \",i)\n",
    "    #if i%100 == 0:              #print every 2000 mini-batches\n",
    "    \n",
    "    print('[%d] loss: %.3f' %\n",
    "              (epoch + 1,  running_loss2 / fraudX_train.shape[0]))\n",
    "      #running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest2(Loader2):\n",
    "    lossList2= []\n",
    "    trueLable1= []\n",
    "    for i,(feature2,lable2) in enumerate(Loader2):\n",
    "        inputs2= torch.tensor(feature2)\n",
    "        output2= model2(inputs2)\n",
    "        loss2= criterion2(output2,inputs2)\n",
    "        trueLable1.append(lable2)\n",
    "        lossList2.append(loss2.item()) #put the loss\n",
    "        lossValue2= str(loss2.item())\n",
    "        f= open(\"./modle2Loss.txt\",'a')\n",
    "        f.write(lossValue2+'\\n')\n",
    "    f.close()\n",
    "        \n",
    "    return lossList2,trueLable1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model1\n",
    "lossList1,trueLable1= modelTest1(nonFraudulentTestLoader)\n",
    "\n",
    "#test the model2\n",
    "lossList2,trueLable1= modelTest2(nonFraudulentTestLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLable1=[]\n",
    "for k in range(len(lossList1)):\n",
    "    if(lossList1[k]<lossList2[k]):\n",
    "        predictedLable1.append(0)\n",
    "    else:\n",
    "        predictedLable1.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model1 with another type of data\n",
    "lossList3,trueLable2= modelTest1(fraudulentTestLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model2 with another type of data\n",
    "lossList4,trueLable2= modelTest2(fraudulentTestLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLable2=[]\n",
    "for k in range(len(lossList3)):\n",
    "    if(lossList3[k]<lossList4[k]):\n",
    "        predictedLable2.append(0)\n",
    "    else:\n",
    "        predictedLable2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result \n",
      "Accuracy: 52.2697795071336\n",
      "Sensitivity: 60.17316017316018\n",
      "Specificity: 51.76600441501103\n",
      "\n",
      "\n",
      "\n",
      "TP: 139\n",
      "\n",
      "FN: 92\n",
      "\n",
      "FP: 1748\n",
      "\n",
      "TN: 1876\n"
     ]
    }
   ],
   "source": [
    "totalLength1= len(trueLable1)\n",
    "TP=FP=FN=TN= 0\n",
    "for i in range(totalLength1):\n",
    "    if(int(trueLable1[i])==1 and predictedLable1[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable1[i])==1 and predictedLable1[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable1[i])==0 and predictedLable1[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable1[i])==0 and predictedLable1[i]==1):\n",
    "        FP += 1\n",
    "\n",
    "totalLength2= len(trueLable2)\n",
    "for i in range(totalLength2):\n",
    "    if(int(trueLable2[i])==1 and predictedLable2[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable2[i])==1 and predictedLable2[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable2[i])==0 and predictedLable2[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable2[i])==0 and predictedLable2[i]==1):\n",
    "        FP += 1\n",
    "        \n",
    "print(\"\\nResult \")\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)\n",
    "print(\"\\n\")\n",
    "print(\"\\nTP:\",TP)\n",
    "print(\"\\nFN:\",FN)\n",
    "print(\"\\nFP:\",FP)\n",
    "print(\"\\nTN:\",TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel1(loader):\n",
    "    lossList= []\n",
    "    predLable= []\n",
    "    for i,(feature,lable) in enumerate(loader):\n",
    "        inputs= torch.tensor(feature)\n",
    "        output= model1(inputs)\n",
    "        loss= criterion2(output,inputs)\n",
    "        if(i!=10):\n",
    "            print(\"loss:\",loss.item())\n",
    "        else:\n",
    "            i= i\n",
    "            \n",
    "        if(loss > 0.9):\n",
    "            predLable.append(1)\n",
    "        else:\n",
    "            predLable.append(0)\n",
    "    return predLable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.988356113433838\n",
      "loss: 0.38591378927230835\n",
      "loss: 1.100298285484314\n",
      "loss: 1.6101622581481934\n",
      "loss: 0.6091102957725525\n",
      "loss: 0.6388607621192932\n",
      "loss: 0.9478121995925903\n",
      "loss: 0.5564133524894714\n",
      "loss: 0.4326455295085907\n",
      "loss: 0.3188246488571167\n",
      "loss: 0.39330604672431946\n",
      "loss: 0.5118290185928345\n",
      "loss: 0.8938324451446533\n",
      "loss: 1.1824462413787842\n",
      "loss: 0.3533158302307129\n",
      "loss: 0.3919377624988556\n",
      "loss: 0.5036945343017578\n",
      "loss: 0.36006638407707214\n",
      "loss: 0.9403480291366577\n",
      "loss: 0.7970864772796631\n",
      "loss: 0.4299071133136749\n",
      "loss: 0.4689094126224518\n",
      "loss: 0.635746955871582\n",
      "loss: 1.1725484132766724\n",
      "loss: 0.32587313652038574\n",
      "loss: 0.5336700677871704\n",
      "loss: 2.6759581565856934\n",
      "loss: 0.40714001655578613\n",
      "loss: 0.8285308480262756\n",
      "loss: 0.3975133001804352\n",
      "loss: 1.1992175579071045\n",
      "loss: 1.3588886260986328\n",
      "loss: 0.8351196646690369\n",
      "loss: 0.3204759359359741\n",
      "loss: 0.2898620367050171\n",
      "loss: 1.4170210361480713\n",
      "loss: 0.3854910135269165\n",
      "loss: 1.1189616918563843\n",
      "loss: 1.046439290046692\n",
      "loss: 0.6354686617851257\n",
      "loss: 0.26096343994140625\n",
      "loss: 0.5573933124542236\n",
      "loss: 0.31106460094451904\n",
      "loss: 0.3649263381958008\n",
      "loss: 0.3216322958469391\n",
      "loss: 0.5169326066970825\n",
      "loss: 13.333964347839355\n",
      "loss: 0.32225707173347473\n",
      "loss: 0.8238619565963745\n",
      "loss: 0.3770948350429535\n",
      "loss: 2.0621724128723145\n",
      "loss: 0.3362099826335907\n",
      "loss: 0.4131329655647278\n",
      "loss: 1.5585781335830688\n",
      "loss: 1.1745449304580688\n",
      "loss: 2.686058759689331\n",
      "loss: 1.1357026100158691\n",
      "loss: 0.4292213022708893\n",
      "loss: 1.3530604839324951\n",
      "loss: 7.449608325958252\n",
      "loss: 0.5091838240623474\n",
      "loss: 0.4331611692905426\n",
      "loss: 3.3847272396087646\n",
      "loss: 0.34195446968078613\n",
      "loss: 1.1929949522018433\n",
      "loss: 0.3072066903114319\n",
      "loss: 0.44075295329093933\n",
      "loss: 2.276484966278076\n",
      "loss: 1.430372953414917\n",
      "loss: 2.2642204761505127\n",
      "loss: 0.7573210597038269\n",
      "loss: 0.3984382450580597\n",
      "loss: 0.3727746605873108\n",
      "loss: 0.9739883542060852\n",
      "loss: 0.48623213171958923\n",
      "loss: 0.32589635252952576\n",
      "loss: 0.3452993631362915\n",
      "loss: 0.7928112149238586\n",
      "loss: 0.3219650685787201\n",
      "loss: 0.3768128752708435\n",
      "loss: 0.33447155356407166\n",
      "loss: 0.3098093271255493\n",
      "loss: 1.0356192588806152\n",
      "loss: 0.3253289461135864\n",
      "loss: 0.4878942668437958\n",
      "loss: 0.3610718548297882\n",
      "loss: 13.40478515625\n",
      "loss: 0.3835841417312622\n",
      "loss: 0.4214310348033905\n",
      "loss: 0.5768438577651978\n",
      "loss: 0.581534206867218\n",
      "loss: 1.1995681524276733\n",
      "loss: 0.3706119954586029\n",
      "loss: 0.5329740047454834\n",
      "loss: 0.5292572379112244\n",
      "loss: 0.3555426001548767\n",
      "loss: 1.406652569770813\n",
      "loss: 0.39421746134757996\n",
      "loss: 12.028281211853027\n",
      "loss: 0.3401796221733093\n",
      "loss: 0.34906795620918274\n",
      "loss: 0.49978598952293396\n",
      "loss: 0.8702894449234009\n",
      "loss: 0.3922345042228699\n",
      "loss: 0.39546188712120056\n",
      "loss: 1.3227814435958862\n",
      "loss: 0.3125665485858917\n",
      "loss: 0.9587917327880859\n",
      "loss: 1.952575445175171\n",
      "loss: 0.39937710762023926\n",
      "loss: 0.5319156050682068\n",
      "loss: 0.6964805722236633\n",
      "loss: 0.8357325196266174\n",
      "loss: 0.8149619698524475\n",
      "loss: 16.21405792236328\n",
      "loss: 4.689952373504639\n",
      "loss: 0.34737664461135864\n",
      "loss: 0.7632920145988464\n",
      "loss: 0.3057308793067932\n",
      "loss: 1.6242702007293701\n",
      "loss: 0.6475581526756287\n",
      "loss: 0.6555298566818237\n",
      "loss: 0.8776376247406006\n",
      "loss: 2.353456974029541\n",
      "loss: 0.949218213558197\n",
      "loss: 0.3011062443256378\n",
      "loss: 0.33754462003707886\n",
      "loss: 0.25696292519569397\n",
      "loss: 0.48086974024772644\n",
      "loss: 1.2564135789871216\n",
      "loss: 0.4485473334789276\n",
      "loss: 0.28371351957321167\n",
      "loss: 0.3396744430065155\n",
      "loss: 0.34035828709602356\n",
      "loss: 1.1904619932174683\n",
      "loss: 0.5150638818740845\n",
      "loss: 0.37490156292915344\n",
      "loss: 0.28260213136672974\n",
      "loss: 0.4856777787208557\n",
      "loss: 0.7494600415229797\n",
      "loss: 2.0594089031219482\n",
      "loss: 1.7887299060821533\n",
      "loss: 0.3497404456138611\n",
      "loss: 0.5422691702842712\n",
      "loss: 0.3491329550743103\n",
      "loss: 0.29513832926750183\n",
      "loss: 0.44073817133903503\n",
      "loss: 0.35419392585754395\n",
      "loss: 1.6145825386047363\n",
      "loss: 0.5977358818054199\n",
      "loss: 0.3886273503303528\n",
      "loss: 0.9571858644485474\n",
      "loss: 4.824301242828369\n",
      "loss: 0.3421492278575897\n",
      "loss: 0.44143325090408325\n",
      "loss: 0.9599289298057556\n",
      "loss: 0.5615164041519165\n",
      "loss: 1.4748585224151611\n",
      "loss: 0.8167210221290588\n",
      "loss: 0.3632794916629791\n",
      "loss: 0.3992536962032318\n",
      "loss: 0.3414754867553711\n",
      "loss: 0.5559988617897034\n",
      "loss: 0.4347979724407196\n",
      "loss: 1.437383770942688\n",
      "loss: 0.6826803088188171\n",
      "loss: 1.1521674394607544\n",
      "loss: 0.4191930592060089\n",
      "loss: 2.151400089263916\n",
      "loss: 0.43445655703544617\n",
      "loss: 0.7892654538154602\n",
      "loss: 0.4772588312625885\n",
      "loss: 0.5363429188728333\n",
      "loss: 0.5836930871009827\n",
      "loss: 0.6803016066551208\n",
      "loss: 0.7635443210601807\n",
      "loss: 0.5655136108398438\n",
      "loss: 0.6072851419448853\n",
      "loss: 0.9694439768791199\n",
      "loss: 0.38771766424179077\n",
      "loss: 1.1546335220336914\n",
      "loss: 0.41726765036582947\n",
      "loss: 0.5001417994499207\n",
      "loss: 1.0564744472503662\n",
      "loss: 1.4394971132278442\n",
      "loss: 0.39378345012664795\n",
      "loss: 1.3789108991622925\n",
      "loss: 0.39119386672973633\n",
      "loss: 1.5713821649551392\n",
      "loss: 0.8708549737930298\n",
      "loss: 0.428862065076828\n",
      "loss: 0.29697683453559875\n",
      "loss: 0.2995317578315735\n",
      "loss: 0.3068481683731079\n",
      "loss: 0.9012888073921204\n",
      "loss: 8.351826667785645\n",
      "loss: 0.42386049032211304\n",
      "loss: 0.31741130352020264\n",
      "loss: 0.33913347125053406\n",
      "loss: 0.3100547194480896\n",
      "loss: 0.47483983635902405\n",
      "loss: 0.4027506411075592\n",
      "loss: 2.246066093444824\n",
      "loss: 0.24886132776737213\n",
      "loss: 0.3248397409915924\n",
      "loss: 12.163408279418945\n",
      "loss: 2.304797649383545\n",
      "loss: 0.3991837203502655\n",
      "loss: 0.7646328210830688\n",
      "loss: 14.684216499328613\n",
      "loss: 0.5378274917602539\n",
      "loss: 0.27868950366973877\n",
      "loss: 0.42746636271476746\n",
      "loss: 1.760003685951233\n",
      "loss: 0.33367830514907837\n",
      "loss: 0.3185857832431793\n",
      "loss: 0.815081000328064\n",
      "loss: 0.2771894037723541\n",
      "loss: 0.31946995854377747\n",
      "loss: 1.5170212984085083\n",
      "loss: 0.451411634683609\n",
      "loss: 0.4582180082798004\n",
      "loss: 0.6153016686439514\n",
      "loss: 0.5435590147972107\n",
      "loss: 2.147700786590576\n",
      "loss: 1.7372444868087769\n",
      "loss: 1.7916895151138306\n",
      "loss: 3.272951126098633\n",
      "loss: 0.3108814060688019\n",
      "loss: 0.46466416120529175\n"
     ]
    }
   ],
   "source": [
    "predLable= testModel1(fraudulentTestLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class1= predLable.count(1)\n",
    "class0= predLable.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043\n",
      "2581\n"
     ]
    }
   ],
   "source": [
    "print(class1)\n",
    "print(class0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
