{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28992, 52)\n",
      "(28992, 1)\n"
     ]
    }
   ],
   "source": [
    "#MwMote\n",
    "combine_car_features = pd.read_csv('finalPreprocessBinaryTogether.csv')\n",
    "combine_car_label = pd.read_csv('label.csv')\n",
    "print(combine_car_features.shape)\n",
    "print(combine_car_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add columns to existing dataframe\n",
    "combine_car_features['FraudFound']=combine_car_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle the data.\n",
    "new_combine_features = combine_car_features.set_index(np.random.permutation(combine_car_features.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now divide the feature and label\n",
    "new_label = new_combine_features['FraudFound']\n",
    "\n",
    "#drop the Fraud Found lable\n",
    "#drop the following attributes\n",
    "new_combine_features.drop(['FraudFound'],inplace=True,axis=1)\n",
    "# new_label.shape\n",
    "# print(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967717381737\n",
      "0.745882479991\n",
      "0.997811358373\n",
      "0.829940233256\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=10)\n",
    "model= DecisionTreeClassifier(random_state= 42)\n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,X=new_combine_features,y=new_label,cv=kfold,scoring=scoring)\n",
    "\n",
    "print(np.mean(results['test_accuracy']))\n",
    "print(np.mean(results['test_precision']))\n",
    "print(np.mean(results['test_recall']))\n",
    "print(np.mean(results['test_f1_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation and plot ROC curves\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "classifier = DecisionTreeClassifier(random_state=42) \n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "i = 0\n",
    "\n",
    "new_combine_features= pd.DataFrame(new_combine_features).values\n",
    "new_label= pd.Series(new_label).values\n",
    "\n",
    "#plot of ROC curve for cross validation\n",
    "accuracy= []\n",
    "precision= []\n",
    "sensitivity= []\n",
    "specificity= []\n",
    "f1score= []\n",
    "\n",
    "for train, test in cv.split(new_combine_features, new_label):\n",
    "    probas_ = classifier.fit(new_combine_features[train], new_label[train]).predict_proba(new_combine_features[test])\n",
    "    j= 0\n",
    "    for j in range(probas_.shape[0]):\n",
    "        if (probas_[j,1]>0.5):\n",
    "            probas_[j,1]= 1\n",
    "        else:\n",
    "            probas_[j,1]= 0\n",
    "\n",
    "      \n",
    "    cm = confusion_matrix(car_label[test],probas_[:,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    acc= (TP+TN)/(TP+FP+FN+TN)*100\n",
    "    prec= TP/(TP+FP)*100\n",
    "    sens= TP/(TP+FN)*100\n",
    "    spec= TN/(TN+FP)*100\n",
    "    f1= (2 * prec * sens)/(prec + sens)\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    precision.append(prec)\n",
    "    sensitivity.append(sens)\n",
    "    specificity.append(spec)\n",
    "    f1score.append(f1)\n",
    "    \n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(new_label[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "          alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DT tress with out class imblance.\n",
    "#random forest without smote with cross validation\n",
    "car_features = pd.read_csv('finalDataPreprocessBinary.csv')\n",
    "#change the label of the data\n",
    "labelNo = LabelEncoder()\n",
    "car_df['FraudFound'] = labelNo.fit_transform(car_df['FraudFound'].astype('str'))\n",
    "car_label = car_df['FraudFound']\n",
    "print(car_label.shape)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "classifier = RandomForestClassifier(n_estimators=100) \n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "i = 0\n",
    "accuracy= []\n",
    "precision= []\n",
    "sensitivity= []\n",
    "specificity= []\n",
    "f1score= []\n",
    "\n",
    "car_features= pd.DataFrame(car_features).values\n",
    "car_label= pd.Series(car_label).values\n",
    "\n",
    "\n",
    "\n",
    "for train, test in cv.split(car_features, car_label):\n",
    "    probas_ = classifier.fit(car_features[train], car_label[train]).predict_proba(car_features[test])\n",
    "    j= 0\n",
    "    for j in range(probas_.shape[0]):\n",
    "        if (probas_[j,1]>0.5):\n",
    "            probas_[j,1]= 1\n",
    "        else:\n",
    "            probas_[j,1]= 0\n",
    "\n",
    "      \n",
    "    cm = confusion_matrix(car_label[test],probas_[:,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    acc= (TP+TN)/(TP+FP+FN+TN)*100\n",
    "    prec= TP/(TP+FP)*100\n",
    "    sens= TP/(TP+FN)*100\n",
    "    spec= TN/(TN+FP)*100\n",
    "    f1= (2 * prec * sens)/(prec + sens)\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    precision.append(prec)\n",
    "    sensitivity.append(sens)\n",
    "    specificity.append(spec)\n",
    "    f1score.append(f1)\n",
    "    \n",
    "print(\"Accuracy:\",np.mean(accuracy))\n",
    "print(\"Precision:\",np.mean(precision))\n",
    "print(\"Sensitivity:\",np.mean(sensitivity))\n",
    "print(\"Specificity:\",np.mean(specificity))\n",
    "print(\"F1_score:\",np.mean(f1score))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
