{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15420 entries, 0 to 15419\n",
      "Data columns (total 33 columns):\n",
      "Month                   15420 non-null object\n",
      "WeekOfMonth             15420 non-null int64\n",
      "DayOfWeek               15420 non-null object\n",
      "Make                    15420 non-null object\n",
      "AccidentArea            15420 non-null object\n",
      "DayOfWeekClaimed        15420 non-null object\n",
      "MonthClaimed            15420 non-null object\n",
      "WeekOfMonthClaimed      15420 non-null int64\n",
      "Sex                     15420 non-null object\n",
      "MaritalStatus           15420 non-null object\n",
      "Age                     15420 non-null int64\n",
      "Fault                   15420 non-null object\n",
      "PolicyType              15420 non-null object\n",
      "VehicleCategory         15420 non-null object\n",
      "VehiclePrice            15420 non-null object\n",
      "FraudFound              15420 non-null object\n",
      "PolicyNumber            15420 non-null int64\n",
      "RepNumber               15420 non-null int64\n",
      "Deductible              15420 non-null int64\n",
      "DriverRating            15420 non-null int64\n",
      "Days:Policy-Accident    15420 non-null object\n",
      "Days:Policy-Claim       15420 non-null object\n",
      "PastNumberOfClaims      15420 non-null object\n",
      "AgeOfVehicle            15420 non-null object\n",
      "AgeOfPolicyHolder       15420 non-null object\n",
      "PoliceReportFiled       15420 non-null object\n",
      "WitnessPresent          15420 non-null object\n",
      "AgentType               15420 non-null object\n",
      "NumberOfSuppliments     15420 non-null object\n",
      "AddressChange-Claim     15420 non-null object\n",
      "NumberOfCars            15420 non-null object\n",
      "Year                    15420 non-null int64\n",
      "BasePolicy              15420 non-null object\n",
      "dtypes: int64(8), object(25)\n",
      "memory usage: 3.9+ MB\n",
      "index:  1516\n",
      "\n",
      "index: 1516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "import preprocessingWithMissingvalues\n",
    "\n",
    "carDf= preprocessingWithMissingvalues.preprocess('../ann/cardata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the lable from the dataset\n",
    "carDf.head()\n",
    "carLable= carDf['Lable']\n",
    "carDf.drop(['Lable'],inplace=True,axis=1) #drop the lable;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.95)\n",
    "pca_result = pca.fit_transform(carDf)\n",
    "#pca_df = pd.DataFrame(columns = ['pca1','pca2','pca3','pca4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pca_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922849</td>\n",
       "      <td>0.132363</td>\n",
       "      <td>-1.117156</td>\n",
       "      <td>0.898516</td>\n",
       "      <td>-0.159263</td>\n",
       "      <td>0.163660</td>\n",
       "      <td>-0.419927</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.082305</td>\n",
       "      <td>0.968195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132322</td>\n",
       "      <td>-0.257047</td>\n",
       "      <td>0.438373</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>-0.099411</td>\n",
       "      <td>0.067951</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>-0.126064</td>\n",
       "      <td>0.646724</td>\n",
       "      <td>-0.262620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.295603</td>\n",
       "      <td>0.231041</td>\n",
       "      <td>-1.291208</td>\n",
       "      <td>0.607421</td>\n",
       "      <td>-0.041502</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>-0.049636</td>\n",
       "      <td>-0.088628</td>\n",
       "      <td>-0.328559</td>\n",
       "      <td>-0.167960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201041</td>\n",
       "      <td>-0.431792</td>\n",
       "      <td>0.022621</td>\n",
       "      <td>-0.226266</td>\n",
       "      <td>1.339788</td>\n",
       "      <td>0.069940</td>\n",
       "      <td>-0.220712</td>\n",
       "      <td>-0.435173</td>\n",
       "      <td>0.553638</td>\n",
       "      <td>-0.376443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.622890</td>\n",
       "      <td>-0.115813</td>\n",
       "      <td>-0.577577</td>\n",
       "      <td>0.881442</td>\n",
       "      <td>-0.310303</td>\n",
       "      <td>0.305865</td>\n",
       "      <td>0.197651</td>\n",
       "      <td>0.111288</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>-0.580193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.149915</td>\n",
       "      <td>0.286175</td>\n",
       "      <td>-0.111701</td>\n",
       "      <td>-0.049428</td>\n",
       "      <td>0.111080</td>\n",
       "      <td>-0.229686</td>\n",
       "      <td>-0.317591</td>\n",
       "      <td>0.510877</td>\n",
       "      <td>-0.409610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.729731</td>\n",
       "      <td>0.539017</td>\n",
       "      <td>0.196202</td>\n",
       "      <td>-0.890831</td>\n",
       "      <td>0.530871</td>\n",
       "      <td>-0.791703</td>\n",
       "      <td>-0.204858</td>\n",
       "      <td>-0.015203</td>\n",
       "      <td>1.105974</td>\n",
       "      <td>-0.568063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439378</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>0.325784</td>\n",
       "      <td>0.106025</td>\n",
       "      <td>1.364357</td>\n",
       "      <td>-0.044349</td>\n",
       "      <td>-0.036467</td>\n",
       "      <td>-0.235350</td>\n",
       "      <td>-0.043242</td>\n",
       "      <td>0.085156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.360981</td>\n",
       "      <td>1.062972</td>\n",
       "      <td>-0.722035</td>\n",
       "      <td>0.906465</td>\n",
       "      <td>0.621309</td>\n",
       "      <td>-0.119266</td>\n",
       "      <td>0.176604</td>\n",
       "      <td>0.514143</td>\n",
       "      <td>0.243289</td>\n",
       "      <td>1.018288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509380</td>\n",
       "      <td>0.347518</td>\n",
       "      <td>-0.164710</td>\n",
       "      <td>-0.454598</td>\n",
       "      <td>-0.046579</td>\n",
       "      <td>0.238470</td>\n",
       "      <td>-0.306709</td>\n",
       "      <td>-0.401991</td>\n",
       "      <td>0.343102</td>\n",
       "      <td>-0.327954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.922849  0.132363 -1.117156  0.898516 -0.159263  0.163660 -0.419927   \n",
       "1  1.295603  0.231041 -1.291208  0.607421 -0.041502  0.569090 -0.049636   \n",
       "2 -0.622890 -0.115813 -0.577577  0.881442 -0.310303  0.305865  0.197651   \n",
       "3 -0.729731  0.539017  0.196202 -0.890831  0.530871 -0.791703 -0.204858   \n",
       "4  1.360981  1.062972 -0.722035  0.906465  0.621309 -0.119266  0.176604   \n",
       "\n",
       "         7         8         9     ...           24        25        26  \\\n",
       "0  0.994513  0.082305  0.968195    ...     0.132322 -0.257047  0.438373   \n",
       "1 -0.088628 -0.328559 -0.167960    ...    -0.201041 -0.431792  0.022621   \n",
       "2  0.111288  0.644124 -0.580193    ...     0.020538  0.149915  0.286175   \n",
       "3 -0.015203  1.105974 -0.568063    ...    -0.439378  0.227609  0.325784   \n",
       "4  0.514143  0.243289  1.018288    ...     0.509380  0.347518 -0.164710   \n",
       "\n",
       "         27        28        29        30        31        32        33  \n",
       "0 -0.036248 -0.099411  0.067951  0.631754 -0.126064  0.646724 -0.262620  \n",
       "1 -0.226266  1.339788  0.069940 -0.220712 -0.435173  0.553638 -0.376443  \n",
       "2 -0.111701 -0.049428  0.111080 -0.229686 -0.317591  0.510877 -0.409610  \n",
       "3  0.106025  1.364357 -0.044349 -0.036467 -0.235350 -0.043242  0.085156  \n",
       "4 -0.454598 -0.046579  0.238470 -0.306709 -0.401991  0.343102 -0.327954  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result= pd.DataFrame(pca_result)\n",
    "pca_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "carDateNormalized= pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into fraudulent and non-fraudulent data\n",
    "columnlable= list(carDateNormalized.columns.values)\n",
    "\n",
    "#create the dataframe for fraudulent and non-fraudulent data\n",
    "nonFraudulent= pd.DataFrame(columns=columnlable)\n",
    "nonFraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "fraudulent= pd.DataFrame(columns=columnlable)\n",
    "fraudulentLable= pd.DataFrame(columns=['lable'])\n",
    "\n",
    "\n",
    "j= 0\n",
    "k= 0\n",
    "for i in range(carDateNormalized.shape[0]):\n",
    "    if(carLable[i]==0):\n",
    "        nonFraudulent.loc[j]= carDateNormalized.loc[i]\n",
    "        nonFraudulentLable.loc[j]= 0.0\n",
    "        j += 1\n",
    "    else:\n",
    "        fraudulent.loc[k]= carDateNormalized.loc[i]\n",
    "        fraudulentLable.loc[i]= 1.0\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14496, 34)\n",
      "(923, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922849</td>\n",
       "      <td>0.132363</td>\n",
       "      <td>-1.117156</td>\n",
       "      <td>0.898516</td>\n",
       "      <td>-0.159263</td>\n",
       "      <td>0.163660</td>\n",
       "      <td>-0.419927</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.082305</td>\n",
       "      <td>0.968195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132322</td>\n",
       "      <td>-0.257047</td>\n",
       "      <td>0.438373</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>-0.099411</td>\n",
       "      <td>0.067951</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>-0.126064</td>\n",
       "      <td>0.646724</td>\n",
       "      <td>-0.262620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.295603</td>\n",
       "      <td>0.231041</td>\n",
       "      <td>-1.291208</td>\n",
       "      <td>0.607421</td>\n",
       "      <td>-0.041502</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>-0.049636</td>\n",
       "      <td>-0.088628</td>\n",
       "      <td>-0.328559</td>\n",
       "      <td>-0.167960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201041</td>\n",
       "      <td>-0.431792</td>\n",
       "      <td>0.022621</td>\n",
       "      <td>-0.226266</td>\n",
       "      <td>1.339788</td>\n",
       "      <td>0.069940</td>\n",
       "      <td>-0.220712</td>\n",
       "      <td>-0.435173</td>\n",
       "      <td>0.553638</td>\n",
       "      <td>-0.376443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.622890</td>\n",
       "      <td>-0.115813</td>\n",
       "      <td>-0.577577</td>\n",
       "      <td>0.881442</td>\n",
       "      <td>-0.310303</td>\n",
       "      <td>0.305865</td>\n",
       "      <td>0.197651</td>\n",
       "      <td>0.111288</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>-0.580193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.149915</td>\n",
       "      <td>0.286175</td>\n",
       "      <td>-0.111701</td>\n",
       "      <td>-0.049428</td>\n",
       "      <td>0.111080</td>\n",
       "      <td>-0.229686</td>\n",
       "      <td>-0.317591</td>\n",
       "      <td>0.510877</td>\n",
       "      <td>-0.409610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.729731</td>\n",
       "      <td>0.539017</td>\n",
       "      <td>0.196202</td>\n",
       "      <td>-0.890831</td>\n",
       "      <td>0.530871</td>\n",
       "      <td>-0.791703</td>\n",
       "      <td>-0.204858</td>\n",
       "      <td>-0.015203</td>\n",
       "      <td>1.105974</td>\n",
       "      <td>-0.568063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439378</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>0.325784</td>\n",
       "      <td>0.106025</td>\n",
       "      <td>1.364357</td>\n",
       "      <td>-0.044349</td>\n",
       "      <td>-0.036467</td>\n",
       "      <td>-0.235350</td>\n",
       "      <td>-0.043242</td>\n",
       "      <td>0.085156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.360981</td>\n",
       "      <td>1.062972</td>\n",
       "      <td>-0.722035</td>\n",
       "      <td>0.906465</td>\n",
       "      <td>0.621309</td>\n",
       "      <td>-0.119266</td>\n",
       "      <td>0.176604</td>\n",
       "      <td>0.514143</td>\n",
       "      <td>0.243289</td>\n",
       "      <td>1.018288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509380</td>\n",
       "      <td>0.347518</td>\n",
       "      <td>-0.164710</td>\n",
       "      <td>-0.454598</td>\n",
       "      <td>-0.046579</td>\n",
       "      <td>0.238470</td>\n",
       "      <td>-0.306709</td>\n",
       "      <td>-0.401991</td>\n",
       "      <td>0.343102</td>\n",
       "      <td>-0.327954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.922849  0.132363 -1.117156  0.898516 -0.159263  0.163660 -0.419927   \n",
       "1  1.295603  0.231041 -1.291208  0.607421 -0.041502  0.569090 -0.049636   \n",
       "2 -0.622890 -0.115813 -0.577577  0.881442 -0.310303  0.305865  0.197651   \n",
       "3 -0.729731  0.539017  0.196202 -0.890831  0.530871 -0.791703 -0.204858   \n",
       "4  1.360981  1.062972 -0.722035  0.906465  0.621309 -0.119266  0.176604   \n",
       "\n",
       "         7         8         9     ...           24        25        26  \\\n",
       "0  0.994513  0.082305  0.968195    ...     0.132322 -0.257047  0.438373   \n",
       "1 -0.088628 -0.328559 -0.167960    ...    -0.201041 -0.431792  0.022621   \n",
       "2  0.111288  0.644124 -0.580193    ...     0.020538  0.149915  0.286175   \n",
       "3 -0.015203  1.105974 -0.568063    ...    -0.439378  0.227609  0.325784   \n",
       "4  0.514143  0.243289  1.018288    ...     0.509380  0.347518 -0.164710   \n",
       "\n",
       "         27        28        29        30        31        32        33  \n",
       "0 -0.036248 -0.099411  0.067951  0.631754 -0.126064  0.646724 -0.262620  \n",
       "1 -0.226266  1.339788  0.069940 -0.220712 -0.435173  0.553638 -0.376443  \n",
       "2 -0.111701 -0.049428  0.111080 -0.229686 -0.317591  0.510877 -0.409610  \n",
       "3  0.106025  1.364357 -0.044349 -0.036467 -0.235350 -0.043242  0.085156  \n",
       "4 -0.454598 -0.046579  0.238470 -0.306709 -0.401991  0.343102 -0.327954  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nonFraudulent.shape)\n",
    "print(fraudulent.shape)\n",
    "nonFraudulent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "nonFraudulentLable= pd.DataFrame(nonFraudulentLable)\n",
    "fraudulentLable= pd.DataFrame(fraudulentLable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lable\n",
       "28    1.0\n",
       "52    1.0\n",
       "53    1.0\n",
       "94    1.0\n",
       "96    1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonFraudulent.head()\n",
    "fraudulentLable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(3624, 34)\n",
      "fraudxtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(231, 34)\n"
     ]
    }
   ],
   "source": [
    "#divide the nonFraudulent into test and train\n",
    "nonFraudX_train,nonFraudX_test,nonFraudY_train,nonFraudY_test = train_test_split(nonFraudulent,nonFraudulentLable,random_state=3,test_size=0.25)\n",
    "print('nonFraudxtrain:',type(nonFraudX_train))\n",
    "print(nonFraudX_test.shape)\n",
    "\n",
    "#divide the fraudulent into test and train\n",
    "fraudX_train,fraudX_test,fraudY_train,fraudY_test = train_test_split(fraudulent, fraudulentLable,random_state=3,test_size=0.25)\n",
    "print('fraudxtrain:',type(fraudX_train))\n",
    "print(fraudX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into numpy\n",
    "nonFraudX_train= nonFraudX_train.values\n",
    "nonFraudX_test= nonFraudX_test.values\n",
    "nonFraudY_train= nonFraudY_train.values\n",
    "nonFraudY_test= nonFraudY_test.values\n",
    "\n",
    "fraudX_train= fraudX_train.values\n",
    "fraudX_test= fraudX_test.values\n",
    "fraudY_train= fraudY_train.values\n",
    "fraudY_test= fraudY_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class Dataset which returns the data and labels\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,features,lables,transform= None):\n",
    "        dataTensor= []\n",
    "        lableTensor= []\n",
    "        dataSize= features.shape[0]\n",
    "        \n",
    "        for data in range(dataSize):\n",
    "            feature= features[data,:]\n",
    "            #feature= torch.from_numpy(feature).float()\n",
    "            feature= torch.Tensor(feature)\n",
    "            dataTensor.append(feature)\n",
    "            \n",
    "            lable= np.asanyarray(lables[data])\n",
    "            lable= torch.from_numpy(lable).float()\n",
    "           \n",
    "            #lable= torch.Tensor(lable)\n",
    "            #print(\"lable:\",lable)\n",
    "            #assert(False)\n",
    "           \n",
    "            lableTensor.append(lable)\n",
    "        \n",
    "        #put everything in features and lables\n",
    "        self.features= dataTensor\n",
    "        self.lables= lableTensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        feature= self.features[index]\n",
    "        lable= self.lables[index]\n",
    "        #print(\"get_item feature:\",feature)\n",
    "        #print(\"get_item lable:\",lable)\n",
    "        return feature,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the  nonfraudulent dataset for train and test loader\n",
    "\n",
    "myNonFraudulentTrainDataset= myDataset(nonFraudX_train,nonFraudY_train)\n",
    "myNonFraudulentTestDataset= myDataset(nonFraudX_test,nonFraudY_test)\n",
    "\n",
    "#make the fraudulent dataset for train and test loader\n",
    "myFraudulentTrainDataset= myDataset(fraudX_train,fraudY_train)\n",
    "myFraudulentTestDataset= myDataset(fraudX_test,fraudY_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the trainloader and test loader for nonfraudulent dataset.\n",
    "nonFraudulentTrainLoader= torch.utils.data.DataLoader(myNonFraudulentTrainDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "nonFraudulentTestLoader= torch.utils.data.DataLoader(myNonFraudulentTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "fraudulentTrainLoader= torch.utils.data.DataLoader(myFraudulentTrainDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "fraudulentTestLoader= torch.utils.data.DataLoader(myFraudulentTestDataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class autoencoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder1,self).__init__()\n",
    "        self.encoder1= nn.Sequential(\n",
    "            nn.Linear(34,70),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(70,60),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(60,45),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(45,20),nn.LeakyReLU(True),nn.Linear(20,10))\n",
    "        \n",
    "        self.decoder1= nn.Sequential(\n",
    "            nn.Linear(10,20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20,45),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(45,60),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(60,70),nn.LeakyReLU(True),nn.Linear(70,34),nn.ReLU())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.encoder1(x)\n",
    "        x= self.decoder1(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1= autoencoder1()\n",
    "criterion1= nn.MSELoss()\n",
    "optimizer1= torch.optim.SGD(model1.parameters(), lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.196\n",
      "[2] loss: 0.196\n",
      "[3] loss: 0.193\n",
      "[4] loss: 0.190\n",
      "[5] loss: 0.190\n",
      "[6] loss: 0.190\n",
      "[7] loss: 0.190\n",
      "[8] loss: 0.190\n",
      "[9] loss: 0.189\n",
      "[10] loss: 0.188\n",
      "[11] loss: 0.185\n",
      "[12] loss: 0.180\n",
      "[13] loss: 0.176\n",
      "[14] loss: 0.174\n",
      "[15] loss: 0.174\n",
      "[16] loss: 0.173\n",
      "[17] loss: 0.173\n",
      "[18] loss: 0.172\n",
      "[19] loss: 0.172\n",
      "[20] loss: 0.172\n",
      "[21] loss: 0.172\n",
      "[22] loss: 0.172\n",
      "[23] loss: 0.170\n",
      "[24] loss: 0.168\n",
      "[25] loss: 0.167\n",
      "[26] loss: 0.167\n",
      "[27] loss: 0.167\n",
      "[28] loss: 0.165\n",
      "[29] loss: 0.165\n",
      "[30] loss: 0.164\n",
      "[31] loss: 0.164\n",
      "[32] loss: 0.164\n",
      "[33] loss: 0.164\n",
      "[34] loss: 0.164\n",
      "[35] loss: 0.163\n",
      "[36] loss: 0.162\n",
      "[37] loss: 0.161\n",
      "[38] loss: 0.160\n",
      "[39] loss: 0.159\n",
      "[40] loss: 0.158\n",
      "[41] loss: 0.156\n",
      "[42] loss: 0.155\n",
      "[43] loss: 0.155\n",
      "[44] loss: 0.154\n",
      "[45] loss: 0.154\n",
      "[46] loss: 0.153\n",
      "[47] loss: 0.153\n",
      "[48] loss: 0.152\n",
      "[49] loss: 0.152\n",
      "[50] loss: 0.151\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for epoch in range(50):\n",
    "    running_loss= 0.0\n",
    "    for i, (feature1,lable1) in enumerate(nonFraudulentTrainLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs1= torch.tensor(feature1)\n",
    "        lables1= torch.tensor(lable1)\n",
    "        lables1= lables1.type(torch.LongTensor)\n",
    "        \n",
    "        # =====================forward====================\n",
    "        output1 = model1(inputs1)\n",
    "        loss1 = criterion1(output1,inputs1)\n",
    "        \n",
    "         # ===================backward====================\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        # =======print the statistics\n",
    "        running_loss += loss1.item()\n",
    "        \n",
    "        #print(\"i: \",i)\n",
    "    #if i%100 == 0:              #print every 2000 mini-batches\n",
    "    \n",
    "    print('[%d] loss: %.3f' %\n",
    "              (epoch + 1,  running_loss /nonFraudX_train.shape[0]))\n",
    "      #running_loss = 0.0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " #test the model\n",
    "def modelTest1(Loader):\n",
    "    lossList1= []\n",
    "    trueLable1= []\n",
    "    for i,(feature1,lable1) in enumerate(Loader):\n",
    "        inputs1= torch.tensor(feature1)\n",
    "        output1= model1(inputs1)\n",
    "        loss1= criterion1(output1,inputs1)\n",
    "        trueLable1.append(lable1)\n",
    "        lossList1.append(loss1.item())\n",
    "        lossValue1= loss1.item()\n",
    "        f= open(\"./modle1Loss.txt\",'a')\n",
    "        f.write(str(lossValue1) + '\\n')\n",
    "        '''\n",
    "        if(i!=10):\n",
    "            print(\"input:\",inputs1)\n",
    "            print(\"output:\",output1)\n",
    "            print(\"loss1:\",loss1)\n",
    "            print(\"lable:\",lable1)\n",
    "        else:\n",
    "            assert(False)\n",
    "        '''\n",
    "    f.close()\n",
    "    return lossList1,trueLable1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16235050559043884"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the model\n",
    "lossList1,trueLable1= modelTest1(nonFraudulentTestLoader)\n",
    "len(lossList1)\n",
    "lossList1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network architecture for the base autoencoders\n",
    "class autoencoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder2,self).__init__()\n",
    "        self.encoder2= nn.Sequential(\n",
    "            nn.Linear(34,87),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(87,77),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(77,55),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(55,45),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(45,25),nn.LeakyReLU(True),nn.Linear(25,10))\n",
    "       \n",
    "        self.decoder2= nn.Sequential(\n",
    "            nn.Linear(10,25),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(25,45),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(45,55),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(55,77),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(77,87),nn.LeakyReLU(True),nn.Linear(87,34),nn.ReLU())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.encoder2(x)\n",
    "        x= self.decoder2(x)\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= autoencoder2()\n",
    "criterion2= nn.MSELoss()\n",
    "optimizer2= torch.optim.SGD(model2.parameters(), lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.062\n",
      "[2] loss: 0.062\n",
      "[3] loss: 0.062\n",
      "[4] loss: 0.062\n",
      "[5] loss: 0.062\n",
      "[6] loss: 0.062\n",
      "[7] loss: 0.062\n",
      "[8] loss: 0.062\n",
      "[9] loss: 0.062\n",
      "[10] loss: 0.062\n",
      "[11] loss: 0.062\n",
      "[12] loss: 0.062\n",
      "[13] loss: 0.062\n",
      "[14] loss: 0.062\n",
      "[15] loss: 0.062\n",
      "[16] loss: 0.062\n",
      "[17] loss: 0.062\n",
      "[18] loss: 0.062\n",
      "[19] loss: 0.062\n",
      "[20] loss: 0.062\n",
      "[21] loss: 0.062\n",
      "[22] loss: 0.062\n",
      "[23] loss: 0.062\n",
      "[24] loss: 0.062\n",
      "[25] loss: 0.062\n",
      "[26] loss: 0.062\n",
      "[27] loss: 0.062\n",
      "[28] loss: 0.062\n",
      "[29] loss: 0.062\n",
      "[30] loss: 0.062\n",
      "[31] loss: 0.062\n",
      "[32] loss: 0.062\n",
      "[33] loss: 0.062\n",
      "[34] loss: 0.062\n",
      "[35] loss: 0.062\n",
      "[36] loss: 0.062\n",
      "[37] loss: 0.062\n",
      "[38] loss: 0.062\n",
      "[39] loss: 0.062\n",
      "[40] loss: 0.062\n",
      "[41] loss: 0.062\n",
      "[42] loss: 0.062\n",
      "[43] loss: 0.062\n",
      "[44] loss: 0.062\n",
      "[45] loss: 0.062\n",
      "[46] loss: 0.062\n",
      "[47] loss: 0.062\n",
      "[48] loss: 0.062\n",
      "[49] loss: 0.062\n",
      "[50] loss: 0.062\n",
      "[51] loss: 0.062\n",
      "[52] loss: 0.062\n",
      "[53] loss: 0.062\n",
      "[54] loss: 0.062\n",
      "[55] loss: 0.062\n",
      "[56] loss: 0.062\n",
      "[57] loss: 0.062\n",
      "[58] loss: 0.062\n",
      "[59] loss: 0.062\n",
      "[60] loss: 0.062\n",
      "[61] loss: 0.062\n",
      "[62] loss: 0.062\n",
      "[63] loss: 0.062\n",
      "[64] loss: 0.062\n",
      "[65] loss: 0.062\n",
      "[66] loss: 0.062\n",
      "[67] loss: 0.062\n",
      "[68] loss: 0.062\n",
      "[69] loss: 0.062\n",
      "[70] loss: 0.062\n",
      "[71] loss: 0.062\n",
      "[72] loss: 0.062\n",
      "[73] loss: 0.062\n",
      "[74] loss: 0.062\n",
      "[75] loss: 0.062\n",
      "[76] loss: 0.062\n",
      "[77] loss: 0.062\n",
      "[78] loss: 0.062\n",
      "[79] loss: 0.062\n",
      "[80] loss: 0.062\n",
      "[81] loss: 0.062\n",
      "[82] loss: 0.062\n",
      "[83] loss: 0.062\n",
      "[84] loss: 0.062\n",
      "[85] loss: 0.062\n",
      "[86] loss: 0.062\n",
      "[87] loss: 0.062\n",
      "[88] loss: 0.062\n",
      "[89] loss: 0.062\n",
      "[90] loss: 0.062\n",
      "[91] loss: 0.062\n",
      "[92] loss: 0.062\n",
      "[93] loss: 0.062\n",
      "[94] loss: 0.062\n",
      "[95] loss: 0.062\n",
      "[96] loss: 0.062\n",
      "[97] loss: 0.062\n",
      "[98] loss: 0.062\n",
      "[99] loss: 0.062\n",
      "[100] loss: 0.062\n",
      "[101] loss: 0.062\n",
      "[102] loss: 0.062\n",
      "[103] loss: 0.062\n",
      "[104] loss: 0.062\n",
      "[105] loss: 0.062\n",
      "[106] loss: 0.062\n",
      "[107] loss: 0.062\n",
      "[108] loss: 0.062\n",
      "[109] loss: 0.062\n",
      "[110] loss: 0.062\n",
      "[111] loss: 0.062\n",
      "[112] loss: 0.062\n",
      "[113] loss: 0.062\n",
      "[114] loss: 0.062\n",
      "[115] loss: 0.062\n",
      "[116] loss: 0.062\n",
      "[117] loss: 0.062\n",
      "[118] loss: 0.062\n",
      "[119] loss: 0.062\n",
      "[120] loss: 0.062\n",
      "[121] loss: 0.062\n",
      "[122] loss: 0.062\n",
      "[123] loss: 0.062\n",
      "[124] loss: 0.062\n",
      "[125] loss: 0.062\n",
      "[126] loss: 0.062\n",
      "[127] loss: 0.062\n",
      "[128] loss: 0.062\n",
      "[129] loss: 0.062\n",
      "[130] loss: 0.062\n",
      "[131] loss: 0.062\n",
      "[132] loss: 0.062\n",
      "[133] loss: 0.062\n",
      "[134] loss: 0.062\n",
      "[135] loss: 0.062\n",
      "[136] loss: 0.062\n",
      "[137] loss: 0.062\n",
      "[138] loss: 0.062\n",
      "[139] loss: 0.062\n",
      "[140] loss: 0.062\n",
      "[141] loss: 0.062\n",
      "[142] loss: 0.062\n",
      "[143] loss: 0.062\n",
      "[144] loss: 0.062\n",
      "[145] loss: 0.062\n",
      "[146] loss: 0.062\n",
      "[147] loss: 0.062\n",
      "[148] loss: 0.062\n",
      "[149] loss: 0.062\n",
      "[150] loss: 0.062\n",
      "[151] loss: 0.062\n",
      "[152] loss: 0.062\n",
      "[153] loss: 0.062\n",
      "[154] loss: 0.062\n",
      "[155] loss: 0.062\n",
      "[156] loss: 0.062\n",
      "[157] loss: 0.062\n",
      "[158] loss: 0.062\n",
      "[159] loss: 0.062\n",
      "[160] loss: 0.062\n",
      "[161] loss: 0.062\n",
      "[162] loss: 0.062\n",
      "[163] loss: 0.062\n",
      "[164] loss: 0.062\n",
      "[165] loss: 0.062\n",
      "[166] loss: 0.062\n",
      "[167] loss: 0.062\n",
      "[168] loss: 0.062\n",
      "[169] loss: 0.062\n",
      "[170] loss: 0.062\n",
      "[171] loss: 0.062\n",
      "[172] loss: 0.062\n",
      "[173] loss: 0.062\n",
      "[174] loss: 0.062\n",
      "[175] loss: 0.062\n",
      "[176] loss: 0.062\n",
      "[177] loss: 0.062\n",
      "[178] loss: 0.062\n",
      "[179] loss: 0.062\n",
      "[180] loss: 0.062\n",
      "[181] loss: 0.062\n",
      "[182] loss: 0.062\n",
      "[183] loss: 0.062\n",
      "[184] loss: 0.062\n",
      "[185] loss: 0.062\n",
      "[186] loss: 0.062\n",
      "[187] loss: 0.062\n",
      "[188] loss: 0.062\n",
      "[189] loss: 0.062\n",
      "[190] loss: 0.062\n",
      "[191] loss: 0.062\n",
      "[192] loss: 0.062\n",
      "[193] loss: 0.062\n",
      "[194] loss: 0.062\n",
      "[195] loss: 0.062\n",
      "[196] loss: 0.062\n",
      "[197] loss: 0.062\n",
      "[198] loss: 0.062\n",
      "[199] loss: 0.062\n",
      "[200] loss: 0.062\n",
      "[201] loss: 0.062\n",
      "[202] loss: 0.062\n",
      "[203] loss: 0.062\n",
      "[204] loss: 0.062\n",
      "[205] loss: 0.062\n",
      "[206] loss: 0.062\n",
      "[207] loss: 0.062\n",
      "[208] loss: 0.062\n",
      "[209] loss: 0.062\n",
      "[210] loss: 0.062\n",
      "[211] loss: 0.062\n",
      "[212] loss: 0.062\n",
      "[213] loss: 0.062\n",
      "[214] loss: 0.062\n",
      "[215] loss: 0.062\n",
      "[216] loss: 0.062\n",
      "[217] loss: 0.062\n",
      "[218] loss: 0.062\n",
      "[219] loss: 0.062\n",
      "[220] loss: 0.062\n",
      "[221] loss: 0.062\n",
      "[222] loss: 0.062\n",
      "[223] loss: 0.062\n",
      "[224] loss: 0.062\n",
      "[225] loss: 0.062\n",
      "[226] loss: 0.062\n",
      "[227] loss: 0.062\n",
      "[228] loss: 0.062\n",
      "[229] loss: 0.062\n",
      "[230] loss: 0.062\n",
      "[231] loss: 0.062\n",
      "[232] loss: 0.062\n",
      "[233] loss: 0.062\n",
      "[234] loss: 0.062\n",
      "[235] loss: 0.062\n",
      "[236] loss: 0.062\n",
      "[237] loss: 0.062\n",
      "[238] loss: 0.062\n",
      "[239] loss: 0.062\n",
      "[240] loss: 0.062\n",
      "[241] loss: 0.062\n",
      "[242] loss: 0.062\n",
      "[243] loss: 0.062\n",
      "[244] loss: 0.062\n",
      "[245] loss: 0.062\n",
      "[246] loss: 0.062\n",
      "[247] loss: 0.062\n",
      "[248] loss: 0.062\n",
      "[249] loss: 0.062\n",
      "[250] loss: 0.062\n",
      "[251] loss: 0.062\n",
      "[252] loss: 0.062\n",
      "[253] loss: 0.062\n",
      "[254] loss: 0.062\n",
      "[255] loss: 0.062\n",
      "[256] loss: 0.062\n",
      "[257] loss: 0.062\n",
      "[258] loss: 0.062\n",
      "[259] loss: 0.062\n",
      "[260] loss: 0.062\n",
      "[261] loss: 0.062\n",
      "[262] loss: 0.062\n",
      "[263] loss: 0.062\n",
      "[264] loss: 0.062\n",
      "[265] loss: 0.062\n",
      "[266] loss: 0.062\n",
      "[267] loss: 0.062\n",
      "[268] loss: 0.062\n",
      "[269] loss: 0.062\n",
      "[270] loss: 0.062\n",
      "[271] loss: 0.062\n",
      "[272] loss: 0.062\n",
      "[273] loss: 0.062\n",
      "[274] loss: 0.062\n",
      "[275] loss: 0.062\n",
      "[276] loss: 0.062\n",
      "[277] loss: 0.062\n",
      "[278] loss: 0.062\n",
      "[279] loss: 0.062\n",
      "[280] loss: 0.062\n",
      "[281] loss: 0.062\n",
      "[282] loss: 0.062\n",
      "[283] loss: 0.062\n",
      "[284] loss: 0.062\n",
      "[285] loss: 0.062\n",
      "[286] loss: 0.062\n",
      "[287] loss: 0.062\n",
      "[288] loss: 0.062\n",
      "[289] loss: 0.062\n",
      "[290] loss: 0.062\n",
      "[291] loss: 0.062\n",
      "[292] loss: 0.062\n",
      "[293] loss: 0.062\n",
      "[294] loss: 0.062\n",
      "[295] loss: 0.062\n",
      "[296] loss: 0.062\n",
      "[297] loss: 0.062\n",
      "[298] loss: 0.062\n",
      "[299] loss: 0.062\n",
      "[300] loss: 0.062\n"
     ]
    }
   ],
   "source": [
    "#train the model2\n",
    "for epoch in range(300):\n",
    "    running_loss2= 0.0\n",
    "    for i, (feature2,lable2) in enumerate(fraudulentTestLoader):\n",
    "        \n",
    "        #gets the inputs\n",
    "        inputs2= torch.tensor(feature2)\n",
    "        lables2= torch.tensor(lable2)\n",
    "        lables2= lables2.type(torch.LongTensor)\n",
    "       \n",
    "        # =====================forward====================\n",
    "        output2= model2(inputs2)\n",
    "        loss2= criterion2(output2,inputs2)\n",
    "        \n",
    "        # ===================backward====================\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        # =======print the statistics\n",
    "        running_loss2 += loss2.item()\n",
    "        \n",
    "        #print(\"i: \",i)\n",
    "    #if i%100 == 0:              #print every 2000 mini-batches\n",
    "    \n",
    "    print('[%d] loss: %.3f' %\n",
    "              (epoch + 1,  running_loss2 / fraudX_train.shape[0]))\n",
    "      #running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest2(Loader2):\n",
    "    lossList2= []\n",
    "    trueLable1= []\n",
    "    for i,(feature2,lable2) in enumerate(Loader2):\n",
    "        inputs2= torch.tensor(feature2)\n",
    "        output2= model2(inputs2)\n",
    "        loss2= criterion2(output2,inputs2)\n",
    "        trueLable1.append(lable2)\n",
    "        lossList2.append(loss2.item()) #put the loss\n",
    "        '''        \n",
    "        if(i!=10):\n",
    "            print(\"input:\",inputs2)\n",
    "            print(\"output:\",output2)\n",
    "            print(\"loss1:\",loss2)\n",
    "            print(\"lable:\",lable2)\n",
    "        else:\n",
    "            assert(False)\n",
    "        '''\n",
    "        lossValue2= str(loss2.item())\n",
    "        f= open(\"./modle2Loss.txt\",'a')\n",
    "        f.write(lossValue2+'\\n')\n",
    "    f.close()\n",
    "        \n",
    "    return lossList2,trueLable1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossList2,trueLable1= modelTest2(nonFraudulentTestLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLable1=[]\n",
    "for k in range(len(lossList1)):\n",
    "    if(lossList1[k]<lossList2[k]):\n",
    "        predictedLable1.append(0)\n",
    "    else:\n",
    "        predictedLable1.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model1 with another type of data\n",
    "lossList3,trueLable2= modelTest1(fraudulentTestLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model2 with another type of data\n",
    "lossList4,trueLable2= modelTest2(fraudulentTestLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLable2=[]\n",
    "for k in range(len(lossList3)):\n",
    "    if(lossList3[k]<lossList4[k]):\n",
    "        predictedLable2.append(0)\n",
    "    else:\n",
    "        predictedLable2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result \n",
      "Accuracy: 78.18417639429313\n",
      "Sensitivity: 29.004329004329005\n",
      "Specificity: 81.31898454746137\n",
      "\n",
      "\n",
      "\n",
      "TP: 67\n",
      "\n",
      "FN: 164\n",
      "\n",
      "FP: 677\n",
      "\n",
      "TN: 2947\n"
     ]
    }
   ],
   "source": [
    "totalLength1= len(trueLable1)\n",
    "TP=FP=FN=TN= 0\n",
    "for i in range(totalLength1):\n",
    "    if(int(trueLable1[i])==1 and predictedLable1[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable1[i])==1 and predictedLable1[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable1[i])==0 and predictedLable1[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable1[i])==0 and predictedLable1[i]==1):\n",
    "        FP += 1\n",
    "\n",
    "totalLength2= len(trueLable2)\n",
    "for i in range(totalLength2):\n",
    "    if(int(trueLable2[i])==1 and predictedLable2[i]==1):\n",
    "        TP += 1\n",
    "    elif(int(trueLable2[i])==1 and predictedLable2[i]==0):\n",
    "        FN += 1\n",
    "    elif(int(trueLable2[i])==0 and predictedLable2[i]==0):\n",
    "        TN += 1\n",
    "    elif(int(trueLable2[i])==0 and predictedLable2[i]==1):\n",
    "        FP += 1\n",
    "        \n",
    "print(\"\\nResult \")\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)\n",
    "print(\"\\n\")\n",
    "print(\"\\nTP:\",TP)\n",
    "print(\"\\nFN:\",FN)\n",
    "print(\"\\nFP:\",FP)\n",
    "print(\"\\nTN:\",TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
