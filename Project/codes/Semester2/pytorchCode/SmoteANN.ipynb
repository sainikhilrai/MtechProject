{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "#read the input\n",
    "carDf= pd.read_csv('oneHotNormalizedWithLable.csv')\n",
    "carLable= carDf['Lable']\n",
    "\n",
    "#drop the Accident Date and Claim Date\n",
    "carDf.drop(['Lable'],inplace=True,axis=1)\n",
    "carDf.head()\n",
    "\n",
    "print(type(carLable[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "carDf= carDf.values\n",
    "carLable= carLable.values\n",
    "print(type(carDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the own data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the class Dataset which returns the data and labels\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,features,lables,transform= None):\n",
    "        dataTensor= []\n",
    "        lableTensor= []\n",
    "        dataSize= features.shape[0]\n",
    "        \n",
    "        for data in range(dataSize):\n",
    "            feature= features[data,:]\n",
    "            #feature= torch.from_numpy(feature).float()\n",
    "            feature= torch.Tensor(feature)\n",
    "            dataTensor.append(feature)\n",
    "            \n",
    "            lable= np.asanyarray(lables[data])\n",
    "            lable= torch.from_numpy(lable).float()\n",
    "           \n",
    "            #lable= torch.Tensor(lable)\n",
    "            #print(\"lable:\",lable)\n",
    "            #assert(False)\n",
    "           \n",
    "            lableTensor.append(lable)\n",
    "        \n",
    "        #put everything in features and lables\n",
    "        self.features= dataTensor\n",
    "        self.lables= lableTensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        feature= self.features[index]\n",
    "        lable= self.lables[index]\n",
    "        #print(\"get_item feature:\",feature)\n",
    "        #print(\"get_item lable:\",lable)\n",
    "        return feature,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into two part train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(carDf,carLable,random_state=3,test_size=0.25)\n",
    "print('xtrain:',type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the dataset for train and test loader\n",
    "mytrainDataset= myDataset(X_train,y_train)\n",
    "mytestDataset= myDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9295, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0705, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "        1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0705,\n",
      "        0.0000, 0.9295, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "        0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0188, 0.2500, 1.0000, 0.0497])\n",
      "lable: tensor(1.)\n",
      "torch.Size([97])\n"
     ]
    }
   ],
   "source": [
    "feature,lable= mytrainDataset.__getitem__(0)\n",
    "print(\"feature:\",feature)\n",
    "print(\"lable:\",lable)\n",
    "print(feature.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(mytrainDataset,batch_size=32,shuffle=True,num_workers=0)\n",
    "testLoader= torch.utils.data.DataLoader(mytestDataset,batch_size=32,shuffle=True,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network architecture\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN,self).__init__()\n",
    "        self.fc1= nn.Linear(97,6)\n",
    "        self.fc2= nn.Linear(6,7)\n",
    "        self.fc3= nn.Linear(7,9)\n",
    "        self.fc4= nn.Linear(9,4)\n",
    "        self.fc5= nn.Linear(4,5)\n",
    "        self.fc6= nn.Linear(5,4)\n",
    "        self.fc7= nn.Linear(4,4)\n",
    "        self.fc8= nn.Linear(4,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.fc1(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc2(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc3(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc4(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc5(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc6(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc7(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.fc8(x)\n",
    "        x= F.sigmoid(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#code on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model= ANN()\n",
    "model= model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a loss function\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= optim.SGD(model.parameters(),lr=0.01,momentum = 0.9)\n",
    "criterion= criterion.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModle(model,epochNo):\n",
    "    correct= 0\n",
    "    total= 0\n",
    "    lableList= []\n",
    "    predictedList= []\n",
    "    #lableList= lableList.to(device)\n",
    "    #predictedList= predictedList.to(device)\n",
    "    for i,(feature,lable) in enumerate(testLoader,0):\n",
    "            #gets the inputs\n",
    "            inputs= feature\n",
    "            lables= lable\n",
    "            lables= lables.type(torch.LongTensor)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            lables = lables.to(device)\n",
    "\n",
    "            output= model(inputs)\n",
    "            _,prediction= torch.max(output.data,1)\n",
    "            correct += (prediction == lables).sum()\n",
    "\n",
    "            total+= lables.size(0)\n",
    "            lableList.append(lables)\n",
    "            predictedList.append(prediction)\n",
    "\n",
    "       \n",
    "    \n",
    "    trueLable= []\n",
    "    predictedLable= []\n",
    "    listLength= len(lableList)\n",
    "    \n",
    "    for i in range(listLength):\n",
    "        size= len(lableList[i])\n",
    "        for j in range(size):\n",
    "            trueLable.append(lableList[i][j].cpu().numpy())\n",
    "            predictedLable.append(predictedList[i][j].cpu().numpy())\n",
    "    totalLength= len(trueLable)\n",
    "    TP=FP=FN=TN= 0\n",
    "    for i in range(totalLength):\n",
    "        if(trueLable[i]==1 and predictedLable[i]==1):\n",
    "            TP += 1\n",
    "        elif(trueLable[i]==1 and predictedLable[i]==0):\n",
    "            FN += 1\n",
    "        elif(trueLable[i]==0 and predictedLable[i]==0):\n",
    "            TN += 1\n",
    "        elif(trueLable[i]==0 and predictedLable[i]==1):\n",
    "            FP += 1\n",
    "    print(\"\\nResult for epoch: \",epochNo)\n",
    "    print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "    print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "    print(\"Specificity:\",TN/(TN+FP)*100)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nTP:\",TP)\n",
    "    print(\"\\nFN:\",FN)\n",
    "    print(\"\\nFP:\",FP)\n",
    "    print(\"\\nTN:\",TN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  1\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[2,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  2\n",
      "Accuracy: 50.01379690949227\n",
      "Sensitivity: 100.0\n",
      "Specificity: 0.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 3625\n",
      "\n",
      "FN: 0\n",
      "\n",
      "FP: 3623\n",
      "\n",
      "TN: 0\n",
      "[3,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  3\n",
      "Accuracy: 50.01379690949227\n",
      "Sensitivity: 100.0\n",
      "Specificity: 0.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 3625\n",
      "\n",
      "FN: 0\n",
      "\n",
      "FP: 3623\n",
      "\n",
      "TN: 0\n",
      "[4,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  4\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[5,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  5\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[6,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  6\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[7,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  7\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[8,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  8\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[9,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  9\n",
      "Accuracy: 49.98620309050772\n",
      "Sensitivity: 0.0\n",
      "Specificity: 100.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 0\n",
      "\n",
      "FN: 3625\n",
      "\n",
      "FP: 0\n",
      "\n",
      "TN: 3623\n",
      "[10,   680] loss: 0.022\n",
      "\n",
      "Result for epoch:  10\n",
      "Accuracy: 50.01379690949227\n",
      "Sensitivity: 100.0\n",
      "Specificity: 0.0\n",
      "\n",
      "\n",
      "\n",
      "TP: 3625\n",
      "\n",
      "FN: 0\n",
      "\n",
      "FP: 3623\n",
      "\n",
      "TN: 0\n",
      "finished Training\n"
     ]
    }
   ],
   "source": [
    "#tarin the network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i,(feature,lable) in enumerate(trainLoader,0):\n",
    "        #gets the inputs\n",
    "                \n",
    "        inputs= torch.tensor(feature)\n",
    "        lables= torch.tensor(lable)\n",
    "        lables= lables.type(torch.LongTensor)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        lables = lables.to(device)\n",
    "        \n",
    "        #zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward+backward+optimize\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output,lables)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print the statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    #print after every epoch\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / X_train.shape[0]))\n",
    "    testModle(model,epoch+1)\n",
    "        \n",
    "print(\"finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on the training set\n",
    "testModle(model,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
