{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import ClusterCentroids \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the data into dataframe\n",
    "car_df = pd.read_csv('newCardata.csv')\n",
    "car_features = pd.read_csv('finalDataPreprocessBinary.csv')\n",
    "car_label = car_df['FraudFound']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15419,)\n",
      "xtrain: <class 'pandas.core.frame.DataFrame'>\n",
      "(15419, 52) (15419,)\n"
     ]
    }
   ],
   "source": [
    "#change the label of the data\n",
    "labelNo = LabelEncoder()\n",
    "car_df['FraudFound'] = labelNo.fit_transform(car_df['FraudFound'].astype('str'))\n",
    "car_label = car_df['FraudFound']\n",
    "print(car_label.shape)\n",
    "\n",
    "#split the data set\n",
    "X_train,X_test,y_train,y_test = train_test_split(car_features,car_label,random_state=3,test_size=0.25)\n",
    "print('xtrain:',type(X_train))\n",
    "\n",
    "#split the data into train and test\n",
    "print(car_features.shape,car_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier:\n",
      "<class 'numpy.ndarray'> (3855,) [0 0 0 ..., 0 0 0]\n",
      "Accuracy is  93.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print('Random forest classifier:')\n",
    "predicted = model.predict(X_test)\n",
    "print(type(predicted),predicted.shape,predicted)\n",
    "print('Accuracy is ',round(accuracy_score(y_test,model.predict(X_test)) * 100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[3603    6]\n",
      " [ 241    5]]\n",
      "TN: 3603\n",
      "FP: 6\n",
      "FN: 241\n",
      "TP: 5\n",
      "Accuracy: 93.5927367056\n",
      "Sensitivity: 2.0325203252\n",
      "Specificity: 99.8337489609\n"
     ]
    }
   ],
   "source": [
    "# calculating specifity and sensitivity\n",
    "# 0  := Negative\n",
    "# 1 := Positive\n",
    "cm = confusion_matrix(y_test,predicted)\n",
    "print(\"Confusion Matrix:\\n\",cm)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(\"TN:\",TN)\n",
    "print(\"FP:\",FP)\n",
    "print(\"FN:\",FN)\n",
    "print(\"TP:\",TP)\n",
    "\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n",
      "3855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#converting pandas.core.series.Series to numpy.ndarray\n",
    "print(type(y_test),type(predicted))\n",
    "ytest = pd.Series(y_test).values\n",
    "print((ytest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of fraud cases: 246\n",
      "No of fauld cases predicted by model: 11\n",
      "No of fraud correctly predicted  as fault: 5\n"
     ]
    }
   ],
   "source": [
    "#find the index where both are 1.\n",
    "count = 0\n",
    "fault = 0\n",
    "predictedfault = 0\n",
    "for i in range(predicted.shape[0]):\n",
    "    if((predicted[i] == 1) and (ytest[i] == 1)):\n",
    "        count += 1\n",
    "    if(ytest[i] == 1):\n",
    "        fault += 1\n",
    "    if(predicted[i]==1):\n",
    "        predictedfault += 1\n",
    "print(\"No of fraud cases:\",fault)\n",
    "print(\"No of fauld cases predicted by model:\",predictedfault)\n",
    "print(\"No of fraud correctly predicted  as fault:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origina dataset shape Counter({0: 14496, 1: 923})\n",
      "origina dataset shape Counter({0: 923, 1: 923})\n",
      "(1384, 52)\n"
     ]
    }
   ],
   "source": [
    "#with Near Miss\n",
    "print('origina dataset shape {}'.format(Counter(car_label)))\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "cc = cc.fit(car_features,car_label)\n",
    "X_resampled,y_resampled = cc.fit_sample(car_features,car_label)\n",
    "print('origina dataset shape {}'.format(Counter(y_resampled)))\n",
    "#print(features.shape,labels.shape)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_resampled,y_resampled,random_state=3,test_size=0.25)\n",
    "\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier:\n",
      "Accuracy is  100.0\n",
      "Confusion Matrix:\n",
      " [[243   0]\n",
      " [  0 219]]\n",
      "Accuracy: 100.0\n",
      "Sensitivity: 100.0\n",
      "Specificity: 100.0\n"
     ]
    }
   ],
   "source": [
    "#model object\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train,y_train)\n",
    "print('Random forest classifier:')\n",
    "predicted = model.predict(X_test)\n",
    "print('Accuracy is ',round(accuracy_score(y_test,model.predict(X_test)) * 100,2))\n",
    "cm = confusion_matrix(y_test,predicted)\n",
    "print(\"Confusion Matrix:\\n\",cm)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+FP+FN+TN)*100)\n",
    "print(\"Sensitivity:\",TP/(TP+FN)*100)\n",
    "print(\"Specificity:\",TN/(TN+FP)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "462\n"
     ]
    }
   ],
   "source": [
    "#converting pandas.core.series.Series to numpy.ndarray\n",
    "ytest = pd.Series(y_test).values\n",
    "print((ytest.shape[0]))\n",
    "print(predicted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of fraud cases: 219\n",
      "No of fauld cases predicted by model: 219\n",
      "No of fraud correctly predicted  as fault: 219\n"
     ]
    }
   ],
   "source": [
    "#find the index where both are 1.\n",
    "count = 0\n",
    "fault = 0\n",
    "predictedfault = 0\n",
    "for i in range(predicted.shape[0]):\n",
    "    \n",
    "    if((predicted[i] == 1) and (ytest[i] == 1)):\n",
    "        count += 1\n",
    "    if(ytest[i] == 1):\n",
    "        fault += 1\n",
    "    if(predicted[i]==1):\n",
    "        predictedfault += 1\n",
    "print(\"No of fraud cases:\",fault)\n",
    "print(\"No of fauld cases predicted by model:\",predictedfault)\n",
    "print(\"No of fraud correctly predicted  as fault:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(y_test, predicted, average='micro')\n",
    "precision_recall_fscore_support(y_test, predicted, average='macro')\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier:\n",
      "Resampled dataset shape Counter({0: 923, 1: 923})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "print('Random forest classifier:')\n",
    "\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_resampled,Y_resampled = cc.fit_sample(car_features,car_label)\n",
    "print('Resampled dataset shape {}'.format(Counter(Y_resampled)))\n",
    "\n",
    "#convert np.array to dataframe \n",
    "df_features = pd.DataFrame(X_resampled)\n",
    "df_lables = pd.DataFrame(Y_resampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994594594595\n",
      "0.566666666667\n",
      "0.6\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "#add the both the labels and features\n",
    "df_features['FraudFound']= df_lables\n",
    "\n",
    "#shuffle the featrues\n",
    "new_combine_features = df_features.set_index(np.random.permutation(df_features.index))\n",
    "\n",
    "\n",
    "new_label = new_combine_features['FraudFound']\n",
    "\n",
    "#drop the FraudFound attribute\n",
    "new_combine_features.drop(['FraudFound'],inplace=True,axis=1)\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=10)\n",
    "model=RandomForestClassifier(n_estimators=100) \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,X=new_combine_features,y=new_label,cv=kfold,scoring=scoring)\n",
    "\n",
    "print(np.mean(results['test_accuracy']))\n",
    "print(np.mean(results['test_precision']))\n",
    "print(np.mean(results['test_recall']))\n",
    "print(np.mean(results['test_f1_score']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
